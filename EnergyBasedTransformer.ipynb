{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM2FfeFvPrr0URrIiDzIFx1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sonnyloweus/QuantumDynamicsAI/blob/main/EnergyBasedTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B70wgiwMLH8R",
        "outputId": "2a38b2d6-9276-47b0-e581-efd0ce88e43b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "['Qiskit Labs', 'Symmetric_Exclusion_Process _Simulator.ipynb', 'Project Presentation Short.gslides', 'dense_small.param', 'quantum_simulation_data.pkl', 'Quantum_Brickworks_Circuit_Simulator.ipynb', 'DiscreteVariationalParameterizations.py', 'Screenshots', '__pycache__', 'BoltzmannEncoderDecoder.ipynb', 'QuantumSimulatorDataset.py', 'GibbsSampling.py', 'DiscreteVariationalParameterizationsDeepV2.py', 'EnergyBasedTransformer.ipynb']\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "import sys\n",
        "directory_path = '/content/drive/MyDrive/Quantum/'\n",
        "sys.path.append('/content/drive/MyDrive/Quantum')\n",
        "\n",
        "print(os.listdir(directory_path))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install qiskit-aer\n",
        "!pip install qiskit\n",
        "!pip install pylatexenc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "u3NkfGVa1e-e",
        "outputId": "d145ddc2-b390-458e-c471-17174c81fe48"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting qiskit-aer\n",
            "  Downloading qiskit_aer-0.14.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting qiskit>=0.45.2 (from qiskit-aer)\n",
            "  Downloading qiskit-1.1.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.10/dist-packages (from qiskit-aer) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-aer) (1.11.4)\n",
            "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.10/dist-packages (from qiskit-aer) (5.9.5)\n",
            "Collecting rustworkx>=0.14.0 (from qiskit>=0.45.2->qiskit-aer)\n",
            "  Downloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.10/dist-packages (from qiskit>=0.45.2->qiskit-aer) (1.12.1)\n",
            "Collecting dill>=0.3 (from qiskit>=0.45.2->qiskit-aer)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from qiskit>=0.45.2->qiskit-aer) (2.8.2)\n",
            "Collecting stevedore>=3.0.0 (from qiskit>=0.45.2->qiskit-aer)\n",
            "  Downloading stevedore-5.2.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qiskit>=0.45.2->qiskit-aer) (4.12.2)\n",
            "Collecting symengine>=0.11 (from qiskit>=0.45.2->qiskit-aer)\n",
            "  Downloading symengine-0.11.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (39.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.4/39.4 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.0->qiskit>=0.45.2->qiskit-aer) (1.16.0)\n",
            "Collecting pbr!=2.1.0,>=2.0.0 (from stevedore>=3.0.0->qiskit>=0.45.2->qiskit-aer)\n",
            "  Downloading pbr-6.0.0-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.5/107.5 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.3->qiskit>=0.45.2->qiskit-aer) (1.3.0)\n",
            "Installing collected packages: symengine, rustworkx, pbr, dill, stevedore, qiskit, qiskit-aer\n",
            "Successfully installed dill-0.3.8 pbr-6.0.0 qiskit-1.1.1 qiskit-aer-0.14.2 rustworkx-0.15.1 stevedore-5.2.0 symengine-0.11.0\n",
            "Requirement already satisfied: qiskit in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Requirement already satisfied: rustworkx>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from qiskit) (0.15.1)\n",
            "Requirement already satisfied: numpy<3,>=1.17 in /usr/local/lib/python3.10/dist-packages (from qiskit) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.10/dist-packages (from qiskit) (1.11.4)\n",
            "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.10/dist-packages (from qiskit) (1.12.1)\n",
            "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.10/dist-packages (from qiskit) (0.3.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from qiskit) (2.8.2)\n",
            "Requirement already satisfied: stevedore>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from qiskit) (5.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qiskit) (4.12.2)\n",
            "Requirement already satisfied: symengine>=0.11 in /usr/local/lib/python3.10/dist-packages (from qiskit) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.0->qiskit) (1.16.0)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from stevedore>=3.0.0->qiskit) (6.0.0)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.3->qiskit) (1.3.0)\n",
            "Collecting pylatexenc\n",
            "  Downloading pylatexenc-2.10.tar.gz (162 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pylatexenc\n",
            "  Building wheel for pylatexenc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136816 sha256=8ac2babadf9fa2e2cd2f9e448284022ebfdfca9612d4adb83b94e009d818b948\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/31/8b/e09b0386afd80cfc556c00408c9aeea5c35c4d484a9c762fd5\n",
            "Successfully built pylatexenc\n",
            "Installing collected packages: pylatexenc\n",
            "Successfully installed pylatexenc-2.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mutual_information(self, x, y, num_ones):\n",
        "        in_dim = x.shape[1]\n",
        "        hidden_dim = 32\n",
        "\n",
        "        linear_1_weight = nn.Parameter(torch.zeros((hidden_dim, in_dim * in_dim), device=x.device))\n",
        "        linear_1_bias = nn.Parameter(torch.zeros((hidden_dim), device=x.device))\n",
        "\n",
        "        linear_2_weight = nn.Parameter(torch.zeros((hidden_dim, hidden_dim), device=x.device))\n",
        "        linear_2_bias = nn.Parameter(torch.zeros((hidden_dim), device=x.device))\n",
        "\n",
        "        linear_3_weight = nn.Parameter(torch.zeros((hidden_dim, hidden_dim), device=x.device))\n",
        "        linear_3_bias = nn.Parameter(torch.zeros((hidden_dim), device=x.device))\n",
        "\n",
        "        linear_4_weight = nn.Parameter(torch.zeros((1, hidden_dim), device=x.device))\n",
        "        linear_4_bias = nn.Parameter(torch.zeros((1), device=x.device))\n",
        "        temp_params1 = (linear_1_weight, linear_1_bias, linear_2_weight, linear_2_bias,\n",
        "                       linear_3_weight, linear_3_bias, linear_4_weight, linear_4_bias)\n",
        "\n",
        "        b = nn.Parameter(torch.zeros((1, in_dim), device=x.device))\n",
        "        W = nn.Parameter(torch.zeros((1, in_dim, in_dim), device=x.device))\n",
        "        padding1 = nn.Parameter(torch.zeros(1, device=x.device))\n",
        "        padding2 = nn.Parameter(torch.zeros(1, device=x.device))\n",
        "        temp_params2 = (b, W, padding1, padding2)\n",
        "\n",
        "        p_x_y_estimate = DVP.EnergyBasedDecoder.estimated_conditional_log_probability_a_given_b(x, y, num_ones, *temp_params1)\n",
        "        p_y_x_estimate = DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params(y, x, *temp_params2)\n",
        "        mutual_info_x_y = p_x_y_estimate - p_y_x_estimate\n",
        "\n",
        "        return mutual_info_x_y\n",
        "\n",
        "\n",
        "    def test_calculate_mutual_information_loss(self, x, y):\n",
        "      # Ensure x and y are tensors and have the same shape\n",
        "      assert x.shape == y.shape, \"x and y must have the same shape\"\n",
        "\n",
        "      # Compute joint histogram\n",
        "      joint_hist = torch.histc((x * y).float(), bins=256, min=0, max=256)\n",
        "\n",
        "      # Compute marginal histograms\n",
        "      x_hist = torch.histc(x.float(), bins=256, min=0, max=256)\n",
        "      y_hist = torch.histc(y.float(), bins=256, min=0, max=256)\n",
        "\n",
        "      # Normalize histograms to get probabilities\n",
        "      joint_prob = joint_hist / joint_hist.sum()\n",
        "      x_prob = x_hist / x_hist.sum()\n",
        "      y_prob = y_hist / y_hist.sum()\n",
        "\n",
        "      # Compute the entropies\n",
        "      H_x = -torch.sum(x_prob * torch.log(x_prob + 1e-12))\n",
        "      H_y = -torch.sum(y_prob * torch.log(y_prob + 1e-12))\n",
        "      H_xy = -torch.sum(joint_prob * torch.log(joint_prob + 1e-12))\n",
        "\n",
        "      # Compute mutual information\n",
        "      I_xy = H_x + H_y - H_xy\n",
        "\n",
        "      # Return mutual information loss (negative mutual information)\n",
        "      mutual_info_loss = -I_xy\n",
        "\n",
        "      return mutual_info_loss"
      ],
      "metadata": {
        "id": "MBHIjet2sAtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from re import X\n",
        "from math import e\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import DiscreteVariationalParameterizationsDeepV2 as DVP\n",
        "from torch.autograd.functional import vjp\n",
        "from torch.autograd.function import Function\n",
        "from QuantumSimulatorDataset import QuantumSimulationDatasetFast, generate_circuit_params\n",
        "from GibbsSampling import BatchedConditionalGibbsSampler, BatchedConditionalDoubleGibbsSampler\n",
        "\n",
        "class EmbeddingMI3(nn.Module):\n",
        "    def __init__(self, batch_size, in_dim, out_dim, num_ones):\n",
        "        super().__init__()\n",
        "        self.encoder = DVP.BoltzmannBasedEncoder(in_dim=in_dim, out_dim=out_dim)\n",
        "        self.decoder = DVP.EnergyBasedDecoder(in_dim=out_dim, out_dim=in_dim, num_ones=num_ones)\n",
        "        self.num_ones = num_ones\n",
        "        self.embedding_dynamics = DVP.EnergyBasedModelEmbeddingDynamics(dim=out_dim)\n",
        "        self.loss_func = MutualInformationLossV3.apply\n",
        "        self.embedding_sampler = BatchedConditionalGibbsSampler(batch_size=batch_size, num_samples=256, # needs to be tuned\n",
        "                                                                mixing_time=5, # seems like this can be low and still work\n",
        "                                                                joint_distribution=self.embedding_dynamics)\n",
        "        self.decoder_sampler = BatchedConditionalDoubleGibbsSampler(batch_size=batch_size, num_samples=256, # needs to be tuned\n",
        "                                                                mixing_time=24, # seems like this can be low and still work\n",
        "                                                                joint_distribution=self.decoder, dim=in_dim, num_ones=self.num_ones)\n",
        "    def test_objective_function(self, x, y):\n",
        "        w = self.encoder.encoder_sample(x).detach()\n",
        "        z = self.encoder.encoder_sample(y).detach()\n",
        "        # print(w, z, x, y)\n",
        "\n",
        "        w_tilde = self.embedding_sampler.run_batched_gibbs(z).detach()\n",
        "        x_tilde = self.decoder_sampler.run_batched_gibbs(w).detach()\n",
        "\n",
        "        return -self.loss_func(self.num_ones, *(z, y, w, x, w_tilde, x_tilde), *self.encoder.params(), *self.decoder.params(), *self.embedding_dynamics.params()), -self.test_calculate_mutual_information_loss(x, y)\n",
        "\n",
        "\n",
        "    def calculate_mutual_information(self, x, y, num_ones):\n",
        "        in_dim = x.shape[1]\n",
        "        hidden_dim = 32\n",
        "\n",
        "        linear_1_weight = nn.Parameter(torch.zeros((hidden_dim, in_dim * in_dim), device=x.device))\n",
        "        linear_1_bias = nn.Parameter(torch.zeros((hidden_dim), device=x.device))\n",
        "\n",
        "        linear_2_weight = nn.Parameter(torch.zeros((hidden_dim, hidden_dim), device=x.device))\n",
        "        linear_2_bias = nn.Parameter(torch.zeros((hidden_dim), device=x.device))\n",
        "\n",
        "        linear_3_weight = nn.Parameter(torch.zeros((hidden_dim, hidden_dim), device=x.device))\n",
        "        linear_3_bias = nn.Parameter(torch.zeros((hidden_dim), device=x.device))\n",
        "\n",
        "        linear_4_weight = nn.Parameter(torch.zeros((1, hidden_dim), device=x.device))\n",
        "        linear_4_bias = nn.Parameter(torch.zeros((1), device=x.device))\n",
        "        temp_params1 = (linear_1_weight, linear_1_bias, linear_2_weight, linear_2_bias,\n",
        "                       linear_3_weight, linear_3_bias, linear_4_weight, linear_4_bias)\n",
        "\n",
        "        b = nn.Parameter(torch.zeros((1, in_dim), device=x.device))\n",
        "        W = nn.Parameter(torch.zeros((1, in_dim, in_dim), device=x.device))\n",
        "        padding1 = nn.Parameter(torch.zeros(1, device=x.device))\n",
        "        padding2 = nn.Parameter(torch.zeros(1, device=x.device))\n",
        "        temp_params2 = (b, W, padding1, padding2)\n",
        "\n",
        "        p_x_y_estimate = DVP.EnergyBasedDecoder.estimated_conditional_log_probability_a_given_b(x, y, num_ones, *temp_params1)\n",
        "        p_y_x_estimate = DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params(y, x, *temp_params2)\n",
        "        mutual_info_x_y = p_x_y_estimate - p_y_x_estimate\n",
        "\n",
        "        return mutual_info_x_y\n",
        "\n",
        "class MutualInformationLossV3(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, *inputs):\n",
        "        num_ones = inputs[0]\n",
        "        zywx_w_tilde_ins = inputs[1:7]\n",
        "        encoder_params = inputs[7:11]\n",
        "        decoder_params = inputs[11:19]\n",
        "        embedding_params = inputs[19:27]\n",
        "\n",
        "        z, y, w, x, _, _ = zywx_w_tilde_ins\n",
        "        p_x_w_estimate = DVP.EnergyBasedDecoder.estimated_conditional_log_probability_a_given_b(x, w, num_ones, *decoder_params)\n",
        "        #p_w_x = DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params(w, x, encoder_params[0], encoder_params[1])\n",
        "        p_w_x = DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params(w, x, *encoder_params)\n",
        "        r_w_z_estimate = DVP.EnergyBasedModelEmbeddingDynamics.estimated_normalized_log_probabilities_w_given_z_params(z, w, *embedding_params)\n",
        "\n",
        "        out = p_x_w_estimate - p_w_x + r_w_z_estimate\n",
        "        ctx.num_ones = num_ones\n",
        "        ctx.save_for_backward(*zywx_w_tilde_ins, *encoder_params, *decoder_params, *embedding_params, r_w_z_estimate, out)\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        num_ones = ctx.num_ones\n",
        "        z, y, w, x, w_tilde, x_tilde = ctx.saved_tensors[0:6]\n",
        "        encoder_params = ctx.saved_tensors[6:10]\n",
        "        decoder_params = ctx.saved_tensors[10:18]\n",
        "        embedding_params = ctx.saved_tensors[18:26]\n",
        "        r_w_z = ctx.saved_tensors[26]\n",
        "        MI = ctx.saved_tensors[27]\n",
        "\n",
        "        decoder_unnormalized_probs = lambda x, w, *params: DVP.EnergyBasedDecoder.unnormalized_log_probs_a_given_b_params(num_ones, x, w, *params)\n",
        "        decoder_expected_unnormalized_probs = lambda x_tilde, w, *params: DVP.EnergyBasedDecoder.expected_unnormalized_log_probs_a_given_b(num_ones, x_tilde, w, *params)\n",
        "\n",
        "        _, decoder_grad_1 = vjp(decoder_unnormalized_probs, (x, w, *decoder_params), grad_output, create_graph=False)\n",
        "        _, decoder_grad_2 = vjp(decoder_expected_unnormalized_probs, (x_tilde, w.expand(x_tilde.shape[0], -1, -1), *decoder_params), grad_output, create_graph=False)\n",
        "\n",
        "        decoder_grad = tuple(map(lambda x, y: x - y, decoder_grad_1[2:], decoder_grad_2[2:]))\n",
        "\n",
        "        #_, encoder_grad_term_1 = vjp(DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params, (w, x, encoder_params[0], encoder_params[1]), grad_output * (MI - 1), create_graph=False)\n",
        "        _, encoder_grad_term_1 = vjp(DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params, (w, x, *encoder_params), grad_output * (MI - 1), create_graph=False)\n",
        "        encoder_grad_term_1 = encoder_grad_term_1[2:]\n",
        "\n",
        "        #_, encoder_grad_term_2 = vjp(DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params, (z, y, encoder_params[0], encoder_params[1]), grad_output * r_w_z, create_graph=False)\n",
        "        _, encoder_grad_term_2 = vjp(DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params, (z, y, *encoder_params), grad_output * r_w_z, create_graph=False)\n",
        "        encoder_grad_term_2 = encoder_grad_term_2[2:]\n",
        "\n",
        "        encoder_grad = tuple(map(lambda x, y: x + y, encoder_grad_term_1, encoder_grad_term_2))\n",
        "\n",
        "        _, embedding_grad_1 = vjp(DVP.EnergyBasedModelEmbeddingDynamics.unnormalized_log_probs_w_given_z_params, (z, w, *embedding_params), grad_output, create_graph=False)\n",
        "        _, embedding_grad_2 = vjp(DVP.EnergyBasedModelEmbeddingDynamics.expected_unnormalized_log_probs_w_given_z, (z.expand(w_tilde.shape[0], -1, -1), w_tilde, *embedding_params), grad_output, create_graph=False)\n",
        "\n",
        "        embedding_grad = tuple(map(lambda x, y: x - y, embedding_grad_1[2:], embedding_grad_2[2:]))\n",
        "\n",
        "        #print(len(encoder_grad), len(decoder_grad), len(embedding_grad))\n",
        "\n",
        "        return None, None, None, None, None, None, None, *encoder_grad, *decoder_grad, *embedding_grad\n",
        "\n",
        "def run_dim_red_process(device, state_space, embedding_space_size, batch_size=256, num_steps=1):\n",
        "\n",
        "    model = EmbeddingMI3(batch_size, state_space, embedding_space_size, num_ones=4)\n",
        "\n",
        "    # Path to the state dictionary file\n",
        "    state_dict_path = 'quantum_experiments_2/initializer.model'\n",
        "\n",
        "    # Check if the state dictionary file exists\n",
        "    if os.path.exists(state_dict_path):\n",
        "        # Load the state dictionary\n",
        "        state_dict = torch.load(state_dict_path)\n",
        "        # Get the current state dictionary of the model\n",
        "        old_state_dict = model.state_dict()\n",
        "        # Modify the state dictionary to match the embedding_space_size\n",
        "        old_state_dict['encoder.b'] = state_dict['encoder.b'][:, :embedding_space_size]\n",
        "        old_state_dict['encoder.W'] = state_dict['encoder.W'][:, :embedding_space_size, :]\n",
        "\n",
        "        # Load the adjusted state dictionary into the model\n",
        "        model.load_state_dict(old_state_dict, strict=False)\n",
        "    #else:\n",
        "        #print(f\"State dictionary file '{state_dict_path}' does not exist. Continuing without loading pre-trained weights.\")\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "    #params = generate_circuit_params(12,12)\n",
        "    params = generate_circuit_params(file_name = directory_path + 'dense_small.param')\n",
        "    dataset = QuantumSimulationDatasetFast(params, batch_size, 4, device, inverse_density=3)\n",
        "\n",
        "    for i, (final_state, initial_state) in enumerate(dataset):\n",
        "        optimizer.zero_grad()\n",
        "        loss, actual_loss = model.test_objective_function(initial_state, final_state)\n",
        "        loss = loss.mean()\n",
        "        actual_loss = actual_loss.mean()\n",
        "        loss.backward()\n",
        "        print('| Iteration', i, 'I(W,Z) > ', f\"{-loss.detach().cpu().item():,.5f}\", ' I(X,Y) > ', f\"{-actual_loss.detach().cpu().item():,.6f}\")\n",
        "        optimizer.step()\n",
        "        if i > num_steps:\n",
        "            print('Training Terminated')\n",
        "            break\n",
        "        if i % 1000 == 999:\n",
        "            torch.save(model.state_dict(), f'quantum_experiments_4/experiment_{state_space}_{embedding_space_size}_{i}.model')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print('Device Running: ', device)\n",
        "\n",
        "    experiments = [\n",
        "        (12, 2, 1024), (12, 3, 1024), (12, 4, 1024), (12, 5, 1024),\n",
        "        (12, 6, 1024), (12, 7, 1024), (12, 8, 1024), (12, 9, 1024),\n",
        "        (12, 10, 1024), (12, 11, 1024), (12, 12, 1024)\n",
        "    ]\n",
        "\n",
        "    for i, params in enumerate(experiments):\n",
        "        print('Running Experiment', i, params)\n",
        "        print('State Space of', params[0], 'to Embedding Space of', params[1])\n",
        "        run_dim_red_process(device, *params)\n"
      ],
      "metadata": {
        "id": "u6Rq3bU6sHLw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "outputId": "e038cf2c-9f6e-4e7f-9cce-d1c6e2a3731f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device Running:  cuda\n",
            "Running Experiment 0 (12, 2, 1024)\n",
            "State Space of 12 to Embedding Space of 2\n",
            "| Iteration 0 I(W,Z) >  -6.34684  I(X,Y) >  -0.845020\n",
            "| Iteration 1 I(W,Z) >  -6.35136  I(X,Y) >  -0.838123\n",
            "| Iteration 2 I(W,Z) >  -6.31572  I(X,Y) >  -0.846275\n",
            "Training Terminated\n",
            "Running Experiment 1 (12, 3, 1024)\n",
            "State Space of 12 to Embedding Space of 3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-307253ca0085>\u001b[0m in \u001b[0;36m<cell line: 196>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running Experiment'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'State Space of'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'to Embedding Space of'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mrun_dim_red_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-307253ca0085>\u001b[0m in \u001b[0;36mrun_dim_red_process\u001b[0;34m(device, state_space, embedding_space_size, batch_size, num_steps)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfinal_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactual_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_objective_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mactual_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactual_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-307253ca0085>\u001b[0m in \u001b[0;36mtest_objective_function\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mx_tilde\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_sampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_batched_gibbs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_ones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_tilde\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_tilde\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dynamics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_calculate_mutual_information_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-307253ca0085>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, *inputs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzywx_w_tilde_ins\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mp_x_w_estimate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDVP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnergyBasedDecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimated_conditional_log_probability_a_given_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_ones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mdecoder_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;31m#p_w_x = DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params(w, x, encoder_params[0], encoder_params[1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mp_w_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDVP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBoltzmannBasedEncoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconditional_log_probability_a_given_b_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mencoder_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Quantum/DiscreteVariationalParameterizationsDeepV2.py\u001b[0m in \u001b[0;36mestimated_conditional_log_probability_a_given_b\u001b[0;34m(a, b, num_ones, W1, b1, W2, b2, W3, b3, W4, b4)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mestimated_conditional_log_probability_a_given_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_ones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         return EnergyBasedDecoder.unnormalized_log_probs_a_given_b_params(num_ones, a, b, W1, b1, W2,\n\u001b[0;32m--> 442\u001b[0;31m                                                                    \u001b[0mb2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mEnergyBasedDecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_log_partition_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_ones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m                                                                                                                                                   b1, W2, b2, W3, b3, W4, b4, samples=4096)\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Quantum/DiscreteVariationalParameterizationsDeepV2.py\u001b[0m in \u001b[0;36mestimate_log_partition_function\u001b[0;34m(num_ones, b, initial_state, W1, b1, W2, b2, W3, b3, W4, b4, samples)\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;31m# these will become the importance samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m         \u001b[0mperms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermuted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mnum_ones\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m         \u001b[0mimportance_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mimportance_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m_generator.pyx\u001b[0m in \u001b[0;36mnumpy.random._generator.Generator.permuted\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_ndim_dispatcher\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   3170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3172\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_ndim_dispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3173\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VYtCQlWbXH7o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}