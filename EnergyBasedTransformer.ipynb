{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPMeg+QPYunP2XoyRdolJ5b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sonnyloweus/QuantumDynamicsAI/blob/main/EnergyBasedTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B70wgiwMLH8R",
        "outputId": "7a21aa53-f2bc-470a-aa1d-ae8f7b734e03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "['Symmetric_Exclusion_Process _Simulator.ipynb', 'Project Presentation Short.gslides', 'dense_small.param', 'quantum_simulation_data.pkl', 'Quantum_Brickworks_Circuit_Simulator.ipynb', 'DiscreteVariationalParameterizations.py', 'Screenshots', '__pycache__', 'QuantumSimulatorDataset.py', 'GibbsSampling.py', 'DiscreteVariationalParameterizationsDeepV2.py', 'DiscreteVariationalParameterizationsDeepV2.ipynb', 'EnergyBasedTransformer.ipynb', 'quantum_experiments', 'DiscreteVariationalParameterizationsDeepV3.py']\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "import sys\n",
        "directory_path = '/content/drive/MyDrive/Quantum/'\n",
        "sys.path.append('/content/drive/MyDrive/Quantum')\n",
        "\n",
        "print(os.listdir(directory_path))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install qiskit-aer\n",
        "!pip install qiskit\n",
        "!pip install pylatexenc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "u3NkfGVa1e-e",
        "outputId": "275fb468-b45a-4ab3-fa7e-ad7a08f19637"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: qiskit-aer in /usr/local/lib/python3.10/dist-packages (0.14.2)\n",
            "Requirement already satisfied: qiskit>=0.45.2 in /usr/local/lib/python3.10/dist-packages (from qiskit-aer) (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.10/dist-packages (from qiskit-aer) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-aer) (1.11.4)\n",
            "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.10/dist-packages (from qiskit-aer) (5.9.5)\n",
            "Requirement already satisfied: rustworkx>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from qiskit>=0.45.2->qiskit-aer) (0.15.1)\n",
            "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.10/dist-packages (from qiskit>=0.45.2->qiskit-aer) (1.13.0)\n",
            "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.10/dist-packages (from qiskit>=0.45.2->qiskit-aer) (0.3.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from qiskit>=0.45.2->qiskit-aer) (2.8.2)\n",
            "Requirement already satisfied: stevedore>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from qiskit>=0.45.2->qiskit-aer) (5.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qiskit>=0.45.2->qiskit-aer) (4.12.2)\n",
            "Requirement already satisfied: symengine>=0.11 in /usr/local/lib/python3.10/dist-packages (from qiskit>=0.45.2->qiskit-aer) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.0->qiskit>=0.45.2->qiskit-aer) (1.16.0)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from stevedore>=3.0.0->qiskit>=0.45.2->qiskit-aer) (6.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.3->qiskit>=0.45.2->qiskit-aer) (1.3.0)\n",
            "Requirement already satisfied: qiskit in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Requirement already satisfied: rustworkx>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from qiskit) (0.15.1)\n",
            "Requirement already satisfied: numpy<3,>=1.17 in /usr/local/lib/python3.10/dist-packages (from qiskit) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.10/dist-packages (from qiskit) (1.11.4)\n",
            "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.10/dist-packages (from qiskit) (1.13.0)\n",
            "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.10/dist-packages (from qiskit) (0.3.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from qiskit) (2.8.2)\n",
            "Requirement already satisfied: stevedore>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from qiskit) (5.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qiskit) (4.12.2)\n",
            "Requirement already satisfied: symengine>=0.11 in /usr/local/lib/python3.10/dist-packages (from qiskit) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.0->qiskit) (1.16.0)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from stevedore>=3.0.0->qiskit) (6.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.3->qiskit) (1.3.0)\n",
            "Requirement already satisfied: pylatexenc in /usr/local/lib/python3.10/dist-packages (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from re import X\n",
        "from math import e\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import DiscreteVariationalParameterizationsDeepV3 as DVP\n",
        "from torch.autograd.functional import vjp\n",
        "from torch.autograd.function import Function\n",
        "from QuantumSimulatorDataset import QuantumSimulationDatasetFast, generate_circuit_params\n",
        "from GibbsSampling import BatchedConditionalGibbsSampler, BatchedConditionalDoubleGibbsSampler\n",
        "\n",
        "class EmbeddingMI3(nn.Module):\n",
        "    def __init__(self, batch_size, in_dim, out_dim, num_ones):\n",
        "        super().__init__()\n",
        "        self.encoder = DVP.BoltzmannBasedEncoder(in_dim=in_dim, out_dim=out_dim)\n",
        "        self.decoder = DVP.EnergyBasedDecoder(in_dim=out_dim, out_dim=in_dim, num_ones=num_ones)\n",
        "        self.num_ones = num_ones\n",
        "        self.embedding_dynamics = DVP.EnergyBasedModelEmbeddingDynamics(dim=out_dim)\n",
        "        self.loss_func = MutualInformationLossV3.apply\n",
        "        self.embedding_sampler = BatchedConditionalGibbsSampler(batch_size=batch_size, num_samples=256, # needs to be tuned\n",
        "                                                                mixing_time=5, # seems like this can be low and still work\n",
        "                                                                joint_distribution=self.embedding_dynamics)\n",
        "        self.decoder_sampler = BatchedConditionalDoubleGibbsSampler(batch_size=batch_size, num_samples=256, # needs to be tuned\n",
        "                                                                mixing_time=24, # seems like this can be low and still work\n",
        "                                                                joint_distribution=self.decoder, dim=in_dim, num_ones=self.num_ones)\n",
        "    def test_objective_function(self, x, y):\n",
        "        w = self.encoder.encoder_sample(x).detach()\n",
        "        z = self.encoder.encoder_sample(y).detach()\n",
        "        # print(w, z, x, y)\n",
        "\n",
        "        w_tilde = self.embedding_sampler.run_batched_gibbs(z).detach()\n",
        "        x_tilde = self.decoder_sampler.run_batched_gibbs(w).detach()\n",
        "\n",
        "        return -self.loss_func(self.num_ones, *(z, y, w, x, w_tilde, x_tilde), *self.encoder.params(), *self.decoder.params(), *self.embedding_dynamics.params()), torch.tensor([ 0.])\n",
        "\n",
        "\n",
        "    def calculate_mutual_information(self, x, y, num_ones):\n",
        "        in_dim = x.shape[1]\n",
        "        hidden_dim = 32\n",
        "\n",
        "        linear_1_weight = nn.Parameter(torch.zeros((hidden_dim, in_dim * in_dim), device=x.device))\n",
        "        linear_1_bias = nn.Parameter(torch.zeros((hidden_dim), device=x.device))\n",
        "\n",
        "        linear_2_weight = nn.Parameter(torch.zeros((hidden_dim, hidden_dim), device=x.device))\n",
        "        linear_2_bias = nn.Parameter(torch.zeros((hidden_dim), device=x.device))\n",
        "\n",
        "        linear_3_weight = nn.Parameter(torch.zeros((hidden_dim, hidden_dim), device=x.device))\n",
        "        linear_3_bias = nn.Parameter(torch.zeros((hidden_dim), device=x.device))\n",
        "\n",
        "        linear_4_weight = nn.Parameter(torch.zeros((1, hidden_dim), device=x.device))\n",
        "        linear_4_bias = nn.Parameter(torch.zeros((1), device=x.device))\n",
        "        temp_params1 = (linear_1_weight, linear_1_bias, linear_2_weight, linear_2_bias,\n",
        "                       linear_3_weight, linear_3_bias, linear_4_weight, linear_4_bias)\n",
        "\n",
        "        b = nn.Parameter(torch.zeros((1, in_dim), device=x.device))\n",
        "        W = nn.Parameter(torch.zeros((1, in_dim, in_dim), device=x.device))\n",
        "        padding1 = nn.Parameter(torch.zeros(1, device=x.device))\n",
        "        padding2 = nn.Parameter(torch.zeros(1, device=x.device))\n",
        "        temp_params2 = (b, W, padding1, padding2)\n",
        "\n",
        "        p_x_y_estimate = DVP.EnergyBasedDecoder.estimated_conditional_log_probability_a_given_b(x, y, num_ones, *temp_params1)\n",
        "        p_y_x_estimate = DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params(y, x, *temp_params2)\n",
        "        mutual_info_x_y = p_x_y_estimate - p_y_x_estimate\n",
        "\n",
        "        return mutual_info_x_y\n",
        "\n",
        "    def test_calculate_mutual_information_loss(self, x, y):\n",
        "        # Ensure x and y are tensors and have the same shape\n",
        "        assert x.shape == y.shape, \"x and y must have the same shape\"\n",
        "\n",
        "        # Compute joint histogram\n",
        "        joint_hist = torch.histc((x * y).float(), bins=256, min=0, max=256)\n",
        "\n",
        "        # Compute marginal histograms\n",
        "        x_hist = torch.histc(x.float(), bins=256, min=0, max=256)\n",
        "        y_hist = torch.histc(y.float(), bins=256, min=0, max=256)\n",
        "\n",
        "        # Normalize histograms to get probabilities\n",
        "        joint_prob = joint_hist / joint_hist.sum()\n",
        "        x_prob = x_hist / x_hist.sum()\n",
        "        y_prob = y_hist / y_hist.sum()\n",
        "\n",
        "        # Compute the entropies\n",
        "        H_x = -torch.sum(x_prob * torch.log(x_prob + 1e-12))\n",
        "        H_y = -torch.sum(y_prob * torch.log(y_prob + 1e-12))\n",
        "        H_xy = -torch.sum(joint_prob * torch.log(joint_prob + 1e-12))\n",
        "\n",
        "        # Compute mutual information\n",
        "        I_xy = H_x + H_y - H_xy\n",
        "\n",
        "        # Return mutual information loss (negative mutual information)\n",
        "        mutual_info_loss = -I_xy\n",
        "\n",
        "        return mutual_info_loss\n",
        "\n",
        "class MutualInformationLossV3(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, *inputs):\n",
        "        num_ones = inputs[0]\n",
        "        zywx_w_tilde_ins = inputs[1:7]\n",
        "        encoder_params = inputs[7:11]\n",
        "        decoder_params = inputs[11:19]\n",
        "        embedding_params = inputs[19:27]\n",
        "\n",
        "        z, y, w, x, _, _ = zywx_w_tilde_ins\n",
        "\n",
        "        #print(x[0], '|' ,y[0])\n",
        "        #print(w[0], '|' ,z[0])\n",
        "\n",
        "        p_x_w_estimate = DVP.EnergyBasedDecoder.estimated_conditional_log_probability_a_given_b(x, w, num_ones, *decoder_params)\n",
        "        p_w_x = DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params(w, x, *encoder_params)\n",
        "        r_w_z_estimate = DVP.EnergyBasedModelEmbeddingDynamics.estimated_normalized_log_probabilities_w_given_z_params(z, w, *embedding_params)\n",
        "\n",
        "        out = p_x_w_estimate - p_w_x + r_w_z_estimate\n",
        "        ctx.num_ones = num_ones\n",
        "        ctx.save_for_backward(*zywx_w_tilde_ins, *encoder_params, *decoder_params, *embedding_params, r_w_z_estimate, out)\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        num_ones = ctx.num_ones\n",
        "        z, y, w, x, w_tilde, x_tilde = ctx.saved_tensors[0:6]\n",
        "        encoder_params = ctx.saved_tensors[6:10]\n",
        "        decoder_params = ctx.saved_tensors[10:18]\n",
        "        embedding_params = ctx.saved_tensors[18:26]\n",
        "        r_w_z = ctx.saved_tensors[26]\n",
        "        MI = ctx.saved_tensors[27]\n",
        "\n",
        "        decoder_unnormalized_probs = lambda x, w, *params: DVP.EnergyBasedDecoder.unnormalized_log_probs_a_given_b_params(num_ones, x, w, *params)\n",
        "        decoder_expected_unnormalized_probs = lambda x_tilde, w, *params: DVP.EnergyBasedDecoder.expected_unnormalized_log_probs_a_given_b(num_ones, x_tilde, w, *params)\n",
        "\n",
        "        _, decoder_grad_1 = vjp(decoder_unnormalized_probs, (x, w, *decoder_params), grad_output, create_graph=False)\n",
        "        _, decoder_grad_2 = vjp(decoder_expected_unnormalized_probs, (x_tilde, w.expand(x_tilde.shape[0], -1, -1), *decoder_params), grad_output, create_graph=False)\n",
        "\n",
        "        decoder_grad = tuple(map(lambda x, y: x - y, decoder_grad_1[2:], decoder_grad_2[2:]))\n",
        "\n",
        "        #_, encoder_grad_term_1 = vjp(DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params, (w, x, encoder_params[0], encoder_params[1]), grad_output * (MI - 1), create_graph=False)\n",
        "        _, encoder_grad_term_1 = vjp(DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params, (w, x, *encoder_params), grad_output * (MI - 1), create_graph=False)\n",
        "        encoder_grad_term_1 = encoder_grad_term_1[2:]\n",
        "\n",
        "        #_, encoder_grad_term_2 = vjp(DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params, (z, y, encoder_params[0], encoder_params[1]), grad_output * r_w_z, create_graph=False)\n",
        "        _, encoder_grad_term_2 = vjp(DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params, (z, y, *encoder_params), grad_output * r_w_z, create_graph=False)\n",
        "        encoder_grad_term_2 = encoder_grad_term_2[2:]\n",
        "\n",
        "        encoder_grad = tuple(map(lambda x, y: x + y, encoder_grad_term_1, encoder_grad_term_2))\n",
        "\n",
        "        _, embedding_grad_1 = vjp(DVP.EnergyBasedModelEmbeddingDynamics.unnormalized_log_probs_w_given_z_params, (z, w, *embedding_params), grad_output, create_graph=False)\n",
        "        _, embedding_grad_2 = vjp(DVP.EnergyBasedModelEmbeddingDynamics.expected_unnormalized_log_probs_w_given_z, (z.expand(w_tilde.shape[0], -1, -1), w_tilde, *embedding_params), grad_output, create_graph=False)\n",
        "\n",
        "        embedding_grad = tuple(map(lambda x, y: x - y, embedding_grad_1[2:], embedding_grad_2[2:]))\n",
        "\n",
        "        #print(len(encoder_grad), len(decoder_grad), len(embedding_grad))\n",
        "\n",
        "        return None, None, None, None, None, None, None, *encoder_grad, *decoder_grad, *embedding_grad\n",
        "\n",
        "def run_dim_red_process(device, state_space, embedding_space_size, batch_size=256, num_steps=10000):\n",
        "\n",
        "    model = EmbeddingMI3(batch_size, state_space, embedding_space_size, num_ones=4)\n",
        "\n",
        "    # Path to the state dictionary file\n",
        "    state_dict_path = 'quantum_experiments/initializer.model'\n",
        "    specific_dict_path = f'quantum_experiments/experiment_{state_space}_{embedding_space_size}.model'\n",
        "\n",
        "    # Check if the state dictionary file exists\n",
        "    if os.path.exists(directory_path + specific_dict_path):\n",
        "        # Load the state dictionary\n",
        "        state_dict = torch.load(state_dict_path)\n",
        "        # Get the current state dictionary of the model\n",
        "        old_state_dict = model.state_dict()\n",
        "        # Modify the state dictionary to match the embedding_space_size\n",
        "        old_state_dict['encoder.b'] = state_dict['encoder.b'][:, :embedding_space_size]\n",
        "        old_state_dict['encoder.W'] = state_dict['encoder.W'][:, :embedding_space_size, :]\n",
        "\n",
        "        # Load the adjusted state dictionary into the model\n",
        "        model.load_state_dict(old_state_dict, strict=False)\n",
        "    #else:\n",
        "        #print(f\"State dictionary file '{state_dict_path}' does not exist. Continuing without loading pre-trained weights.\")\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "    #params = generate_circuit_params(12,12)\n",
        "    params = generate_circuit_params(file_name = directory_path + 'dense_small.param')\n",
        "    dataset = QuantumSimulationDatasetFast(params, batch_size, 4, device, inverse_density=3)\n",
        "\n",
        "    for i, (final_state, initial_state) in enumerate(dataset):\n",
        "        optimizer.zero_grad()\n",
        "        loss, actual_loss = model.test_objective_function(initial_state, final_state)\n",
        "        loss = loss.mean()\n",
        "        actual_loss = actual_loss.mean()\n",
        "        loss.backward()\n",
        "        print('| Iteration', i, 'I(W,Z) > ', f\"{-loss.detach().cpu().item():,.5f}\", ' I(X,Y) > ', f\"{-actual_loss.detach().cpu().item():,.6f}\")\n",
        "        optimizer.step()\n",
        "        if i > num_steps:\n",
        "            print('Training Terminated')\n",
        "            break\n",
        "        if i % 1000 == 999:\n",
        "            torch.save(model.state_dict(), f'quantum_experiments/experiment_{state_space}_{embedding_space_size}_{i}.model')\n",
        "\n",
        "    # Save the model at the end of training\n",
        "    final_save_path = f'quantum_experiments/experiment_{state_space}_{embedding_space_size}_v1.model'\n",
        "    torch.save(model.state_dict(), final_save_path)\n",
        "    print(f\"Final model saved to {final_save_path}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print('Device Running: ', device)\n",
        "\n",
        "    # experiments = [\n",
        "    #     (12, 2, 1024), (12, 3, 1024), (12, 4, 1024), (12, 5, 1024),\n",
        "    #     (12, 6, 1024), (12, 7, 1024), (12, 8, 1024), (12, 9, 1024),\n",
        "    #     (12, 10, 1024), (12, 11, 1024), (12, 12, 1024)\n",
        "    # ]\n",
        "\n",
        "    experiments = [\n",
        "        (12, 12, 512)\n",
        "    ]\n",
        "\n",
        "    for i, params in enumerate(experiments):\n",
        "        print('Running Experiment', i, params)\n",
        "        print('State Space of', params[0], 'to Embedding Space of', params[1])\n",
        "        run_dim_red_process(device, *params)\n"
      ],
      "metadata": {
        "id": "u6Rq3bU6sHLw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "56cf9751-34c6-47ac-c8b8-86c58473b086"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'torch.nn' has no attribute 'DModule'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-436c8046f2a7>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mDiscreteVariationalParameterizationsDeepV3\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mDVP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvjp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Quantum/DiscreteVariationalParameterizationsDeepV3.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mrng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mEnergyBasedModelEmbeddingDynamics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'DModule'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VYtCQlWbXH7o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}