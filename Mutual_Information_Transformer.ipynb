{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "q4ns8pdIBhQb",
        "7lOv4H9iBVGJ"
      ],
      "authorship_tag": "ABX9TyO2+HhMgcSkau+jZyzzWrPw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sonnyloweus/QuantumDynamicsAI/blob/main/Mutual_Information_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# File Setup"
      ],
      "metadata": {
        "id": "q4ns8pdIBhQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "directory_path = '/content/drive/MyDrive/Quantum/'\n",
        "sys.path.append('/content/drive/MyDrive/Quantum')\n",
        "\n",
        "print(os.listdir(directory_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j10YqJU051l8",
        "outputId": "72930d25-1de5-4e9d-c619-e29b6099f1a9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "['Symmetric_Exclusion_Process _Simulator.ipynb', 'Project Presentation Short.gslides', 'dense_small.param', 'quantum_simulation_data.pkl', 'Quantum_Brickworks_Circuit_Simulator.ipynb', 'DiscreteVariationalParameterizations.py', 'Screenshots', '__pycache__', 'QuantumSimulatorDataset.py', 'GibbsSampling.py', 'DiscreteVariationalParameterizationsDeepV2.py', 'DiscreteVariationalParameterizationsDeepV2.ipynb', 'quantum_experiments', 'DiscreteVariationalParameterizationsDeepV3.py', 'Mutual_Information_Maximizing_Model.ipynb', 'Mutual_Information_Transformer.ipynb']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install qiskit-aer\n",
        "!pip install qiskit\n",
        "!pip install pylatexenc\n",
        "!pip install tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "d19Pk1nd54_V",
        "outputId": "fb62d185-755b-4169-968c-217c529563b7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: qiskit-aer in /usr/local/lib/python3.10/dist-packages (0.14.2)\n",
            "Requirement already satisfied: qiskit>=0.45.2 in /usr/local/lib/python3.10/dist-packages (from qiskit-aer) (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.10/dist-packages (from qiskit-aer) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-aer) (1.11.4)\n",
            "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.10/dist-packages (from qiskit-aer) (5.9.5)\n",
            "Requirement already satisfied: rustworkx>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from qiskit>=0.45.2->qiskit-aer) (0.15.1)\n",
            "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.10/dist-packages (from qiskit>=0.45.2->qiskit-aer) (1.13.0)\n",
            "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.10/dist-packages (from qiskit>=0.45.2->qiskit-aer) (0.3.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from qiskit>=0.45.2->qiskit-aer) (2.8.2)\n",
            "Requirement already satisfied: stevedore>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from qiskit>=0.45.2->qiskit-aer) (5.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qiskit>=0.45.2->qiskit-aer) (4.12.2)\n",
            "Requirement already satisfied: symengine>=0.11 in /usr/local/lib/python3.10/dist-packages (from qiskit>=0.45.2->qiskit-aer) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.0->qiskit>=0.45.2->qiskit-aer) (1.16.0)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from stevedore>=3.0.0->qiskit>=0.45.2->qiskit-aer) (6.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.3->qiskit>=0.45.2->qiskit-aer) (1.3.0)\n",
            "Requirement already satisfied: qiskit in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Requirement already satisfied: rustworkx>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from qiskit) (0.15.1)\n",
            "Requirement already satisfied: numpy<3,>=1.17 in /usr/local/lib/python3.10/dist-packages (from qiskit) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.10/dist-packages (from qiskit) (1.11.4)\n",
            "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.10/dist-packages (from qiskit) (1.13.0)\n",
            "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.10/dist-packages (from qiskit) (0.3.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from qiskit) (2.8.2)\n",
            "Requirement already satisfied: stevedore>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from qiskit) (5.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qiskit) (4.12.2)\n",
            "Requirement already satisfied: symengine>=0.11 in /usr/local/lib/python3.10/dist-packages (from qiskit) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.0->qiskit) (1.16.0)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from stevedore>=3.0.0->qiskit) (6.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.3->qiskit) (1.3.0)\n",
            "Requirement already satisfied: pylatexenc in /usr/local/lib/python3.10/dist-packages (2.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from QuantumSimulatorDataset import QuantumSimulationDatasetFast, generate_circuit_params\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import ast\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "0il2PJTIXtg6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing"
      ],
      "metadata": {
        "id": "7lOv4H9iBVGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and prepare the data\n",
        "data_path = '/content/drive/MyDrive/Quantum/quantum_simulation_data.pkl'\n",
        "data = pd.read_pickle(data_path)\n",
        "\n",
        "def to_tensor(item):\n",
        "    if isinstance(item, torch.Tensor):\n",
        "        # If already a tensor, clone and detach it to prevent issues\n",
        "        return item.clone().detach()\n",
        "    elif isinstance(item, str):\n",
        "        # Convert string to list using ast.literal_eval\n",
        "        item_list = ast.literal_eval(item)\n",
        "        return torch.tensor(item_list)\n",
        "    elif isinstance(item, list):\n",
        "        return torch.tensor(item)\n",
        "    else:\n",
        "        raise ValueError(f\"Expected a list or tensor, but got {type(item)}\")\n",
        "\n",
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbEmyWJ-BRcc",
        "outputId": "1bbd7db6-f63b-4750-f464-02f7151151f9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64000, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['Initial_State'] = [to_tensor(lst) for lst in data['Initial_State']]\n",
        "data['Final_State'] = [to_tensor(lst) for lst in data['Final_State']]\n",
        "\n",
        "print(data['Final_State'][0])\n",
        "\n",
        "# Convert Series to list of tensors before stacking\n",
        "initial_state_tensors = list(data['Initial_State'])\n",
        "final_state_tensors = list(data['Final_State'])\n",
        "\n",
        "dataset = TensorDataset(torch.stack(initial_state_tensors), torch.stack(final_state_tensors))\n",
        "print(dataset)\n",
        "\n",
        "# Define the split sizes (e.g., 80% train, 20% test)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "# Perform the train-test split\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Example of how to use DataLoader for training and testing\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(initial_state_tensors, final_state_tensors, test_size=0.2, random_state=42)\n",
        "# train_loader = torch.utils.data.DataLoader(list(zip(X_train, y_train)), batch_size=32, shuffle=False)\n",
        "# test_loader = torch.utils.data.DataLoader(list(zip(X_test, y_test)), batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXw4nTGMBSck",
        "outputId": "a4a0e731-38ab-4a2a-979b-84e65d2e64c7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1.])\n",
            "<torch.utils.data.dataset.TensorDataset object at 0x7df4c7acdea0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mutual Information Loss"
      ],
      "metadata": {
        "id": "WVcbgeweEgzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mutual_information_loss(x, y):\n",
        "    # Ensure x and y are tensors and have the same shape\n",
        "    assert x.shape == y.shape, \"x and y must have the same shape\"\n",
        "\n",
        "    # Compute joint histogram\n",
        "    joint_hist = torch.histc((x * y).float(), bins=256, min=0, max=256)\n",
        "\n",
        "    # Compute marginal histograms\n",
        "    x_hist = torch.histc(x.float(), bins=256, min=0, max=256)\n",
        "    y_hist = torch.histc(y.float(), bins=256, min=0, max=256)\n",
        "\n",
        "    # Normalize histograms to get probabilities\n",
        "    joint_prob = joint_hist / joint_hist.sum()\n",
        "    x_prob = x_hist / x_hist.sum()\n",
        "    y_prob = y_hist / y_hist.sum()\n",
        "\n",
        "    # Compute the entropies\n",
        "    H_x = -torch.sum(x_prob * torch.log(x_prob + 1e-12))\n",
        "    H_y = -torch.sum(y_prob * torch.log(y_prob + 1e-12))\n",
        "    H_xy = -torch.sum(joint_prob * torch.log(joint_prob + 1e-12))\n",
        "\n",
        "    # Compute mutual information\n",
        "    I_xy = H_x + H_y - H_xy\n",
        "\n",
        "    # Return mutual information loss (negative mutual information)\n",
        "    mutual_info_loss = -I_xy\n",
        "\n",
        "    return mutual_info_loss"
      ],
      "metadata": {
        "id": "NbG02hvNEgJQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition"
      ],
      "metadata": {
        "id": "j87DiZJLfiXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QuantumTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, num_heads, output_dim):\n",
        "        super(QuantumTransformer, self).__init__()\n",
        "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=hidden_dim,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, src):\n",
        "        src_emb = self.embedding(src).unsqueeze(0)  # Add sequence dimension\n",
        "        transformer_out = self.transformer(src_emb, src_emb).squeeze(0)  # Remove sequence dimension\n",
        "        out = self.fc(transformer_out)\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "    def predict_binary(self, src):\n",
        "        outputs = self.forward(src)\n",
        "        binary_outputs = (outputs > 0.5).float()  # Convert probabilities to binary\n",
        "        return binary_outputs"
      ],
      "metadata": {
        "id": "QkPRHQTkflC-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Initialization and Training"
      ],
      "metadata": {
        "id": "UKRswzQ1BmyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 1\n",
        "# Instantiate the model\n",
        "input_dim = 12\n",
        "hidden_dim = 64\n",
        "num_layers = 4\n",
        "num_heads = 8\n",
        "output_dim = 12\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = QuantumTransformer(input_dim, hidden_dim, num_layers, num_heads, output_dim)\n",
        "print('Device Running: ', device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJHfp3CDA-0N",
        "outputId": "1335a2d1-b855-436a-b96c-32f012c5a772"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device Running:  cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=10)\n",
        "\n",
        "losses = []\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for idx, (inputs, targets) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Calculate binary accuracy\n",
        "        predicted = (outputs > 0.5).float()\n",
        "        correct_predictions += (predicted == targets).sum().item()\n",
        "        total_samples += targets.numel()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    losses.append(avg_loss)\n",
        "    binary_accuracy = correct_predictions / total_samples\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.10f}, Binary Accuracy: {binary_accuracy:.10f}')\n",
        "\n",
        "\n",
        "# If you want to include mutual information calculations\n",
        "    # if idx % 100 == 0:\n",
        "    #     mi_dynamic_loss = mutual_information_loss(inputs, outputs).mean()\n",
        "    #     expected_mi_dynamic_loss = mutual_information_loss(inputs, targets).mean()\n",
        "    #     mi_final_loss = mutual_information_loss(targets, outputs).mean()\n",
        "    #     print(' Iteration', idx, 'I(x,y) >', f\"{expected_mi_dynamic_loss:,.5f}\", \\\n",
        "    #           ' I(x̄,y) >', f\"{mi_dynamic_loss:,.5f}\", \\\n",
        "    #           ' I(x,x̄) >', f\"{mi_final_loss:,.5f}\", \\\n",
        "    #           ' Loss > ', f\"{loss:,.5f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "-tTWaujEBEV4",
        "outputId": "eb4a830d-0597-4463-8fa2-4235255cf836"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|██████████| 1600/1600 [02:59<00:00,  8.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 49.9350431339, Binary Accuracy: 0.5003206380\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10:  70%|███████   | 1120/1600 [02:36<01:06,  7.18it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-cfdc46ba15e7>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "losses_np = np.array(losses)\n",
        "\n",
        "# Plot the loss after training\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, len(losses_np)+1), losses_np, marker='o')\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.xticks(range(1, len(losses_np)))\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "twCAozYjS4J9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing loop\n",
        "model.eval()\n",
        "total_loss = 0.0\n",
        "predictions = []\n",
        "given = []\n",
        "actual = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx, (inputs, targets) in enumerate(tqdm(test_loader, desc=f'Epoch {epoch+1}/{epochs}')):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        outputs = model(inputs.long(), targets.long())\n",
        "        loss = criterion(outputs, targets.float())\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        for i in range(len(inputs)):\n",
        "            predictions.append(outputs[i].tolist())\n",
        "            given.append(inputs[i].tolist())\n",
        "            actual.append(targets[i].tolist())\n",
        "\n",
        "    avg_test_loss = total_loss / len(test_loader)\n",
        "    print(f'Test Loss: {avg_test_loss:.4f}')\n",
        "\n",
        "# Save the model and predictions\n",
        "torch.save(model.state_dict(), 'quantum_transformer_model.pth')\n",
        "preds = {\n",
        "    'Initial_States': given,\n",
        "    'Final_State_Pred': predictions,\n",
        "    'Final_States': actual\n",
        "}\n",
        "final_df = pd.DataFrame(preds)\n",
        "final_df.head()"
      ],
      "metadata": {
        "id": "d2BA8KDbP0E5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QuantumTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, num_heads, output_dim):\n",
        "        super(QuantumTransformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(2, hidden_dim)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=hidden_dim,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src shape: [batch_size, seq_len]\n",
        "        batch_size, seq_len = src.shape\n",
        "        src_emb = self.embedding(src)  # shape: [batch_size, seq_len, hidden_dim]\n",
        "        # print(\"src_emb shape: \", src_emb.shape)\n",
        "\n",
        "        pos_enc = self.position_encoding[:, :seq_len, :] # shape: [1, seq_len, hidden_dim]\n",
        "        # print(\"pos_enc shape: \", pos_enc.shape)\n",
        "\n",
        "        src_emb = src_emb + pos_enc\n",
        "\n",
        "        # Permute for Transformer\n",
        "        src_emb = src_emb.permute(1, 0, 2)  # shape: [seq_len, batch_size, hidden_dim]\n",
        "        # print(\"permuted shape: \", src_emb.shape)\n",
        "\n",
        "        transformer_out = self.transformer(src_emb, src_emb)  # Remove sequence dimension\n",
        "        final_hidden_state = transformer_out[-1]\n",
        "\n",
        "        out = self.fc(final_hidden_state)\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "\n",
        "    def predict_binary(self, src):\n",
        "        outputs = self.forward(src)\n",
        "        binary_outputs = (outputs > 0.5).float()  # Convert probabilities to binary\n",
        "        return binary_outputs"
      ],
      "metadata": {
        "id": "7GDRkMZNegfh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}