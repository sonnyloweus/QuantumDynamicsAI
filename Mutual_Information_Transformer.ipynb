{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "q4ns8pdIBhQb",
    "7lOv4H9iBVGJ"
   ],
   "authorship_tag": "ABX9TyO2+HhMgcSkau+jZyzzWrPw",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# File Setup"
   ],
   "metadata": {
    "id": "q4ns8pdIBhQb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "print(os.listdir())\n",
    "directory_path = ''\n",
    "\n",
    "#from google.colab import drive\n",
    "#import sys\n",
    "#drive.mount('/content/drive')\n",
    "#directory_path = '/content/drive/MyDrive/Quantum/'\n",
    "#sys.path.append('/content/drive/MyDrive/Quantum')\n",
    "# print(os.listdir(directory_path))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j10YqJU051l8",
    "outputId": "72930d25-1de5-4e9d-c619-e29b6099f1a9",
    "ExecuteTime": {
     "end_time": "2024-07-18T20:06:00.067969Z",
     "start_time": "2024-07-18T20:06:00.063769Z"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dense_small.param', '.DS_Store', 'quantum_experiments', 'DiscreteVariationalParameterizationsDeepV2.py', 'Mutual_Information_Transformer.ipynb', '__pycache__', 'DiscreteVariationalParameterizationsDeepV3.py', 'README.md', 'Mutual_Information_Maximizing_Model.ipynb', 'temp.txt', '.ipynb_checkpoints', '.git', 'QuantumSimulatorDataset.py', 'GibbsSampling.py', '.idea']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install torch\n",
    "!pip install torchmetrics\n",
    "!pip install qiskit-aer\n",
    "!pip install qiskit\n",
    "!pip install pylatexenc\n",
    "!pip install tqdm\n",
    "!pip install scikit-learn"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "d19Pk1nd54_V",
    "outputId": "fb62d185-755b-4169-968c-217c529563b7",
    "ExecuteTime": {
     "end_time": "2024-07-18T19:45:28.664798Z",
     "start_time": "2024-07-18T19:45:16.293399Z"
    }
   },
   "execution_count": 221,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.11/site-packages (2.3.1)\r\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.11/site-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1.3)\r\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.11/site-packages (from torch) (2023.10.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\r\n",
      "Collecting torchmetrics\r\n",
      "  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl.metadata (19 kB)\r\n",
      "Requirement already satisfied: numpy>1.20.0 in /opt/anaconda3/lib/python3.11/site-packages (from torchmetrics) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>17.1 in /opt/anaconda3/lib/python3.11/site-packages (from torchmetrics) (23.1)\r\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/anaconda3/lib/python3.11/site-packages (from torchmetrics) (2.3.1)\r\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\r\n",
      "  Downloading lightning_utilities-0.11.5-py3-none-any.whl.metadata (4.7 kB)\r\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (68.2.2)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.11/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.9.0)\r\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\r\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\r\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (2023.10.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\r\n",
      "Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m868.8/868.8 kB\u001B[0m \u001B[31m2.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0mm\r\n",
      "\u001B[?25hDownloading lightning_utilities-0.11.5-py3-none-any.whl (26 kB)\r\n",
      "Installing collected packages: lightning-utilities, torchmetrics\r\n",
      "Successfully installed lightning-utilities-0.11.5 torchmetrics-1.4.0.post0\r\n",
      "Requirement already satisfied: qiskit-aer in /opt/anaconda3/lib/python3.11/site-packages (0.14.2)\r\n",
      "Requirement already satisfied: qiskit>=0.45.2 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit-aer) (1.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.16.3 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit-aer) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit-aer) (1.11.4)\r\n",
      "Requirement already satisfied: psutil>=5 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit-aer) (5.9.0)\r\n",
      "Requirement already satisfied: rustworkx>=0.14.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (0.14.2)\r\n",
      "Requirement already satisfied: sympy>=1.3 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (1.12)\r\n",
      "Requirement already satisfied: dill>=0.3 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (0.3.7)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (2.8.2)\r\n",
      "Requirement already satisfied: stevedore>=3.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (5.2.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (4.9.0)\r\n",
      "Requirement already satisfied: symengine>=0.11 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (0.11.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.0->qiskit>=0.45.2->qiskit-aer) (1.16.0)\r\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from stevedore>=3.0.0->qiskit>=0.45.2->qiskit-aer) (6.0.0)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy>=1.3->qiskit>=0.45.2->qiskit-aer) (1.3.0)\r\n",
      "Requirement already satisfied: qiskit in /opt/anaconda3/lib/python3.11/site-packages (1.1.0)\r\n",
      "Requirement already satisfied: rustworkx>=0.14.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (0.14.2)\r\n",
      "Requirement already satisfied: numpy<3,>=1.17 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (1.11.4)\r\n",
      "Requirement already satisfied: sympy>=1.3 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (1.12)\r\n",
      "Requirement already satisfied: dill>=0.3 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (0.3.7)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (2.8.2)\r\n",
      "Requirement already satisfied: stevedore>=3.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (5.2.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (4.9.0)\r\n",
      "Requirement already satisfied: symengine>=0.11 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (0.11.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.0->qiskit) (1.16.0)\r\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from stevedore>=3.0.0->qiskit) (6.0.0)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy>=1.3->qiskit) (1.3.0)\r\n",
      "Requirement already satisfied: pylatexenc in /opt/anaconda3/lib/python3.11/site-packages (2.10)\r\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (4.65.0)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.11/site-packages (1.2.2)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.11.4)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.2.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (2.2.0)\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import ast\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.clustering import MutualInfoScore\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from QuantumSimulatorDataset import QuantumSimulationDatasetFast, generate_circuit_params"
   ],
   "metadata": {
    "id": "0il2PJTIXtg6",
    "ExecuteTime": {
     "end_time": "2024-07-18T20:06:04.829879Z",
     "start_time": "2024-07-18T20:06:02.590527Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MI and Entropy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def entropy(X):\n",
    "    # Flatten the tensor to 1D\n",
    "    X_flat = X.view(-1)\n",
    "    \n",
    "    # Count the occurrences of each unique value\n",
    "    unique_vals, counts = X_flat.unique(return_counts=True)\n",
    "    probabilities = counts.float() / counts.sum()\n",
    "    entropy = -torch.sum(probabilities * torch.log(probabilities))\n",
    "        \n",
    "    return entropy.item()\n",
    "\n",
    "def mutual_info_loss(X, Y):\n",
    "    # Flatten the tensor to 1D\n",
    "    X_flat = X.view(-1)\n",
    "    Y_flat = Y.view(-1)\n",
    "    \n",
    "    return -mutual_info_score(X_flat, Y_flat)\n",
    "\n",
    "class MutualInformationLoss(nn.Module):\n",
    "    def forward(self, input, target):\n",
    "        X_flat = input.view(-1)\n",
    "        Y_flat = target.view(-1)\n",
    "        \n",
    "        # Compute mutual information\n",
    "        mi = mutual_info_score(X_flat, Y_flat)\n",
    "    \n",
    "        # Convert mutual information to a tensor and return the negative as loss\n",
    "        mi_tensor = torch.tensor(mi, dtype=torch.float32, requires_grad=True).to(input.device)\n",
    "        return -mi_tensor\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-18T20:23:48.098901Z",
     "start_time": "2024-07-18T20:23:48.082046Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Definition"
   ],
   "metadata": {
    "id": "j87DiZJLfiXR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.heads = heads\n",
    "        # distribution of dimensionaltiy\n",
    "        self.head_dim = d_model // heads\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, d_model) # concat all heads back into one\n",
    "\n",
    "        # binary output\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        query = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(query)\n",
    "\n",
    "        # computing attention scores\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [query, keys])\n",
    "        # masking\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        # attention weights\n",
    "        attention = torch.softmax(energy / (self.d_model ** (1/2)), dim=3)\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(N, query_len, self.heads * self.head_dim)\n",
    "        out = self.fc_out(out)\n",
    "        out = self.sigmoid(out) # output is within range 0, 1\n",
    "\n",
    "        out_binary = (out >= 0.5).float()\n",
    "\n",
    "        return out_binary\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        # attention block, normalization, feedforward, normalization again (mimicing GPT)\n",
    "        self.attention = SelfAttention(d_model, heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, forward_expansion * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * d_model, d_model),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "\n",
    "        # skip connection\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, device, input_dim, d_model, heads, dropout, num_layers, forward_expansion):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.device = device\n",
    "        self.num_tokens = 2 # binary\n",
    "\n",
    "        self.embedding = nn.Embedding(self.num_tokens, d_model)\n",
    "        self.position_embedding = nn.Embedding(input_dim, d_model)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [TransformerBlock(d_model, heads, dropout, forward_expansion)\n",
    "            for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        N, seq_len = x.shape\n",
    "        positions = torch.arange(0, seq_len).expand(N, seq_len).to(self.device)\n",
    "        out = self.dropout(self.embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask) # encoder has same keys, queries, and values\n",
    "\n",
    "        #print('Encoder Out:', out.shape)\n",
    "\n",
    "        return out\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, device, d_model, heads, dropout, forward_expansion):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.attention = SelfAttention(d_model, heads)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.transformer_block = TransformerBlock(d_model, heads, dropout, forward_expansion)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key, src_mask, tgt_mask):\n",
    "        attention = self.attention(x, x, x, tgt_mask)\n",
    "        query = self.dropout(self.norm(attention + x)) # skip connection\n",
    "        out = self.transformer_block(value, key, query, src_mask)\n",
    "        return out\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, device, output_dim, d_model, heads, dropout, num_layers, forward_expansion, num_ones):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.num_tokens = 2 # binary\n",
    "        self.device = device\n",
    "        self.num_ones = num_ones\n",
    "        self.word_embedding = nn.Embedding(self.num_tokens, d_model)\n",
    "        self.position_embedding = nn.Embedding(output_dim, d_model)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [DecoderBlock(device, d_model, heads, dropout, forward_expansion)\n",
    "             for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # binary output\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    # x is input into decoder\n",
    "    def forward(self, x, enc_out, src_mask, tgt_mask):\n",
    "        N, seq_len = x.shape\n",
    "        positions = torch.arange(0, seq_len).expand(N, seq_len).to(self.device)\n",
    "        x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, enc_out, src_mask, tgt_mask)\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "        out = self.sigmoid(out)  # Output is now within range 0, 1\n",
    "        \n",
    "        mode_tensor, _ = out.mode(dim=1)\n",
    "        # Flatten the mode_tensor to get a single tensor of 12 elements\n",
    "        flattened_mode_tensor = mode_tensor.view(out.size(0), out.size(2))\n",
    "        \n",
    "        # out_binary = (out >= 0.5).float()  # Convert to binary if needed\n",
    "        out_binary = self.conservation_ones_process(flattened_mode_tensor)\n",
    "\n",
    "        return out_binary\n",
    "    \n",
    "    def conservation_ones_process(self, out):\n",
    "        sorted_indices = torch.argsort(out, dim=1, descending=True)\n",
    "        threshold_output = torch.zeros_like(out)\n",
    "    \n",
    "        for j in range(out.size(0)):\n",
    "            threshold_output[j, sorted_indices[j, :self.num_ones]] = 1\n",
    "        \n",
    "        return threshold_output\n",
    "\n",
    "class QuantumTransformer(nn.Module):\n",
    "    def __init__(self, src_pad_idx, tgt_pad_idx, d_model, heads, dropout, forward_expansion, num_layers, input_dim, output_dim, num_ones):\n",
    "        super(QuantumTransformer, self).__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.encoder = Encoder(self.device, input_dim, d_model, heads, dropout, num_layers, forward_expansion)\n",
    "        self.decoder = Decoder(self.device, output_dim, d_model, heads, dropout, num_layers, forward_expansion, num_ones)\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.tgt_pad_idx = tgt_pad_idx\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # source mask is N,1,1,src_len\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def make_tgt_mask(self, tgt):\n",
    "        N, tgt_len = tgt.shape\n",
    "        # triangular matrix (not raw attention weights)\n",
    "        tgt_mask = torch.tril(torch.ones((tgt_len, tgt_len))).expand(\n",
    "            N, 1, tgt_len, tgt_len\n",
    "        )\n",
    "        return tgt_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        tgt_mask = self.make_tgt_mask(tgt)\n",
    "        enc_src = self.encoder(src.long(), src_mask)\n",
    "        out = self.decoder(tgt.long(), enc_src, src_mask, tgt_mask)\n",
    "        loss = F.binary_cross_entropy(out, tgt)\n",
    "        return out, loss"
   ],
   "metadata": {
    "id": "QkPRHQTkflC-",
    "ExecuteTime": {
     "end_time": "2024-07-18T20:59:14.042105Z",
     "start_time": "2024-07-18T20:59:14.039781Z"
    }
   },
   "execution_count": 111,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Initialization and Training"
   ],
   "metadata": {
    "id": "UKRswzQ1BmyZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Version 1\n",
    "src_pad_idx = 0\n",
    "tgt_pad_idx = 0\n",
    "d_model = 256\n",
    "heads = 4\n",
    "dropout = 0.1\n",
    "forward_expansion = 4\n",
    "num_layers = 6\n",
    "input_dim = 12\n",
    "output_dim = 12\n",
    "inverse_density = 3\n",
    "num_ones = int(input_dim/inverse_density)\n",
    "num_iterations = 1000\n",
    "batch_size = 128\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = QuantumTransformer(src_pad_idx, tgt_pad_idx, d_model, heads, dropout, forward_expansion, num_layers, input_dim, output_dim, num_ones)\n",
    "model = model.to(device)\n",
    "print('Device:', device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oJHfp3CDA-0N",
    "outputId": "1335a2d1-b855-436a-b96c-32f012c5a772",
    "ExecuteTime": {
     "end_time": "2024-07-18T20:59:15.193881Z",
     "start_time": "2024-07-18T20:59:15.167408Z"
    }
   },
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "# criterion = MutualInformationLoss()\n",
    "# criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "#for name, param in model.named_parameters():\n",
    "#    print(f\"Name: {name}, Shape: {param.shape}, Requires grad: {param.requires_grad}\")\n",
    "\n",
    "params = generate_circuit_params(0,input_dim)\n",
    "#params = generate_circuit_params(file_name = directory_path + 'dense_small.param')\n",
    "dataset = QuantumSimulationDatasetFast(params, batch_size, 4, device, inverse_density=inverse_density)\n",
    "\n",
    "BCElosses = []\n",
    "MI = []"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-18T20:59:16.265391Z",
     "start_time": "2024-07-18T20:59:16.162315Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "tensor(44.7917, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Model parameters involved in loss: False\n",
      "| Iteration 0  > Loss(Y,ȳ): 44.79167  I(X,ȳ): 0.00000  I(X,Y): 0.00000  H(X): 0.00000\n",
      "tensor(44.6615, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Model parameters involved in loss: False\n",
      "tensor(42.7083, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Model parameters involved in loss: False\n",
      "tensor(40.3646, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Model parameters involved in loss: False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[116], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTraining\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      4\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m idx, (initial_state, final_state) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataset):\n\u001B[1;32m      6\u001B[0m     initial_state, final_state \u001B[38;5;241m=\u001B[39m initial_state\u001B[38;5;241m.\u001B[39mto(device), final_state\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m      7\u001B[0m     final_state\u001B[38;5;241m.\u001B[39mrequires_grad \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/QuantumDynamicsAI/QuantumSimulatorDataset.py:188\u001B[0m, in \u001B[0;36mQuantumSimulationDatasetFast.__getitem__\u001B[0;34m(self, _)\u001B[0m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, _):\n\u001B[0;32m--> 188\u001B[0m     initial_states, final_states \u001B[38;5;241m=\u001B[39m run_batch_dense_torch(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmat, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_qubits, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_size,\n\u001B[1;32m    189\u001B[0m                                                          \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_samples, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minverse_density)\n\u001B[1;32m    190\u001B[0m     initial_states \u001B[38;5;241m=\u001B[39m initial_states\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mexpand_as(final_states)\u001B[38;5;241m.\u001B[39mclone()\n\u001B[1;32m    191\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m initial_states\u001B[38;5;241m.\u001B[39mflatten(end_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m), final_states\u001B[38;5;241m.\u001B[39mflatten(end_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/Desktop/QuantumDynamicsAI/QuantumSimulatorDataset.py:170\u001B[0m, in \u001B[0;36mrun_batch_dense_torch\u001B[0;34m(unitary_circuit, num_qbits, batch_size, num_samples, device, inverse_density)\u001B[0m\n\u001B[1;32m    168\u001B[0m decimal \u001B[38;5;241m=\u001B[39m decimal_from_initial_state(bits, num_qbits)\n\u001B[1;32m    169\u001B[0m one_hot \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mone_hot(decimal, num_classes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m num_qbits)\n\u001B[0;32m--> 170\u001B[0m final_state \u001B[38;5;241m=\u001B[39m get_final_state_vector(one_hot\u001B[38;5;241m.\u001B[39mcfloat(), unitary_circuit)\n\u001B[1;32m    171\u001B[0m sample \u001B[38;5;241m=\u001B[39m sample_from_state_vector(final_state, num_qbits, num_samples)\n\u001B[1;32m    172\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m bits\u001B[38;5;241m.\u001B[39mfloat(), sample\n",
      "File \u001B[0;32m~/Desktop/QuantumDynamicsAI/QuantumSimulatorDataset.py:139\u001B[0m, in \u001B[0;36mget_final_state_vector\u001B[0;34m(batched_initial_state, op_mat)\u001B[0m\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_final_state_vector\u001B[39m(batched_initial_state, op_mat):\n\u001B[0;32m--> 139\u001B[0m     sampled_final_state \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmatmul(op_mat\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m), batched_initial_state\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m2\u001B[39m))\n\u001B[1;32m    140\u001B[0m     probs \u001B[38;5;241m=\u001B[39m sampled_final_state \u001B[38;5;241m*\u001B[39m sampled_final_state\u001B[38;5;241m.\u001B[39mconj()\n\u001B[1;32m    141\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mreal(probs)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from torch.autograd import grad\n",
    "\n",
    "print('Training')\n",
    "model.train()\n",
    "for idx, (initial_state, final_state) in enumerate(dataset):\n",
    "    initial_state, final_state = initial_state.to(device), final_state.to(device)\n",
    "    final_state.requires_grad = True\n",
    "    \n",
    "    out, loss = model(initial_state, final_state)\n",
    "    optimizer.zero_grad(set_to_none=True)    \n",
    "    #mutual_info = mutual_info_score(initial_state.detach().cpu().view(-1), out.detach().cpu().view(-1))\n",
    "    #expected_mutual_info = mutual_info_score(initial_state.detach().cpu().view(-1), final_state.detach().cpu().view(-1))\n",
    "    #initial_entropy = entropy(initial_state)\n",
    "    mutual_info = 0\n",
    "    expected_mutual_info = 0\n",
    "    initial_entropy = 0\n",
    "    \n",
    "    print(loss)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    print(\"Model parameters involved in loss:\", any(param.grad is not None for param in model.parameters()))\n",
    "            \n",
    "    #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    #BCElosses.append(loss.detach().item())\n",
    "    #MI.append(mutual_info)\n",
    "\n",
    "    if idx % 10 == 0:\n",
    "        print('| Iteration', idx, ' > Loss(Y,ȳ):', f\"{loss:,.5f}\", \n",
    "              ' I(X,ȳ):', f\"{mutual_info:,.5f}\",\n",
    "              ' I(X,Y):', f\"{expected_mutual_info:,.5f}\",\n",
    "              ' H(X):', f\"{initial_entropy:,.5f}\")\n",
    "\n",
    "    if idx > num_iterations:\n",
    "        print('Training Terminated')\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-18T21:01:04.985757Z",
     "start_time": "2024-07-18T21:00:59.833308Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "BCEloss_np = np.array(BCElosses)\n",
    "MI_np = np.array(MI)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(MI_np)+1), MI_np, marker='o')\n",
    "plt.plot(range(1, len(BCEloss_np)+1), BCEloss_np, marker='x')\n",
    "\n",
    "plt.title('Training BCW Loss and MI')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss and MI')\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "twCAozYjS4J9",
    "ExecuteTime": {
     "start_time": "2024-07-18T20:58:16.017016Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Testing loop\n",
    "model.eval()\n",
    "total_loss = 0.0\n",
    "predictions = []\n",
    "given = []\n",
    "actual = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, (inputs, targets) in enumerate(tqdm(test_loader, desc=f'Epoch {epoch+1}/{epochs}')):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        outputs = model(inputs.long(), targets.long())\n",
    "        loss = criterion(outputs, targets.float())\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        for i in range(len(inputs)):\n",
    "            predictions.append(outputs[i].tolist())\n",
    "            given.append(inputs[i].tolist())\n",
    "            actual.append(targets[i].tolist())\n",
    "\n",
    "    avg_test_loss = total_loss / len(test_loader)\n",
    "    print(f'Test Loss: {avg_test_loss:.4f}')\n",
    "\n",
    "# Save the model and predictions\n",
    "torch.save(model.state_dict(), 'quantum_transformer_model.pth')\n",
    "preds = {\n",
    "    'Initial_States': given,\n",
    "    'Final_State_Pred': predictions,\n",
    "    'Final_States': actual\n",
    "}\n",
    "final_df = pd.DataFrame(preds)\n",
    "final_df.head()"
   ],
   "metadata": {
    "id": "d2BA8KDbP0E5"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
