{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "q4ns8pdIBhQb",
    "7lOv4H9iBVGJ"
   ],
   "authorship_tag": "ABX9TyO2+HhMgcSkau+jZyzzWrPw",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# File Setup"
   ],
   "metadata": {
    "id": "q4ns8pdIBhQb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "print(os.listdir())\n",
    "directory_path = ''\n",
    "\n",
    "#from google.colab import drive\n",
    "#import sys\n",
    "#drive.mount('/content/drive')\n",
    "#directory_path = '/content/drive/MyDrive/Quantum/'\n",
    "#sys.path.append('/content/drive/MyDrive/Quantum')\n",
    "# print(os.listdir(directory_path))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j10YqJU051l8",
    "outputId": "72930d25-1de5-4e9d-c619-e29b6099f1a9",
    "ExecuteTime": {
     "end_time": "2024-07-18T20:06:00.067969Z",
     "start_time": "2024-07-18T20:06:00.063769Z"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dense_small.param', '.DS_Store', 'quantum_experiments', 'DiscreteVariationalParameterizationsDeepV2.py', 'Mutual_Information_Transformer.ipynb', '__pycache__', 'DiscreteVariationalParameterizationsDeepV3.py', 'README.md', 'Mutual_Information_Maximizing_Model.ipynb', 'temp.txt', '.ipynb_checkpoints', '.git', 'QuantumSimulatorDataset.py', 'GibbsSampling.py', '.idea']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install torch\n",
    "!pip install torchmetrics\n",
    "!pip install qiskit-aer\n",
    "!pip install qiskit\n",
    "!pip install pylatexenc\n",
    "!pip install tqdm\n",
    "!pip install scikit-learn"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "d19Pk1nd54_V",
    "outputId": "fb62d185-755b-4169-968c-217c529563b7",
    "ExecuteTime": {
     "end_time": "2024-07-18T19:45:28.664798Z",
     "start_time": "2024-07-18T19:45:16.293399Z"
    }
   },
   "execution_count": 221,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.11/site-packages (2.3.1)\r\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.11/site-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1.3)\r\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.11/site-packages (from torch) (2023.10.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\r\n",
      "Collecting torchmetrics\r\n",
      "  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl.metadata (19 kB)\r\n",
      "Requirement already satisfied: numpy>1.20.0 in /opt/anaconda3/lib/python3.11/site-packages (from torchmetrics) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>17.1 in /opt/anaconda3/lib/python3.11/site-packages (from torchmetrics) (23.1)\r\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/anaconda3/lib/python3.11/site-packages (from torchmetrics) (2.3.1)\r\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\r\n",
      "  Downloading lightning_utilities-0.11.5-py3-none-any.whl.metadata (4.7 kB)\r\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (68.2.2)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.11/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.9.0)\r\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\r\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\r\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (2023.10.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\r\n",
      "Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m868.8/868.8 kB\u001B[0m \u001B[31m2.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0mm\r\n",
      "\u001B[?25hDownloading lightning_utilities-0.11.5-py3-none-any.whl (26 kB)\r\n",
      "Installing collected packages: lightning-utilities, torchmetrics\r\n",
      "Successfully installed lightning-utilities-0.11.5 torchmetrics-1.4.0.post0\r\n",
      "Requirement already satisfied: qiskit-aer in /opt/anaconda3/lib/python3.11/site-packages (0.14.2)\r\n",
      "Requirement already satisfied: qiskit>=0.45.2 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit-aer) (1.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.16.3 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit-aer) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit-aer) (1.11.4)\r\n",
      "Requirement already satisfied: psutil>=5 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit-aer) (5.9.0)\r\n",
      "Requirement already satisfied: rustworkx>=0.14.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (0.14.2)\r\n",
      "Requirement already satisfied: sympy>=1.3 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (1.12)\r\n",
      "Requirement already satisfied: dill>=0.3 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (0.3.7)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (2.8.2)\r\n",
      "Requirement already satisfied: stevedore>=3.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (5.2.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (4.9.0)\r\n",
      "Requirement already satisfied: symengine>=0.11 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (0.11.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.0->qiskit>=0.45.2->qiskit-aer) (1.16.0)\r\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from stevedore>=3.0.0->qiskit>=0.45.2->qiskit-aer) (6.0.0)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy>=1.3->qiskit>=0.45.2->qiskit-aer) (1.3.0)\r\n",
      "Requirement already satisfied: qiskit in /opt/anaconda3/lib/python3.11/site-packages (1.1.0)\r\n",
      "Requirement already satisfied: rustworkx>=0.14.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (0.14.2)\r\n",
      "Requirement already satisfied: numpy<3,>=1.17 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (1.11.4)\r\n",
      "Requirement already satisfied: sympy>=1.3 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (1.12)\r\n",
      "Requirement already satisfied: dill>=0.3 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (0.3.7)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (2.8.2)\r\n",
      "Requirement already satisfied: stevedore>=3.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (5.2.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (4.9.0)\r\n",
      "Requirement already satisfied: symengine>=0.11 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (0.11.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.0->qiskit) (1.16.0)\r\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from stevedore>=3.0.0->qiskit) (6.0.0)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy>=1.3->qiskit) (1.3.0)\r\n",
      "Requirement already satisfied: pylatexenc in /opt/anaconda3/lib/python3.11/site-packages (2.10)\r\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (4.65.0)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.11/site-packages (1.2.2)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.11.4)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.2.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (2.2.0)\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import ast\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.clustering import MutualInfoScore\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from QuantumSimulatorDataset import QuantumSimulationDatasetFast, generate_circuit_params\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "id": "0il2PJTIXtg6",
    "ExecuteTime": {
     "end_time": "2024-07-19T17:28:57.478885Z",
     "start_time": "2024-07-19T17:28:57.209237Z"
    }
   },
   "execution_count": 321,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MI and Entropy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "outputs": [],
   "source": [
    "def entropy(X):\n",
    "    # Flatten the tensor to 1D\n",
    "    X_flat = X.view(-1)\n",
    "    \n",
    "    # Count the occurrences of each unique value\n",
    "    unique_vals, counts = X_flat.unique(return_counts=True)\n",
    "    probabilities = counts.float() / counts.sum()\n",
    "    entropy = -torch.sum(probabilities * torch.log(probabilities))\n",
    "        \n",
    "    return entropy.item()\n",
    "\n",
    "def mutual_info_loss(X, Y):\n",
    "    # Flatten the tensor to 1D\n",
    "    X_flat = X.view(-1)\n",
    "    Y_flat = Y.view(-1)\n",
    "    \n",
    "    return -mutual_info_score(X_flat, Y_flat)\n",
    "\n",
    "class MutualInformationLoss(nn.Module):\n",
    "    def forward(self, input, target):\n",
    "        X_flat = input.view(-1)\n",
    "        Y_flat = target.view(-1)\n",
    "        \n",
    "        # Compute mutual information\n",
    "        mi = mutual_info_score(X_flat, Y_flat)\n",
    "    \n",
    "        # Convert mutual information to a tensor and return the negative as loss\n",
    "        mi_tensor = torch.tensor(mi, dtype=torch.float32, requires_grad=True).to(input.device)\n",
    "        return -mi_tensor\n",
    "    \n",
    "def calc_binary_accuracy(predictions, targets):\n",
    "    correct = (predictions == targets).float().sum()\n",
    "    accuracy = correct / targets.numel()\n",
    "    return accuracy.item()\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T17:54:55.373730Z",
     "start_time": "2024-07-19T17:54:55.368542Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Definition"
   ],
   "metadata": {
    "id": "j87DiZJLfiXR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class QuantumTransformer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_qbits,\n",
    "            embedding_size,\n",
    "            num_heads,\n",
    "            num_encoder_layers,\n",
    "            num_decoder_layers,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            device,\n",
    "            num_ones\n",
    "    ):\n",
    "        super(QuantumTransformer, self).__init__()\n",
    "        self.src_emb = nn.Embedding(2, embedding_size)\n",
    "        self.src_position_emb = nn.Embedding(num_qbits, embedding_size)\n",
    "        self.tgt_emb = nn.Embedding(2, embedding_size)\n",
    "        self.tgt_position_emb = nn.Embedding(num_qbits, embedding_size)\n",
    "        self.device = device\n",
    "        self.num_ones = num_ones\n",
    "        self.transformer = nn.Transformer(\n",
    "            embedding_size,\n",
    "            num_heads,\n",
    "            num_encoder_layers,\n",
    "            num_decoder_layers,\n",
    "            forward_expansion,\n",
    "            dropout\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embedding_size, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def create_mask(self, size):\n",
    "        mask = torch.tril(torch.ones(size, size)).to(self.device)\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        batch_size, input_dim = src.shape\n",
    "        batch_size, output_dim = tgt.shape\n",
    "        \n",
    "        src_positions = torch.arange(0, input_dim).unsqueeze(0).expand(batch_size, input_dim).to(self.device)\n",
    "        tgt_positions = torch.arange(0, output_dim).unsqueeze(0).expand(batch_size, output_dim).to(self.device)\n",
    "        \n",
    "        embed_src = self.dropout(self.src_emb(src) + self.src_position_emb(src_positions))\n",
    "        embed_tgt = self.dropout(self.tgt_emb(tgt) + self.tgt_position_emb(tgt_positions))\n",
    "        \n",
    "        src_mask = self.create_mask(input_dim)\n",
    "        tgt_mask = self.create_mask(output_dim)\n",
    "        \n",
    "        out = self.transformer(\n",
    "            embed_src.permute(1, 0, 2),  # (S, N, E)\n",
    "            embed_tgt.permute(1, 0, 2),  # (T, N, E)\n",
    "            src_mask=src_mask,\n",
    "            tgt_mask = tgt_mask\n",
    "        )\n",
    "        out = self.fc_out(out.permute(1, 0, 2))\n",
    "        out = self.sigmoid(out) # (N, S, 1)\n",
    "        \n",
    "        return out.squeeze(-1) # (N, S)\n",
    "    \n",
    "    # Issue: predicting S-1 length sequence may not have num_ones amount of ones\n",
    "    def conservation_ones_process(self, probabilities):\n",
    "        sorted_indices = torch.argsort(probabilities, dim=1, descending=True)\n",
    "        threshold_output = torch.zeros_like(probabilities)\n",
    "    \n",
    "        for j in range(probabilities.size(0)):\n",
    "            threshold_output[j, sorted_indices[j, : self.num_ones]] = 1\n",
    "        \n",
    "        return threshold_output\n",
    "    \n",
    "    def predict(self, src):\n",
    "        batch_size, dim = src.shape\n",
    "        src_positions = torch.arange(0, dim).unsqueeze(0).expand(batch_size, dim).to(self.device)\n",
    "        \n",
    "        # Encode the source sequence\n",
    "        embed_src = self.dropout(self.src_emb(src) + self.src_position_emb(src_positions))\n",
    "        memory = self.transformer.encoder(embed_src.permute(1, 0, 2))  # (src_seq_len, batch_size, embedding_size)\n",
    "        \n",
    "        # Initialize the target sequence with a start token (here, we use zeros)\n",
    "        tgt = torch.zeros((batch_size, 1), dtype=torch.long).to(self.device)\n",
    "        all_probs = []  # List to store probabilities at each step\n",
    "    \n",
    "        for i in range(dim):\n",
    "            tgt_positions = torch.arange(0, tgt.size(1)).unsqueeze(0).expand(batch_size, tgt.size(1)).to(self.device)\n",
    "            embed_tgt = self.dropout(self.tgt_emb(tgt) + self.tgt_position_emb(tgt_positions))\n",
    "            \n",
    "            tgt_mask = self.transformer.generate_square_subsequent_mask(tgt.size(1)).to(self.device)\n",
    "            \n",
    "            out = self.transformer.decoder(\n",
    "                embed_tgt.permute(1, 0, 2),  # (tgt_seq_len, batch_size, embedding_size)\n",
    "                memory,\n",
    "                tgt_mask=tgt_mask\n",
    "            )\n",
    "            out = self.fc_out(out.permute(1, 0, 2))  # (batch_size, tgt_seq_len, num_classes)\n",
    "            out = self.sigmoid(out)\n",
    "            \n",
    "            all_probs.append(out[:, -1, :])  # Collect the probabilities for the last position\n",
    "    \n",
    "            next_token = out[:, -1, :].argmax(dim=-1, keepdim=True)  # Choose the most probable token\n",
    "            tgt = torch.cat((tgt, next_token), dim=1)\n",
    "        \n",
    "        all_probs = torch.stack(all_probs, dim=1).squeeze(-1)  # Shape: (batch_size, max_length)\n",
    "        \n",
    "        return self.conservation_ones_process(all_probs)\n",
    "\n",
    "        "
   ],
   "metadata": {
    "id": "QkPRHQTkflC-",
    "ExecuteTime": {
     "end_time": "2024-07-19T17:54:55.935999Z",
     "start_time": "2024-07-19T17:54:55.930828Z"
    }
   },
   "execution_count": 329,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Initialization and Training"
   ],
   "metadata": {
    "id": "UKRswzQ1BmyZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Version 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "load_model = False\n",
    "save_model = True\n",
    "\n",
    "#Hyperparameters\n",
    "num_iterations = 1000\n",
    "learning_rate = 1e-4\n",
    "batch_size = 40\n",
    "binary_vocab_size = 2\n",
    "embedding_size = 512\n",
    "num_heads = 8\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "dropout = 0.1\n",
    "qbits = 12\n",
    "forward_expansion = 4\n",
    "inverse_density = 3\n",
    "num_qbits = 12\n",
    "num_ones = int(num_qbits/inverse_density)\n",
    "num_final_per_initial = 4\n",
    "\n",
    "model = QuantumTransformer(qbits, embedding_size, num_heads, num_encoder_layers, num_decoder_layers, forward_expansion, dropout, device, num_ones).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "params = generate_circuit_params(0,num_qbits)\n",
    "#params = generate_circuit_params(file_name = directory_path + 'dense_small.param')\n",
    "dataset = QuantumSimulationDatasetFast(params, batch_size, num_final_per_initial, device, inverse_density=inverse_density)\n",
    "print('Device:', device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oJHfp3CDA-0N",
    "outputId": "1335a2d1-b855-436a-b96c-32f012c5a772",
    "ExecuteTime": {
     "end_time": "2024-07-19T17:54:56.565242Z",
     "start_time": "2024-07-19T17:54:56.339538Z"
    }
   },
   "execution_count": 330,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "| Iteration 0  > BCELoss(Y,ȳ): 0.97019 Accuracy: 0.53125  I(Y,ȳ): 0.00051  I(X,ȳ): 0.02535  I(X,Y): 0.63651  H(X): 0.63651\n",
      "| Iteration 10  > BCELoss(Y,ȳ): 0.65100 Accuracy: 0.57102  I(Y,ȳ): 0.00173  I(X,ȳ): 0.02535  I(X,Y): 0.63651  H(X): 0.63651\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[331], line 18\u001B[0m\n\u001B[1;32m     16\u001B[0m out \u001B[38;5;241m=\u001B[39m model(initial_state, final_state[:, :\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])    \n\u001B[1;32m     17\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(out, final_state[:,\u001B[38;5;241m1\u001B[39m:]\u001B[38;5;241m.\u001B[39mfloat())    \n\u001B[0;32m---> 18\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     19\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Training Metrics\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:525\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    515\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    517\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    518\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    523\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    524\u001B[0m     )\n\u001B[0;32m--> 525\u001B[0m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mbackward(\n\u001B[1;32m    526\u001B[0m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs\u001B[38;5;241m=\u001B[39minputs\n\u001B[1;32m    527\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    262\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 267\u001B[0m _engine_run_backward(\n\u001B[1;32m    268\u001B[0m     tensors,\n\u001B[1;32m    269\u001B[0m     grad_tensors_,\n\u001B[1;32m    270\u001B[0m     retain_graph,\n\u001B[1;32m    271\u001B[0m     create_graph,\n\u001B[1;32m    272\u001B[0m     inputs,\n\u001B[1;32m    273\u001B[0m     allow_unreachable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    274\u001B[0m     accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    275\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    742\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    743\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 744\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    745\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    746\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    747\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    748\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "BCELoss_array = []\n",
    "MI_array = []\n",
    "\n",
    "print('Training')\n",
    "model.train()\n",
    "for idx, (initial_state, final_state) in enumerate(dataset):\n",
    "    train_indices, test_indices = train_test_split(torch.arange(batch_size), test_size=0.2)\n",
    "    initial_state_test = initial_state[test_indices].to(device).long()\n",
    "    initial_state = initial_state[train_indices].to(device).long()\n",
    "    final_state_test = final_state[test_indices].to(device).long()\n",
    "    final_state = final_state[train_indices].to(device).long()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward and backward pass\n",
    "    out = model(initial_state, final_state[:, :-1])    \n",
    "    loss = criterion(out, final_state[:,1:].float())    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "            \n",
    "    # Training Metrics\n",
    "    train_predictions = model.conservation_ones_process(out)\n",
    "    train_mutual_info = mutual_info_score(final_state[:,1:].reshape(-1), train_predictions.reshape(-1))\n",
    "    initial_entropy = entropy(initial_state)\n",
    "    BAccuracy = calc_binary_accuracy(train_predictions, final_state[:,1:])\n",
    "    \n",
    "    # Testing Metrics\n",
    "    inference = model.predict(initial_state_test)\n",
    "    test_mutual_info = mutual_info_score(initial_state_test.reshape(-1), inference.reshape(-1))\n",
    "    expected_mutual_info = mutual_info_score(initial_state_test.reshape(-1), final_state_test.reshape(-1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Tracking\n",
    "    BCELoss_array.append(loss.item())\n",
    "    MI_array.append(train_mutual_info)\n",
    "    if idx % 10 == 0:\n",
    "        print('| Iteration', idx, ' > BCELoss(Y,ȳ):', f\"{loss:,.5f}\",\n",
    "              'Accuracy:', f\"{BAccuracy:,.5f}\",\n",
    "              ' I(Y,ȳ):', f\"{train_mutual_info:,.5f}\",\n",
    "              ' I(X,ȳ):', f\"{test_mutual_info:,.5f}\",\n",
    "              ' I(X,Y):', f\"{expected_mutual_info:,.5f}\",\n",
    "              ' H(X):', f\"{initial_entropy:,.5f}\")\n",
    "\n",
    "    if idx > num_iterations:\n",
    "        print('Training Terminated')\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T17:55:10.158526Z",
     "start_time": "2024-07-19T17:54:56.567716Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "BCEloss_np = np.array(BCELoss_array)\n",
    "MI_np = np.array(MI_array)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(MI_np)+1), MI_np, marker='o')\n",
    "plt.plot(range(1, len(BCEloss_np)+1), BCEloss_np, marker='x')\n",
    "\n",
    "plt.title('Training BCW Loss and MI')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss and MI')\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "twCAozYjS4J9",
    "ExecuteTime": {
     "end_time": "2024-07-19T17:55:10.162425Z",
     "start_time": "2024-07-19T17:55:10.159685Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Testing loop\n",
    "model.eval()\n",
    "total_loss = 0.0\n",
    "predictions = []\n",
    "given = []\n",
    "actual = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, (inputs, targets) in enumerate(tqdm(test_loader, desc=f'Epoch {epoch+1}/{epochs}')):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        outputs = model(inputs.long(), targets.long())\n",
    "        loss = criterion(outputs, targets.float())\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        for i in range(len(inputs)):\n",
    "            predictions.append(outputs[i].tolist())\n",
    "            given.append(inputs[i].tolist())\n",
    "            actual.append(targets[i].tolist())\n",
    "\n",
    "    avg_test_loss = total_loss / len(test_loader)\n",
    "    print(f'Test Loss: {avg_test_loss:.4f}')\n",
    "\n",
    "# Save the model and predictions\n",
    "torch.save(model.state_dict(), 'quantum_transformer_model.pth')\n",
    "preds = {\n",
    "    'Initial_States': given,\n",
    "    'Final_State_Pred': predictions,\n",
    "    'Final_States': actual\n",
    "}\n",
    "final_df = pd.DataFrame(preds)\n",
    "final_df.head()"
   ],
   "metadata": {
    "id": "d2BA8KDbP0E5",
    "ExecuteTime": {
     "start_time": "2024-07-19T17:55:10.160985Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
