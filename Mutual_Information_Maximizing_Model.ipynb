{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T21:21:55.730731Z",
     "start_time": "2024-07-12T21:21:55.729084Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B70wgiwMLH8R",
    "outputId": "7a21aa53-f2bc-470a-aa1d-ae8f7b734e03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.git', 'DiscreteVariationalParameterizationsDeepV2.py', 'GibbsSampling.py', 'README.md', 'Symmetric_Exclusion_Process _Simulator.ipynb', 'DiscreteVariationalParameterizationsDeepV3.py', 'Quantum_Transformer.ipynb', 'Quantum_Brickworks_Circuit_Generator.ipynb', 'temp.txt', 'QuantumSimulatorDataset.py', 'circuit_diagram.png', 'Mutual_Information_Maximizing_Model.ipynb', 'dense_small.param', '.ipynb_checkpoints', '__pycache__', 'quantum_experiments']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T21:22:05.612062Z",
     "start_time": "2024-07-12T21:21:58.141076Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u3NkfGVa1e-e",
    "outputId": "275fb468-b45a-4ab3-fa7e-ad7a08f19637"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/sl6346/.local/lib/python3.11/site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in /usr/licensed/anaconda3/2024.2/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/licensed/anaconda3/2024.2/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/licensed/anaconda3/2024.2/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/licensed/anaconda3/2024.2/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/licensed/anaconda3/2024.2/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/licensed/anaconda3/2024.2/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/sl6346/.local/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/sl6346/.local/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/sl6346/.local/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/sl6346/.local/lib/python3.11/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/sl6346/.local/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/sl6346/.local/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/sl6346/.local/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/sl6346/.local/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/sl6346/.local/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/sl6346/.local/lib/python3.11/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/sl6346/.local/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /home/sl6346/.local/lib/python3.11/site-packages (from torch) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/sl6346/.local/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/licensed/anaconda3/2024.2/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/licensed/anaconda3/2024.2/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: qiskit-aer in /home/sl6346/.local/lib/python3.11/site-packages (0.14.2)\n",
      "Requirement already satisfied: qiskit>=0.45.2 in /home/sl6346/.local/lib/python3.11/site-packages (from qiskit-aer) (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.16.3 in /usr/licensed/anaconda3/2024.2/lib/python3.11/site-packages (from qiskit-aer) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/licensed/anaconda3/2024.2/lib/python3.11/site-packages (from qiskit-aer) (1.11.4)\n",
      "Requirement already satisfied: psutil>=5 in /usr/licensed/anaconda3/2024.2/lib/python3.11/site-packages (from qiskit-aer) (5.9.0)\n",
      "Requirement already satisfied: rustworkx>=0.14.0 in /home/sl6346/.local/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (0.15.1)\n",
      "Requirement already satisfied: sympy>=1.3 in /usr/licensed/anaconda3/2024.2/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (1.12)\n",
      "Requirement already satisfied: dill>=0.3 in /usr/licensed/anaconda3/2024.2/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (0.3.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/licensed/anaconda3/2024.2/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (2.8.2)\n",
      "Requirement already satisfied: stevedore>=3.0.0 in /home/sl6346/.local/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (5.2.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/licensed/anaconda3/2024.2/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (4.9.0)\n",
      "Requirement already satisfied: symengine>=0.11 in /home/sl6346/.local/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/licensed/anaconda3/2024.2/lib/python3.11/site-packages (from python-dateutil>=2.8.0->qiskit>=0.45.2->qiskit-aer) (1.16.0)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /home/sl6346/.local/lib/python3.11/site-packages (from stevedore>=3.0.0->qiskit>=0.45.2->qiskit-aer) (6.0.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/licensed/anaconda3/2024.2/lib/python3.11/site-packages (from sympy>=1.3->qiskit>=0.45.2->qiskit-aer) (1.3.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: qiskit in /home/sl6346/.local/lib/python3.11/site-packages (1.1.1)\n",
      "Requirement already satisfied: rustworkx>=0.14.0 in /home/sl6346/.local/lib/python3.11/site-packages (from qiskit) (0.15.1)\n",
      "Requirement already satisfied: numpy<3,>=1.17 in /usr/licensed/anaconda3/2024.2/lib/python3.11/site-packages (from qiskit) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5 in /usr/licensed/anaconda3/2024.2/lib/python3.11/site-packages (from qiskit) (1.11.4)\n",
      "Requirement already satisfied: sympy>=1.3 in /usr/licensed/anaconda3/2024.2/lib/python3.11/site-packages (from qiskit) (1.12)\n",
      "Requirement already satisfied: dill>=0.3 in /usr/licensed/anaconda3/2024.2/lib/python3.11/site-packages (from qiskit) (0.3.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/licensed/anaconda3/2024.2/lib/python3.11/site-packages (from qiskit) (2.8.2)\n",
      "Requirement already satisfied: stevedore>=3.0.0 in /home/sl6346/.local/lib/python3.11/site-packages (from qiskit) (5.2.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/licensed/anaconda3/2024.2/lib/python3.11/site-packages (from qiskit) (4.9.0)\n",
      "Requirement already satisfied: symengine>=0.11 in /home/sl6346/.local/lib/python3.11/site-packages (from qiskit) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/licensed/anaconda3/2024.2/lib/python3.11/site-packages (from python-dateutil>=2.8.0->qiskit) (1.16.0)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /home/sl6346/.local/lib/python3.11/site-packages (from stevedore>=3.0.0->qiskit) (6.0.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/licensed/anaconda3/2024.2/lib/python3.11/site-packages (from sympy>=1.3->qiskit) (1.3.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pylatexenc in /home/sl6346/.local/lib/python3.11/site-packages (2.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install qiskit-aer\n",
    "!pip install qiskit\n",
    "!pip install pylatexenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T22:57:00.329255Z",
     "start_time": "2024-07-12T22:53:53.808573Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "id": "u6Rq3bU6sHLw",
    "outputId": "56cf9751-34c6-47ac-c8b8-86c58473b086"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Running:  cuda\n",
      "Running Experiment 0 (12, 2, 1024)\n",
      "State Space of 12 to Embedding Space of 2\n",
      "| Iteration 0 I(W,Z) >  -6.42764  I(X,Y) >  -0.000000\n",
      "| Iteration 10 I(W,Z) >  -6.24964  I(X,Y) >  -0.000000\n",
      "| Iteration 20 I(W,Z) >  -6.25433  I(X,Y) >  -0.000000\n",
      "| Iteration 30 I(W,Z) >  -6.24715  I(X,Y) >  -0.000000\n",
      "| Iteration 40 I(W,Z) >  -6.23422  I(X,Y) >  -0.000000\n",
      "| Iteration 50 I(W,Z) >  -6.23985  I(X,Y) >  -0.000000\n",
      "| Iteration 60 I(W,Z) >  -6.21957  I(X,Y) >  -0.000000\n",
      "| Iteration 70 I(W,Z) >  -6.22572  I(X,Y) >  -0.000000\n",
      "| Iteration 80 I(W,Z) >  -6.22748  I(X,Y) >  -0.000000\n",
      "| Iteration 90 I(W,Z) >  -6.22268  I(X,Y) >  -0.000000\n",
      "| Iteration 100 I(W,Z) >  -6.22806  I(X,Y) >  -0.000000\n",
      "| Iteration 110 I(W,Z) >  -6.24158  I(X,Y) >  -0.000000\n",
      "| Iteration 120 I(W,Z) >  -6.22060  I(X,Y) >  -0.000000\n",
      "| Iteration 130 I(W,Z) >  -6.21065  I(X,Y) >  -0.000000\n",
      "| Iteration 140 I(W,Z) >  -6.21757  I(X,Y) >  -0.000000\n",
      "| Iteration 150 I(W,Z) >  -6.21542  I(X,Y) >  -0.000000\n",
      "| Iteration 160 I(W,Z) >  -6.20958  I(X,Y) >  -0.000000\n",
      "| Iteration 170 I(W,Z) >  -6.21516  I(X,Y) >  -0.000000\n",
      "| Iteration 180 I(W,Z) >  -6.21057  I(X,Y) >  -0.000000\n",
      "| Iteration 190 I(W,Z) >  -6.21091  I(X,Y) >  -0.000000\n",
      "| Iteration 200 I(W,Z) >  -6.22079  I(X,Y) >  -0.000000\n",
      "| Iteration 210 I(W,Z) >  -6.21890  I(X,Y) >  -0.000000\n",
      "| Iteration 220 I(W,Z) >  -6.21548  I(X,Y) >  -0.000000\n",
      "| Iteration 230 I(W,Z) >  -6.21192  I(X,Y) >  -0.000000\n",
      "| Iteration 240 I(W,Z) >  -6.22225  I(X,Y) >  -0.000000\n",
      "| Iteration 250 I(W,Z) >  -6.20642  I(X,Y) >  -0.000000\n",
      "| Iteration 260 I(W,Z) >  -6.20385  I(X,Y) >  -0.000000\n",
      "| Iteration 270 I(W,Z) >  -6.21774  I(X,Y) >  -0.000000\n",
      "| Iteration 280 I(W,Z) >  -6.21139  I(X,Y) >  -0.000000\n",
      "| Iteration 290 I(W,Z) >  -6.20977  I(X,Y) >  -0.000000\n",
      "| Iteration 300 I(W,Z) >  -6.21592  I(X,Y) >  -0.000000\n",
      "| Iteration 310 I(W,Z) >  -6.20878  I(X,Y) >  -0.000000\n",
      "| Iteration 320 I(W,Z) >  -6.21303  I(X,Y) >  -0.000000\n",
      "| Iteration 330 I(W,Z) >  -6.20917  I(X,Y) >  -0.000000\n",
      "| Iteration 340 I(W,Z) >  -6.21105  I(X,Y) >  -0.000000\n",
      "| Iteration 350 I(W,Z) >  -6.20571  I(X,Y) >  -0.000000\n",
      "| Iteration 360 I(W,Z) >  -6.20997  I(X,Y) >  -0.000000\n",
      "| Iteration 370 I(W,Z) >  -6.20855  I(X,Y) >  -0.000000\n",
      "| Iteration 380 I(W,Z) >  -6.20964  I(X,Y) >  -0.000000\n",
      "| Iteration 390 I(W,Z) >  -6.20409  I(X,Y) >  -0.000000\n",
      "| Iteration 400 I(W,Z) >  -6.21285  I(X,Y) >  -0.000000\n",
      "| Iteration 410 I(W,Z) >  -6.20158  I(X,Y) >  -0.000000\n",
      "| Iteration 420 I(W,Z) >  -6.20937  I(X,Y) >  -0.000000\n",
      "| Iteration 430 I(W,Z) >  -6.21088  I(X,Y) >  -0.000000\n",
      "| Iteration 440 I(W,Z) >  -6.21074  I(X,Y) >  -0.000000\n",
      "| Iteration 450 I(W,Z) >  -6.20914  I(X,Y) >  -0.000000\n",
      "| Iteration 460 I(W,Z) >  -6.20405  I(X,Y) >  -0.000000\n",
      "| Iteration 470 I(W,Z) >  -6.20726  I(X,Y) >  -0.000000\n",
      "| Iteration 480 I(W,Z) >  -6.20727  I(X,Y) >  -0.000000\n",
      "| Iteration 490 I(W,Z) >  -6.20596  I(X,Y) >  -0.000000\n",
      "| Iteration 500 I(W,Z) >  -6.21105  I(X,Y) >  -0.000000\n",
      "| Iteration 510 I(W,Z) >  -6.20992  I(X,Y) >  -0.000000\n",
      "| Iteration 520 I(W,Z) >  -6.20840  I(X,Y) >  -0.000000\n",
      "| Iteration 530 I(W,Z) >  -6.20694  I(X,Y) >  -0.000000\n",
      "| Iteration 540 I(W,Z) >  -6.20729  I(X,Y) >  -0.000000\n",
      "| Iteration 550 I(W,Z) >  -6.20585  I(X,Y) >  -0.000000\n",
      "| Iteration 560 I(W,Z) >  -6.20602  I(X,Y) >  -0.000000\n",
      "| Iteration 570 I(W,Z) >  -6.20576  I(X,Y) >  -0.000000\n",
      "| Iteration 580 I(W,Z) >  -6.21230  I(X,Y) >  -0.000000\n",
      "| Iteration 590 I(W,Z) >  -6.20681  I(X,Y) >  -0.000000\n",
      "| Iteration 600 I(W,Z) >  -6.20292  I(X,Y) >  -0.000000\n",
      "| Iteration 610 I(W,Z) >  -6.20217  I(X,Y) >  -0.000000\n",
      "| Iteration 620 I(W,Z) >  -6.20938  I(X,Y) >  -0.000000\n",
      "| Iteration 630 I(W,Z) >  -6.20483  I(X,Y) >  -0.000000\n",
      "| Iteration 640 I(W,Z) >  -6.20648  I(X,Y) >  -0.000000\n",
      "| Iteration 650 I(W,Z) >  -6.20833  I(X,Y) >  -0.000000\n",
      "| Iteration 660 I(W,Z) >  -6.20886  I(X,Y) >  -0.000000\n",
      "| Iteration 670 I(W,Z) >  -6.20859  I(X,Y) >  -0.000000\n",
      "| Iteration 680 I(W,Z) >  -6.21665  I(X,Y) >  -0.000000\n",
      "| Iteration 690 I(W,Z) >  -6.20797  I(X,Y) >  -0.000000\n",
      "| Iteration 700 I(W,Z) >  -6.20804  I(X,Y) >  -0.000000\n",
      "| Iteration 710 I(W,Z) >  -6.20755  I(X,Y) >  -0.000000\n",
      "| Iteration 720 I(W,Z) >  -6.20320  I(X,Y) >  -0.000000\n",
      "| Iteration 730 I(W,Z) >  -6.20728  I(X,Y) >  -0.000000\n",
      "| Iteration 740 I(W,Z) >  -6.20525  I(X,Y) >  -0.000000\n",
      "| Iteration 750 I(W,Z) >  -6.20897  I(X,Y) >  -0.000000\n",
      "| Iteration 760 I(W,Z) >  -6.20665  I(X,Y) >  -0.000000\n",
      "| Iteration 770 I(W,Z) >  -6.20504  I(X,Y) >  -0.000000\n",
      "| Iteration 780 I(W,Z) >  -6.20781  I(X,Y) >  -0.000000\n",
      "| Iteration 790 I(W,Z) >  -6.20447  I(X,Y) >  -0.000000\n",
      "| Iteration 800 I(W,Z) >  -6.20871  I(X,Y) >  -0.000000\n",
      "| Iteration 810 I(W,Z) >  -6.20993  I(X,Y) >  -0.000000\n",
      "| Iteration 820 I(W,Z) >  -6.20646  I(X,Y) >  -0.000000\n",
      "| Iteration 830 I(W,Z) >  -6.20950  I(X,Y) >  -0.000000\n",
      "| Iteration 840 I(W,Z) >  -6.20737  I(X,Y) >  -0.000000\n",
      "| Iteration 850 I(W,Z) >  -6.20580  I(X,Y) >  -0.000000\n",
      "| Iteration 860 I(W,Z) >  -6.20553  I(X,Y) >  -0.000000\n",
      "| Iteration 870 I(W,Z) >  -6.20821  I(X,Y) >  -0.000000\n",
      "| Iteration 880 I(W,Z) >  -6.20924  I(X,Y) >  -0.000000\n",
      "| Iteration 890 I(W,Z) >  -6.20633  I(X,Y) >  -0.000000\n",
      "| Iteration 900 I(W,Z) >  -6.20556  I(X,Y) >  -0.000000\n",
      "| Iteration 910 I(W,Z) >  -6.20469  I(X,Y) >  -0.000000\n",
      "| Iteration 920 I(W,Z) >  -6.21101  I(X,Y) >  -0.000000\n",
      "| Iteration 930 I(W,Z) >  -6.21058  I(X,Y) >  -0.000000\n",
      "| Iteration 940 I(W,Z) >  -6.20467  I(X,Y) >  -0.000000\n",
      "| Iteration 950 I(W,Z) >  -6.20780  I(X,Y) >  -0.000000\n",
      "| Iteration 960 I(W,Z) >  -6.20332  I(X,Y) >  -0.000000\n",
      "| Iteration 970 I(W,Z) >  -6.20325  I(X,Y) >  -0.000000\n",
      "| Iteration 980 I(W,Z) >  -6.21072  I(X,Y) >  -0.000000\n",
      "| Iteration 990 I(W,Z) >  -6.20552  I(X,Y) >  -0.000000\n",
      "| Iteration 1000 I(W,Z) >  -6.20616  I(X,Y) >  -0.000000\n",
      "| Iteration 1010 I(W,Z) >  -6.21303  I(X,Y) >  -0.000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 221\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRunning Experiment\u001b[39m\u001b[38;5;124m'\u001b[39m, i, params)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mState Space of\u001b[39m\u001b[38;5;124m'\u001b[39m, params[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto Embedding Space of\u001b[39m\u001b[38;5;124m'\u001b[39m, params[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 221\u001b[0m run_dim_red_process(device, \u001b[38;5;241m*\u001b[39mparams)\n",
      "Cell \u001b[0;32mIn[5], line 190\u001b[0m, in \u001b[0;36mrun_dim_red_process\u001b[0;34m(device, state_space, embedding_space_size, batch_size, num_steps)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (final_state, initial_state) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset):\n\u001b[1;32m    189\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 190\u001b[0m     loss, actual_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtest_objective_function(initial_state, final_state)\n\u001b[1;32m    191\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    192\u001b[0m     actual_loss \u001b[38;5;241m=\u001b[39m actual_loss\u001b[38;5;241m.\u001b[39mmean()\n",
      "Cell \u001b[0;32mIn[5], line 37\u001b[0m, in \u001b[0;36mEmbeddingMI3.test_objective_function\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     34\u001b[0m w_tilde \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_sampler\u001b[38;5;241m.\u001b[39mrun_batched_gibbs(z)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     35\u001b[0m x_tilde \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_sampler\u001b[38;5;241m.\u001b[39mrun_batched_gibbs(w)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_ones, \u001b[38;5;241m*\u001b[39m(z, y, w, x, w_tilde, x_tilde), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mparams(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mparams(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dynamics\u001b[38;5;241m.\u001b[39mparams()), torch\u001b[38;5;241m.\u001b[39mtensor([ \u001b[38;5;241m0.\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/autograd/function.py:598\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    606\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[5], line 113\u001b[0m, in \u001b[0;36mMutualInformationLossV3.forward\u001b[0;34m(ctx, *inputs)\u001b[0m\n\u001b[1;32m    108\u001b[0m z, y, w, x, _, _ \u001b[38;5;241m=\u001b[39m zywx_w_tilde_ins\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m#print(x[0], '|' ,y[0])\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m#print(w[0], '|' ,z[0])\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m p_x_w_estimate \u001b[38;5;241m=\u001b[39m DVP\u001b[38;5;241m.\u001b[39mEnergyBasedDecoder\u001b[38;5;241m.\u001b[39mestimated_conditional_log_probability_a_given_b(x, w, num_ones, \u001b[38;5;241m*\u001b[39mdecoder_params)\n\u001b[1;32m    114\u001b[0m p_w_x \u001b[38;5;241m=\u001b[39m DVP\u001b[38;5;241m.\u001b[39mBoltzmannBasedEncoder\u001b[38;5;241m.\u001b[39mconditional_log_probability_a_given_b_params(w, x, \u001b[38;5;241m*\u001b[39mencoder_params)\n\u001b[1;32m    115\u001b[0m r_w_z_estimate \u001b[38;5;241m=\u001b[39m DVP\u001b[38;5;241m.\u001b[39mEnergyBasedModelEmbeddingDynamics\u001b[38;5;241m.\u001b[39mestimated_normalized_log_probabilities_w_given_z_params(z, w, \u001b[38;5;241m*\u001b[39membedding_params)\n",
      "File \u001b[0;32m~/QuantumDynamicsAI/DiscreteVariationalParameterizationsDeepV3.py:443\u001b[0m, in \u001b[0;36mEnergyBasedDecoder.estimated_conditional_log_probability_a_given_b\u001b[0;34m(a, b, num_ones, W1, b1, W2, b2, W3, b3, W4, b4)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mestimated_conditional_log_probability_a_given_b\u001b[39m(a, b, num_ones, W1, b1, W2, b2, W3, b3, W4, b4):\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m EnergyBasedDecoder\u001b[38;5;241m.\u001b[39munnormalized_log_probs_a_given_b_params(num_ones, a, b, W1, b1, W2,\n\u001b[0;32m--> 443\u001b[0m                                                                b2, W3, b3, W4, b4) \u001b[38;5;241m-\u001b[39m EnergyBasedDecoder\u001b[38;5;241m.\u001b[39mestimate_log_partition_function(num_ones, b, a, W1,\n\u001b[1;32m    444\u001b[0m                                                                                                                                               b1, W2, b2, W3, b3, W4, b4, samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/QuantumDynamicsAI/DiscreteVariationalParameterizationsDeepV3.py:419\u001b[0m, in \u001b[0;36mEnergyBasedDecoder.estimate_log_partition_function\u001b[0;34m(num_ones, b, initial_state, W1, b1, W2, b2, W3, b3, W4, b4, samples)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# these will become the importance samples\u001b[39;00m\n\u001b[1;32m    418\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(initial_state\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 419\u001b[0m perms \u001b[38;5;241m=\u001b[39m rng\u001b[38;5;241m.\u001b[39mpermuted(np\u001b[38;5;241m.\u001b[39mtile(x, [samples, batch_size])\u001b[38;5;241m.\u001b[39mreshape(samples, batch_size, x\u001b[38;5;241m.\u001b[39msize), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :num_ones]\n\u001b[1;32m    420\u001b[0m importance_samples \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(initial_state)\n\u001b[1;32m    421\u001b[0m importance_samples[np\u001b[38;5;241m.\u001b[39marange(samples)\u001b[38;5;241m.\u001b[39mreshape(samples, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), np\u001b[38;5;241m.\u001b[39marange(batch_size)\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m1\u001b[39m), perms] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from re import X\n",
    "from math import e\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import DiscreteVariationalParameterizationsDeepV3 as DVP\n",
    "from torch.autograd.functional import vjp\n",
    "from torch.autograd.function import Function\n",
    "from QuantumSimulatorDataset import QuantumSimulationDatasetFast, generate_circuit_params\n",
    "from GibbsSampling import BatchedConditionalGibbsSampler, BatchedConditionalDoubleGibbsSampler\n",
    "\n",
    "# uncomment if mounting google drive\n",
    "directory_path = ''\n",
    "\n",
    "class EmbeddingMI3(nn.Module):\n",
    "    def __init__(self, batch_size, in_dim, out_dim, num_ones):\n",
    "        super().__init__()\n",
    "        self.encoder = DVP.BoltzmannBasedEncoder(in_dim=in_dim, out_dim=out_dim)\n",
    "        self.decoder = DVP.EnergyBasedDecoder(in_dim=out_dim, out_dim=in_dim, num_ones=num_ones)\n",
    "        self.num_ones = num_ones\n",
    "        self.embedding_dynamics = DVP.EnergyBasedModelEmbeddingDynamics(dim=out_dim)\n",
    "        self.loss_func = MutualInformationLossV3.apply\n",
    "        self.embedding_sampler = BatchedConditionalGibbsSampler(batch_size=batch_size, num_samples=256, # needs to be tuned\n",
    "                                                                mixing_time=5, # seems like this can be low and still work\n",
    "                                                                joint_distribution=self.embedding_dynamics)\n",
    "        self.decoder_sampler = BatchedConditionalDoubleGibbsSampler(batch_size=batch_size, num_samples=256, # needs to be tuned\n",
    "                                                                mixing_time=24, # seems like this can be low and still work\n",
    "                                                                joint_distribution=self.decoder, dim=in_dim, num_ones=self.num_ones)\n",
    "    def test_objective_function(self, x, y):\n",
    "        w = self.encoder.encoder_sample(x).detach()\n",
    "        z = self.encoder.encoder_sample(y).detach()\n",
    "        # print(w, z, x, y)\n",
    "\n",
    "        w_tilde = self.embedding_sampler.run_batched_gibbs(z).detach()\n",
    "        x_tilde = self.decoder_sampler.run_batched_gibbs(w).detach()\n",
    "\n",
    "        return -self.loss_func(self.num_ones, *(z, y, w, x, w_tilde, x_tilde), *self.encoder.params(), *self.decoder.params(), *self.embedding_dynamics.params()), torch.tensor([ 0.])\n",
    "\n",
    "\n",
    "    def calculate_mutual_information(self, x, y, num_ones):\n",
    "        in_dim = x.shape[1]\n",
    "        hidden_dim = 32\n",
    "\n",
    "        linear_1_weight = nn.Parameter(torch.zeros((hidden_dim, in_dim * in_dim), device=x.device))\n",
    "        linear_1_bias = nn.Parameter(torch.zeros((hidden_dim), device=x.device))\n",
    "\n",
    "        linear_2_weight = nn.Parameter(torch.zeros((hidden_dim, hidden_dim), device=x.device))\n",
    "        linear_2_bias = nn.Parameter(torch.zeros((hidden_dim), device=x.device))\n",
    "\n",
    "        linear_3_weight = nn.Parameter(torch.zeros((hidden_dim, hidden_dim), device=x.device))\n",
    "        linear_3_bias = nn.Parameter(torch.zeros((hidden_dim), device=x.device))\n",
    "\n",
    "        linear_4_weight = nn.Parameter(torch.zeros((1, hidden_dim), device=x.device))\n",
    "        linear_4_bias = nn.Parameter(torch.zeros((1), device=x.device))\n",
    "        temp_params1 = (linear_1_weight, linear_1_bias, linear_2_weight, linear_2_bias,\n",
    "                       linear_3_weight, linear_3_bias, linear_4_weight, linear_4_bias)\n",
    "\n",
    "        b = nn.Parameter(torch.zeros((1, in_dim), device=x.device))\n",
    "        W = nn.Parameter(torch.zeros((1, in_dim, in_dim), device=x.device))\n",
    "        padding1 = nn.Parameter(torch.zeros(1, device=x.device))\n",
    "        padding2 = nn.Parameter(torch.zeros(1, device=x.device))\n",
    "        temp_params2 = (b, W, padding1, padding2)\n",
    "\n",
    "        p_x_y_estimate = DVP.EnergyBasedDecoder.estimated_conditional_log_probability_a_given_b(x, y, num_ones, *temp_params1)\n",
    "        p_y_x_estimate = DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params(y, x, *temp_params2)\n",
    "        mutual_info_x_y = p_x_y_estimate - p_y_x_estimate\n",
    "\n",
    "        return mutual_info_x_y\n",
    "\n",
    "    def test_calculate_mutual_information_loss(self, x, y):\n",
    "        # Ensure x and y are tensors and have the same shape\n",
    "        assert x.shape == y.shape, \"x and y must have the same shape\"\n",
    "\n",
    "        # Compute joint histogram\n",
    "        joint_hist = torch.histc((x * y).float(), bins=256, min=0, max=256)\n",
    "\n",
    "        # Compute marginal histograms\n",
    "        x_hist = torch.histc(x.float(), bins=256, min=0, max=256)\n",
    "        y_hist = torch.histc(y.float(), bins=256, min=0, max=256)\n",
    "\n",
    "        # Normalize histograms to get probabilities\n",
    "        joint_prob = joint_hist / joint_hist.sum()\n",
    "        x_prob = x_hist / x_hist.sum()\n",
    "        y_prob = y_hist / y_hist.sum()\n",
    "\n",
    "        # Compute the entropies\n",
    "        H_x = -torch.sum(x_prob * torch.log(x_prob + 1e-12))\n",
    "        H_y = -torch.sum(y_prob * torch.log(y_prob + 1e-12))\n",
    "        H_xy = -torch.sum(joint_prob * torch.log(joint_prob + 1e-12))\n",
    "\n",
    "        # Compute mutual information\n",
    "        I_xy = H_x + H_y - H_xy\n",
    "\n",
    "        # Return mutual information loss (negative mutual information)\n",
    "        mutual_info_loss = -I_xy\n",
    "\n",
    "        return mutual_info_loss\n",
    "\n",
    "class MutualInformationLossV3(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, *inputs):\n",
    "        num_ones = inputs[0]\n",
    "        zywx_w_tilde_ins = inputs[1:7]\n",
    "        encoder_params = inputs[7:11]\n",
    "        decoder_params = inputs[11:19]\n",
    "        embedding_params = inputs[19:27]\n",
    "\n",
    "        z, y, w, x, _, _ = zywx_w_tilde_ins\n",
    "\n",
    "        #print(x[0], '|' ,y[0])\n",
    "        #print(w[0], '|' ,z[0])\n",
    "\n",
    "        p_x_w_estimate = DVP.EnergyBasedDecoder.estimated_conditional_log_probability_a_given_b(x, w, num_ones, *decoder_params)\n",
    "        p_w_x = DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params(w, x, *encoder_params)\n",
    "        r_w_z_estimate = DVP.EnergyBasedModelEmbeddingDynamics.estimated_normalized_log_probabilities_w_given_z_params(z, w, *embedding_params)\n",
    "\n",
    "        out = p_x_w_estimate - p_w_x + r_w_z_estimate\n",
    "        ctx.num_ones = num_ones\n",
    "        ctx.save_for_backward(*zywx_w_tilde_ins, *encoder_params, *decoder_params, *embedding_params, r_w_z_estimate, out)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        num_ones = ctx.num_ones\n",
    "        z, y, w, x, w_tilde, x_tilde = ctx.saved_tensors[0:6]\n",
    "        encoder_params = ctx.saved_tensors[6:10]\n",
    "        decoder_params = ctx.saved_tensors[10:18]\n",
    "        embedding_params = ctx.saved_tensors[18:26]\n",
    "        r_w_z = ctx.saved_tensors[26]\n",
    "        MI = ctx.saved_tensors[27]\n",
    "\n",
    "        decoder_unnormalized_probs = lambda x, w, *params: DVP.EnergyBasedDecoder.unnormalized_log_probs_a_given_b_params(num_ones, x, w, *params)\n",
    "        decoder_expected_unnormalized_probs = lambda x_tilde, w, *params: DVP.EnergyBasedDecoder.expected_unnormalized_log_probs_a_given_b(num_ones, x_tilde, w, *params)\n",
    "\n",
    "        _, decoder_grad_1 = vjp(decoder_unnormalized_probs, (x, w, *decoder_params), grad_output, create_graph=False)\n",
    "        _, decoder_grad_2 = vjp(decoder_expected_unnormalized_probs, (x_tilde, w.expand(x_tilde.shape[0], -1, -1), *decoder_params), grad_output, create_graph=False)\n",
    "\n",
    "        decoder_grad = tuple(map(lambda x, y: x - y, decoder_grad_1[2:], decoder_grad_2[2:]))\n",
    "\n",
    "        #_, encoder_grad_term_1 = vjp(DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params, (w, x, encoder_params[0], encoder_params[1]), grad_output * (MI - 1), create_graph=False)\n",
    "        _, encoder_grad_term_1 = vjp(DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params, (w, x, *encoder_params), grad_output * (MI - 1), create_graph=False)\n",
    "        encoder_grad_term_1 = encoder_grad_term_1[2:]\n",
    "\n",
    "        #_, encoder_grad_term_2 = vjp(DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params, (z, y, encoder_params[0], encoder_params[1]), grad_output * r_w_z, create_graph=False)\n",
    "        _, encoder_grad_term_2 = vjp(DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params, (z, y, *encoder_params), grad_output * r_w_z, create_graph=False)\n",
    "        encoder_grad_term_2 = encoder_grad_term_2[2:]\n",
    "\n",
    "        encoder_grad = tuple(map(lambda x, y: x + y, encoder_grad_term_1, encoder_grad_term_2))\n",
    "\n",
    "        _, embedding_grad_1 = vjp(DVP.EnergyBasedModelEmbeddingDynamics.unnormalized_log_probs_w_given_z_params, (z, w, *embedding_params), grad_output, create_graph=False)\n",
    "        _, embedding_grad_2 = vjp(DVP.EnergyBasedModelEmbeddingDynamics.expected_unnormalized_log_probs_w_given_z, (z.expand(w_tilde.shape[0], -1, -1), w_tilde, *embedding_params), grad_output, create_graph=False)\n",
    "\n",
    "        embedding_grad = tuple(map(lambda x, y: x - y, embedding_grad_1[2:], embedding_grad_2[2:]))\n",
    "\n",
    "        #print(len(encoder_grad), len(decoder_grad), len(embedding_grad))\n",
    "\n",
    "        return None, None, None, None, None, None, None, *encoder_grad, *decoder_grad, *embedding_grad\n",
    "\n",
    "def run_dim_red_process(device, state_space, embedding_space_size, batch_size=256, num_steps=10000):\n",
    "\n",
    "    model = EmbeddingMI3(batch_size, state_space, embedding_space_size, num_ones=4)\n",
    "\n",
    "    # Path to the state dictionary file\n",
    "    state_dict_path = 'quantum_experiments/initializer.model'\n",
    "    specific_dict_path = f'quantum_experiments/experiment_{state_space}_{embedding_space_size}.model'\n",
    "\n",
    "    # Check if the state dictionary file exists\n",
    "    if os.path.exists(directory_path + specific_dict_path):\n",
    "        # Load the state dictionary\n",
    "        state_dict = torch.load(state_dict_path)\n",
    "        # Get the current state dictionary of the model\n",
    "        old_state_dict = model.state_dict()\n",
    "        # Modify the state dictionary to match the embedding_space_size\n",
    "        old_state_dict['encoder.b'] = state_dict['encoder.b'][:, :embedding_space_size]\n",
    "        old_state_dict['encoder.W'] = state_dict['encoder.W'][:, :embedding_space_size, :]\n",
    "\n",
    "        # Load the adjusted state dictionary into the model\n",
    "        model.load_state_dict(old_state_dict, strict=False)\n",
    "    #else:\n",
    "        #print(f\"State dictionary file '{state_dict_path}' does not exist. Continuing without loading pre-trained weights.\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "    #params = generate_circuit_params(12,12)\n",
    "    params = generate_circuit_params(file_name = directory_path + 'dense_small.param')\n",
    "    dataset = QuantumSimulationDatasetFast(params, batch_size, 4, device, inverse_density=3)\n",
    "\n",
    "    for i, (final_state, initial_state) in enumerate(dataset):\n",
    "        optimizer.zero_grad()\n",
    "        loss, actual_loss = model.test_objective_function(initial_state, final_state)\n",
    "        loss = loss.mean()\n",
    "        actual_loss = actual_loss.mean()\n",
    "        loss.backward()\n",
    "        if i % 10 == 0:\n",
    "            print('| Iteration', i, 'I(W,Z) > ', f\"{-loss.detach().cpu().item():,.5f}\", ' I(X,Y) > ', f\"{-actual_loss.detach().cpu().item():,.6f}\")\n",
    "        optimizer.step()\n",
    "        if i > num_steps:\n",
    "            print('Training Terminated')\n",
    "            break\n",
    "        if i % 1000 == 999:\n",
    "            torch.save(model.state_dict(), f'quantum_experiments/experiment_{state_space}_{embedding_space_size}_{i}.model')\n",
    "\n",
    "    # Save the model at the end of training\n",
    "    final_save_path = f'quantum_experiments/experiment_{state_space}_{embedding_space_size}_final.model'\n",
    "    torch.save(model.state_dict(), final_save_path)\n",
    "    print(f\"Final model saved to {final_save_path}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('Device Running: ', device)\n",
    "\n",
    "    experiments = [\n",
    "        (12, 2, 1024), (12, 3, 1024), (12, 4, 1024), (12, 5, 1024),\n",
    "        (12, 6, 1024), (12, 7, 1024), (12, 8, 1024), (12, 9, 1024),\n",
    "        (12, 10, 1024), (12, 11, 1024), (12, 12, 1024)\n",
    "    ]\n",
    "\n",
    "    for i, params in enumerate(experiments):\n",
    "        print('Running Experiment', i, params)\n",
    "        print('State Space of', params[0], 'to Embedding Space of', params[1])\n",
    "        run_dim_red_process(device, *params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPMeg+QPYunP2XoyRdolJ5b",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
