{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# File/Environment Preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B70wgiwMLH8R",
    "outputId": "7a21aa53-f2bc-470a-aa1d-ae8f7b734e03",
    "ExecuteTime": {
     "end_time": "2024-07-17T19:09:27.484091Z",
     "start_time": "2024-07-17T19:09:27.479205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Symmetric_Exclusion_Process _Simulator.ipynb', 'Quantum_Transformer.ipynb', 'dense_small.param', '.DS_Store', 'quantum_experiments', 'DiscreteVariationalParameterizationsDeepV2.py', 'Mutual_Information_Transformer.ipynb', 'DiscreteVariationalParameterizationsDeepV3.py', 'README.md', 'Mutual_Information_Maximizing_Model.ipynb', 'temp.txt', '.ipynb_checkpoints', '.git', 'Quantum_Brickworks_Circuit_Generator.ipynb', 'QuantumSimulatorDataset.py', 'GibbsSampling.py', '.idea']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir())\n",
    "\n",
    "# uncomment if mounting google drive\n",
    "directory_path = ''\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# import sys\n",
    "# directory_path = '/content/drive/MyDrive/Quantum/'\n",
    "# sys.path.append('/content/drive/MyDrive/Quantum')\n",
    "#print(os.listdir(directory_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u3NkfGVa1e-e",
    "outputId": "275fb468-b45a-4ab3-fa7e-ad7a08f19637",
    "ExecuteTime": {
     "end_time": "2024-07-17T21:05:43.541956Z",
     "start_time": "2024-07-17T21:05:34.765730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.11/site-packages (2.3.1)\r\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.11/site-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1.3)\r\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.11/site-packages (from torch) (2023.10.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\r\n",
      "Requirement already satisfied: qiskit-aer in /opt/anaconda3/lib/python3.11/site-packages (0.14.2)\r\n",
      "Requirement already satisfied: qiskit>=0.45.2 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit-aer) (1.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.16.3 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit-aer) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit-aer) (1.11.4)\r\n",
      "Requirement already satisfied: psutil>=5 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit-aer) (5.9.0)\r\n",
      "Requirement already satisfied: rustworkx>=0.14.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (0.14.2)\r\n",
      "Requirement already satisfied: sympy>=1.3 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (1.12)\r\n",
      "Requirement already satisfied: dill>=0.3 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (0.3.7)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (2.8.2)\r\n",
      "Requirement already satisfied: stevedore>=3.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (5.2.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (4.9.0)\r\n",
      "Requirement already satisfied: symengine>=0.11 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (0.11.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.0->qiskit>=0.45.2->qiskit-aer) (1.16.0)\r\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from stevedore>=3.0.0->qiskit>=0.45.2->qiskit-aer) (6.0.0)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy>=1.3->qiskit>=0.45.2->qiskit-aer) (1.3.0)\r\n",
      "Requirement already satisfied: qiskit in /opt/anaconda3/lib/python3.11/site-packages (1.1.0)\r\n",
      "Requirement already satisfied: rustworkx>=0.14.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (0.14.2)\r\n",
      "Requirement already satisfied: numpy<3,>=1.17 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (1.11.4)\r\n",
      "Requirement already satisfied: sympy>=1.3 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (1.12)\r\n",
      "Requirement already satisfied: dill>=0.3 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (0.3.7)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (2.8.2)\r\n",
      "Requirement already satisfied: stevedore>=3.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (5.2.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (4.9.0)\r\n",
      "Requirement already satisfied: symengine>=0.11 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (0.11.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.0->qiskit) (1.16.0)\r\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from stevedore>=3.0.0->qiskit) (6.0.0)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy>=1.3->qiskit) (1.3.0)\r\n",
      "Requirement already satisfied: pylatexenc in /opt/anaconda3/lib/python3.11/site-packages (2.10)\r\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (4.65.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install qiskit-aer\n",
    "!pip install qiskit\n",
    "!pip install pylatexenc\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import DiscreteVariationalParameterizationsDeepV3 as DVP\n",
    "from torch.autograd.functional import vjp\n",
    "from torch.autograd.function import Function\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from QuantumSimulatorDataset import QuantumSimulationDatasetFast, generate_circuit_params\n",
    "from GibbsSampling import BatchedConditionalGibbsSampler, BatchedConditionalDoubleGibbsSampler"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T21:06:55.910948Z",
     "start_time": "2024-07-17T21:06:55.907287Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MI Model Definition"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "id": "u6Rq3bU6sHLw",
    "outputId": "56cf9751-34c6-47ac-c8b8-86c58473b086",
    "ExecuteTime": {
     "end_time": "2024-07-17T23:06:54.657030Z",
     "start_time": "2024-07-17T23:06:54.641437Z"
    }
   },
   "outputs": [],
   "source": [
    "class EmbeddingMI3(nn.Module):\n",
    "    def __init__(self, batch_size, in_dim, out_dim, num_ones):\n",
    "        super().__init__()\n",
    "        self.encoder = DVP.BoltzmannBasedEncoder(in_dim=in_dim, out_dim=out_dim)\n",
    "        self.decoder = DVP.EnergyBasedDecoder(in_dim=out_dim, out_dim=in_dim, num_ones=num_ones)\n",
    "        self.num_ones = num_ones\n",
    "        self.embedding_dynamics = DVP.EnergyBasedModelEmbeddingDynamics(dim=out_dim)\n",
    "        self.loss_func = MutualInformationLossV3.apply\n",
    "        self.embedding_sampler = BatchedConditionalGibbsSampler(batch_size=batch_size, num_samples=256, # needs to be tuned\n",
    "                                                                mixing_time=5, # seems like this can be low and still work\n",
    "                                                                joint_distribution=self.embedding_dynamics)\n",
    "        self.decoder_sampler = BatchedConditionalDoubleGibbsSampler(batch_size=batch_size, num_samples=256, # needs to be tuned\n",
    "                                                                mixing_time=24, # seems like this can be low and still work\n",
    "                                                                joint_distribution=self.decoder, dim=in_dim, num_ones=self.num_ones)\n",
    "    def test_objective_function(self, x, y):\n",
    "        w = self.encoder.encoder_sample(x).detach()\n",
    "        z = self.encoder.encoder_sample(y).detach()\n",
    "        # print(w, z, x, y)\n",
    "\n",
    "        w_tilde = self.embedding_sampler.run_batched_gibbs(z).detach()\n",
    "        x_tilde = self.decoder_sampler.run_batched_gibbs(w).detach()\n",
    "        \n",
    "        #print(x[0], '-->', w[0], '|', y[0], '-->', z[0])\n",
    "\n",
    "        return -self.loss_func(self.num_ones, *(z, y, w, x, w_tilde, x_tilde), *self.encoder.params(), *self.decoder.params(), *self.embedding_dynamics.params()), \\\n",
    "                mutual_info_score(w.view(-1), z.view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MI Loss and Entropy Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [],
   "source": [
    "class MutualInformationLossV3(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, *inputs):\n",
    "        num_ones = inputs[0]\n",
    "        zywx_w_tilde_ins = inputs[1:7]\n",
    "        encoder_params = inputs[7:11]\n",
    "        decoder_params = inputs[11:19]\n",
    "        embedding_params = inputs[19:27]\n",
    "\n",
    "        z, y, w, x, _, _ = zywx_w_tilde_ins\n",
    "\n",
    "        # print(encoder_params, decoder_params, embedding_params)\n",
    "        #print(x[0], '|' ,y[0])\n",
    "        #print(w[0], '|' ,z[0])\n",
    "\n",
    "        p_x_w_estimate = DVP.EnergyBasedDecoder.estimated_conditional_log_probability_a_given_b(x, w, num_ones, *decoder_params)\n",
    "        p_w_x = DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params(w, x, *encoder_params)\n",
    "        r_w_z_estimate = DVP.EnergyBasedModelEmbeddingDynamics.estimated_normalized_log_probabilities_w_given_z_params(z, w, *embedding_params)\n",
    "\n",
    "        out = p_x_w_estimate - p_w_x + r_w_z_estimate\n",
    "        ctx.num_ones = num_ones\n",
    "        ctx.save_for_backward(*zywx_w_tilde_ins, *encoder_params, *decoder_params, *embedding_params, r_w_z_estimate, out)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        num_ones = ctx.num_ones\n",
    "        z, y, w, x, w_tilde, x_tilde = ctx.saved_tensors[0:6]\n",
    "        encoder_params = ctx.saved_tensors[6:10]\n",
    "        decoder_params = ctx.saved_tensors[10:18]\n",
    "        embedding_params = ctx.saved_tensors[18:26]\n",
    "        r_w_z = ctx.saved_tensors[26]\n",
    "        MI = ctx.saved_tensors[27]\n",
    "\n",
    "        # decoder gradient handling\n",
    "        decoder_unnormalized_probs = lambda x, w, *params: DVP.EnergyBasedDecoder.unnormalized_log_probs_a_given_b_params(num_ones, x, w, *params)\n",
    "        decoder_expected_unnormalized_probs = lambda x_tilde, w, *params: DVP.EnergyBasedDecoder.expected_unnormalized_log_probs_a_given_b(num_ones, x_tilde, w, *params)\n",
    "\n",
    "        _, decoder_grad_1 = vjp(decoder_unnormalized_probs, (x, w, *decoder_params), grad_output, create_graph=False)\n",
    "        _, decoder_grad_2 = vjp(decoder_expected_unnormalized_probs, (x_tilde, w.expand(x_tilde.shape[0], -1, -1), *decoder_params), grad_output, create_graph=False)\n",
    "\n",
    "        decoder_grad = tuple(map(lambda x, y: x - y, decoder_grad_1[2:], decoder_grad_2[2:]))\n",
    "\n",
    "        # encoder gradient handling\n",
    "        _, encoder_grad_term_1 = vjp(DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params, (w, x, *encoder_params), grad_output * (MI - 1), create_graph=False)\n",
    "        encoder_grad_term_1 = encoder_grad_term_1[2:]\n",
    "\n",
    "        _, encoder_grad_term_2 = vjp(DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params, (z, y, *encoder_params), grad_output * r_w_z, create_graph=False)\n",
    "        encoder_grad_term_2 = encoder_grad_term_2[2:]\n",
    "\n",
    "        encoder_grad = tuple(map(lambda x, y: x + y, encoder_grad_term_1, encoder_grad_term_2))\n",
    "\n",
    "        # embedding gradient handling\n",
    "        _, embedding_grad_1 = vjp(DVP.EnergyBasedModelEmbeddingDynamics.unnormalized_log_probs_w_given_z_params, (z, w, *embedding_params), grad_output, create_graph=False)\n",
    "        _, embedding_grad_2 = vjp(DVP.EnergyBasedModelEmbeddingDynamics.expected_unnormalized_log_probs_w_given_z, (z.expand(w_tilde.shape[0], -1, -1), w_tilde, *embedding_params), grad_output, create_graph=False)\n",
    "\n",
    "        embedding_grad = tuple(map(lambda x, y: x - y, embedding_grad_1[2:], embedding_grad_2[2:]))\n",
    "\n",
    "        return None, None, None, None, None, None, None, *encoder_grad, *decoder_grad, *embedding_grad"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T23:06:55.230338Z",
     "start_time": "2024-07-17T23:06:55.227663Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [],
   "source": [
    "def entropy(X):\n",
    "    # Flatten the tensor to 1D\n",
    "    X_flat = X.view(-1)\n",
    "    \n",
    "    # Count the occurrences of each unique value\n",
    "    unique_vals, counts = X_flat.unique(return_counts=True)\n",
    "    probabilities = counts.float() / counts.sum()\n",
    "    entropy = -torch.sum(probabilities * torch.log(probabilities))\n",
    "        \n",
    "    return entropy.item()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T23:06:55.375472Z",
     "start_time": "2024-07-17T23:06:55.369948Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dimension Reduction Process"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [],
   "source": [
    "def run_dim_red_process(device, state_space, embedding_space_size, batch_size=256, num_steps=10000):\n",
    "\n",
    "    model = EmbeddingMI3(batch_size, state_space, embedding_space_size, num_ones=4).to(device)\n",
    "\n",
    "    # Path to the state dictionary file\n",
    "    state_dict_path = 'quantum_experiments/initializer.model'\n",
    "    specific_dict_path = f'quantum_experiments/experiment_{state_space}_{embedding_space_size}_good.model'\n",
    "\n",
    "    # Check if the state dictionary file exists\n",
    "    if os.path.exists(directory_path + specific_dict_path):\n",
    "        state_dict = torch.load(specific_dict_path, map_location=device)\n",
    "        old_state_dict = model.state_dict()\n",
    "        old_state_dict['encoder.b'] = state_dict['encoder.b'][:, :embedding_space_size]\n",
    "        old_state_dict['encoder.W'] = state_dict['encoder.W'][:, :embedding_space_size, :]\n",
    "    \n",
    "        model.load_state_dict(old_state_dict, strict=False)\n",
    "        print(\"| Successfully loaded initializer model\", specific_dict_path)\n",
    "    #else:\n",
    "        #print(f\"State dictionary file '{state_dict_path}' does not exist. Continuing without loading pre-trained weights.\")\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    # Uncomment below to test a Circuit Length of 0 (no time evolution)\n",
    "    params = generate_circuit_params(0,12)\n",
    "    #params = generate_circuit_params(file_name = directory_path + 'dense_small.param')\n",
    "    dataset = QuantumSimulationDatasetFast(params, batch_size, 4, device, inverse_density=3)\n",
    "    \n",
    "    for i, (final_state, initial_state) in enumerate(dataset):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss, loss2 = model.test_objective_function(initial_state, final_state)\n",
    "        loss = loss.mean()\n",
    "        loss.backward()\n",
    "        \n",
    "        expected_mutual_info = mutual_info_score(initial_state.view(-1), final_state.view(-1))\n",
    "        initial_entropy = entropy(initial_state)\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Adjust max_norm as needed\n",
    "        \n",
    "        # if i % 10 == 0:\n",
    "        print('| Iteration', i, ' > I(W,Z):', f\"{-loss.detach().cpu().item():,.5f}\", \n",
    "              ' I(W,Z):', f\"{loss2:,.5f}\",\n",
    "              ' I(X,Y):', f\"{expected_mutual_info:,.5f}\",\n",
    "              ' H(X):', f\"{initial_entropy:,.5f}\")\n",
    "        \n",
    "        optimizer.step()\n",
    "        if i > num_steps:\n",
    "            print('Training Terminated')\n",
    "            break\n",
    "        if i % 1000 == 999:\n",
    "            torch.save(model.state_dict(), f'quantum_experiments/experiment_{state_space}_{embedding_space_size}_{i}.model')\n",
    "\n",
    "    # Save the model at the end of training\n",
    "    final_save_path = f'quantum_experiments/experiment_{state_space}_{embedding_space_size}_final.model'\n",
    "    torch.save(model.state_dict(), final_save_path)\n",
    "    print(f\"Final model saved to {final_save_path}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T23:06:55.631605Z",
     "start_time": "2024-07-17T23:06:55.627255Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hyperparameter Tuning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T23:06:55.895234Z",
     "start_time": "2024-07-17T23:06:55.891096Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_learning_rate(device, state_space, embedding_space_size, batch_size=256, num_steps=300):\n",
    "    model = EmbeddingMI3(batch_size, state_space, embedding_space_size, num_ones=4).to(device)\n",
    "    \n",
    "    # Load parameters (adapt as needed)\n",
    "    params = generate_circuit_params(file_name=directory_path + 'dense_small.param')\n",
    "    dataset = QuantumSimulationDatasetFast(params, batch_size, 4, device, inverse_density=3)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-7)\n",
    "    lrs = []\n",
    "    losses = []\n",
    "    lr = 1e-7\n",
    "\n",
    "    for i, (final_state, initial_state) in enumerate(dataset):\n",
    "        if i > num_steps:\n",
    "            break\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = model.test_objective_function(initial_state, final_state).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        lrs.append(lr)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Increase learning rate\n",
    "        lr *= 1.05\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f'| Iteration {i}, Learning Rate: {lr:.8f}, Loss: {loss.item():.8f}')\n",
    "    \n",
    "    # Plot results\n",
    "    plt.plot(lrs, losses)\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Learning Rate')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Learning Rate Finder')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T23:07:05.929603Z",
     "start_time": "2024-07-17T23:06:56.505139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Running:  cpu\n",
      "Running Experiment 0 (12, 12, 256)\n",
      "State Space of 12 to Embedding Space of 12\n",
      "torch.Size([256, 256, 12])\n",
      "| Iteration 0  > I(W,Z): -6.33469  I(W,Z): 0.00029  I(X,Y): 0.63651  H(X): 0.63651\n",
      "torch.Size([256, 256, 12])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[207], line 23\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRunning Experiment\u001B[39m\u001B[38;5;124m'\u001B[39m, i, params)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mState Space of\u001B[39m\u001B[38;5;124m'\u001B[39m, params[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mto Embedding Space of\u001B[39m\u001B[38;5;124m'\u001B[39m, params[\u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m---> 23\u001B[0m run_dim_red_process(device, \u001B[38;5;241m*\u001B[39mparams)\n",
      "Cell \u001B[0;32mIn[205], line 31\u001B[0m, in \u001B[0;36mrun_dim_red_process\u001B[0;34m(device, state_space, embedding_space_size, batch_size, num_steps)\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, (final_state, initial_state) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataset):\n\u001B[1;32m     30\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 31\u001B[0m     loss, loss2 \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtest_objective_function(initial_state, final_state)\n\u001B[1;32m     32\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mmean()\n\u001B[1;32m     33\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "Cell \u001B[0;32mIn[202], line 27\u001B[0m, in \u001B[0;36mEmbeddingMI3.test_objective_function\u001B[0;34m(self, x, y)\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# print(x[0], '-->', w[0], '|', y[0], '-->', z[0])\u001B[39;00m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# print(x_tilde[0][0], '<--', w_tilde[0][0], '<--', z[0])\u001B[39;00m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28mprint\u001B[39m(w_tilde\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m---> 27\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m-\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_func(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_ones, \u001B[38;5;241m*\u001B[39m(z, y, w, x, w_tilde, x_tilde), \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder\u001B[38;5;241m.\u001B[39mparams(), \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder\u001B[38;5;241m.\u001B[39mparams(), \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding_dynamics\u001B[38;5;241m.\u001B[39mparams()), \\\n\u001B[1;32m     28\u001B[0m         mutual_info_score(w\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m), z\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/autograd/function.py:598\u001B[0m, in \u001B[0;36mFunction.apply\u001B[0;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[1;32m    595\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_are_functorch_transforms_active():\n\u001B[1;32m    596\u001B[0m     \u001B[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001B[39;00m\n\u001B[1;32m    597\u001B[0m     args \u001B[38;5;241m=\u001B[39m _functorch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39munwrap_dead_wrappers(args)\n\u001B[0;32m--> 598\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m    600\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_setup_ctx_defined:\n\u001B[1;32m    601\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    602\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIn order to use an autograd.Function with functorch transforms \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    603\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    604\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstaticmethod. For more details, please see \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    605\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    606\u001B[0m     )\n",
      "Cell \u001B[0;32mIn[203], line 16\u001B[0m, in \u001B[0;36mMutualInformationLossV3.forward\u001B[0;34m(ctx, *inputs)\u001B[0m\n\u001B[1;32m     10\u001B[0m z, y, w, x, _, _ \u001B[38;5;241m=\u001B[39m zywx_w_tilde_ins\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# print(encoder_params, decoder_params, embedding_params)\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m#print(x[0], '|' ,y[0])\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m#print(w[0], '|' ,z[0])\u001B[39;00m\n\u001B[0;32m---> 16\u001B[0m p_x_w_estimate \u001B[38;5;241m=\u001B[39m DVP\u001B[38;5;241m.\u001B[39mEnergyBasedDecoder\u001B[38;5;241m.\u001B[39mestimated_conditional_log_probability_a_given_b(x, w, num_ones, \u001B[38;5;241m*\u001B[39mdecoder_params)\n\u001B[1;32m     17\u001B[0m p_w_x \u001B[38;5;241m=\u001B[39m DVP\u001B[38;5;241m.\u001B[39mBoltzmannBasedEncoder\u001B[38;5;241m.\u001B[39mconditional_log_probability_a_given_b_params(w, x, \u001B[38;5;241m*\u001B[39mencoder_params)\n\u001B[1;32m     18\u001B[0m r_w_z_estimate \u001B[38;5;241m=\u001B[39m DVP\u001B[38;5;241m.\u001B[39mEnergyBasedModelEmbeddingDynamics\u001B[38;5;241m.\u001B[39mestimated_normalized_log_probabilities_w_given_z_params(z, w, \u001B[38;5;241m*\u001B[39membedding_params)\n",
      "File \u001B[0;32m~/Desktop/QuantumDynamicsAI/DiscreteVariationalParameterizationsDeepV3.py:443\u001B[0m, in \u001B[0;36mestimated_conditional_log_probability_a_given_b\u001B[0;34m(a, b, num_ones, W1, b1, W2, b2, W3, b3, W4, b4)\u001B[0m\n\u001B[1;32m    440\u001B[0m log_probs \u001B[38;5;241m=\u001B[39m EnergyBasedDecoder\u001B[38;5;241m.\u001B[39munnormalized_log_probs_a_given_b_double_batched_params(num_ones, A, b, W1, b1,\n\u001B[1;32m    441\u001B[0m                                                                                       W2, b2, W3, b3, W4, b4)\n\u001B[1;32m    442\u001B[0m partitions \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mlogsumexp(log_probs[:, bit_strings\u001B[38;5;241m.\u001B[39msum(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m==\u001B[39m num_ones, :], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m--> 443\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m partitions\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/Desktop/QuantumDynamicsAI/DiscreteVariationalParameterizationsDeepV3.py:419\u001B[0m, in \u001B[0;36mestimate_log_partition_function\u001B[0;34m(num_ones, b, initial_state, W1, b1, W2, b2, W3, b3, W4, b4, samples)\u001B[0m\n\u001B[1;32m    415\u001B[0m dim_a \u001B[38;5;241m=\u001B[39m a\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m]\n\u001B[1;32m    416\u001B[0m dim_b \u001B[38;5;241m=\u001B[39m b\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m]\n\u001B[1;32m    417\u001B[0m energy \u001B[38;5;241m=\u001B[39m EnergyBasedDecoder\u001B[38;5;241m.\u001B[39menergy(a\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, dim_a), b\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, dim_b),\n\u001B[1;32m    418\u001B[0m                                    \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear_1_weight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear_1_bias,\n\u001B[0;32m--> 419\u001B[0m                                    \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear_2_weight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear_2_bias,\n\u001B[1;32m    420\u001B[0m                                    \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear_3_weight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear_3_bias,\n\u001B[1;32m    421\u001B[0m                                    \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear_4_weight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear_4_bias)\u001B[38;5;241m.\u001B[39mreshape(\n\u001B[1;32m    422\u001B[0m     first_dim, second_dim, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    423\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mwhere(a\u001B[38;5;241m.\u001B[39msum(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, keepdim\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_ones, \u001B[38;5;241m-\u001B[39menergy,\n\u001B[1;32m    424\u001B[0m                    \u001B[38;5;241m-\u001B[39mtorch\u001B[38;5;241m.\u001B[39mones_like(energy) \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minf\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('Device Running: ', device)\n",
    "\n",
    "    # experiments = [\n",
    "    #     (12, 2, 1024), (12, 3, 1024), (12, 4, 1024), (12, 5, 1024),\n",
    "    #     (12, 6, 1024), (12, 7, 1024), (12, 8, 1024), (12, 9, 1024),\n",
    "    #     (12, 10, 1024), (12, 11, 1024), (12, 12, 1024)\n",
    "    # ]\n",
    "    \n",
    "    experiments = [\n",
    "         (12, 8, 1024), (12, 2, 1024), (12, 4, 1024), (12, 12, 1024)\n",
    "    ]\n",
    "\n",
    "    for i, params in enumerate(experiments):\n",
    "        print('Running Experiment', i, params)\n",
    "        print('State Space of', params[0], 'to Embedding Space of', params[1])\n",
    "        \n",
    "        run_dim_red_process(device, *params)\n",
    "        #find_learning_rate(device, *params)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPMeg+QPYunP2XoyRdolJ5b",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
