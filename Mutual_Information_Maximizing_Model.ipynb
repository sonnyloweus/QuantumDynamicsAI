{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B70wgiwMLH8R",
    "outputId": "7a21aa53-f2bc-470a-aa1d-ae8f7b734e03",
    "ExecuteTime": {
     "end_time": "2024-07-15T19:05:02.209953Z",
     "start_time": "2024-07-15T19:05:02.205107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Symmetric_Exclusion_Process _Simulator.ipynb', 'Quantum_Transformer.ipynb', 'dense_small.param', '.DS_Store', 'quantum_experiments', 'DiscreteVariationalParameterizationsDeepV2.py', 'DiscreteVariationalParameterizationsDeepV3.py', 'README.md', 'Mutual_Information_Maximizing_Model.ipynb', 'temp.txt', '.ipynb_checkpoints', '.git', 'Quantum_Brickworks_Circuit_Generator.ipynb', 'QuantumSimulatorDataset.py', 'GibbsSampling.py', '.idea']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u3NkfGVa1e-e",
    "outputId": "275fb468-b45a-4ab3-fa7e-ad7a08f19637",
    "ExecuteTime": {
     "end_time": "2024-07-15T19:05:12.173926Z",
     "start_time": "2024-07-15T19:05:05.339444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.11/site-packages (2.3.1)\r\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.11/site-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1.3)\r\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.11/site-packages (from torch) (2023.10.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\r\n",
      "Requirement already satisfied: qiskit-aer in /opt/anaconda3/lib/python3.11/site-packages (0.14.2)\r\n",
      "Requirement already satisfied: qiskit>=0.45.2 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit-aer) (1.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.16.3 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit-aer) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit-aer) (1.11.4)\r\n",
      "Requirement already satisfied: psutil>=5 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit-aer) (5.9.0)\r\n",
      "Requirement already satisfied: rustworkx>=0.14.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (0.14.2)\r\n",
      "Requirement already satisfied: sympy>=1.3 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (1.12)\r\n",
      "Requirement already satisfied: dill>=0.3 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (0.3.7)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (2.8.2)\r\n",
      "Requirement already satisfied: stevedore>=3.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (5.2.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (4.9.0)\r\n",
      "Requirement already satisfied: symengine>=0.11 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (0.11.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.0->qiskit>=0.45.2->qiskit-aer) (1.16.0)\r\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from stevedore>=3.0.0->qiskit>=0.45.2->qiskit-aer) (6.0.0)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy>=1.3->qiskit>=0.45.2->qiskit-aer) (1.3.0)\r\n",
      "Requirement already satisfied: qiskit in /opt/anaconda3/lib/python3.11/site-packages (1.1.0)\r\n",
      "Requirement already satisfied: rustworkx>=0.14.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (0.14.2)\r\n",
      "Requirement already satisfied: numpy<3,>=1.17 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (1.11.4)\r\n",
      "Requirement already satisfied: sympy>=1.3 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (1.12)\r\n",
      "Requirement already satisfied: dill>=0.3 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (0.3.7)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (2.8.2)\r\n",
      "Requirement already satisfied: stevedore>=3.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (5.2.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (4.9.0)\r\n",
      "Requirement already satisfied: symengine>=0.11 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (0.11.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.0->qiskit) (1.16.0)\r\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from stevedore>=3.0.0->qiskit) (6.0.0)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy>=1.3->qiskit) (1.3.0)\r\n",
      "Requirement already satisfied: pylatexenc in /opt/anaconda3/lib/python3.11/site-packages (2.10)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install qiskit-aer\n",
    "!pip install qiskit\n",
    "!pip install pylatexenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "id": "u6Rq3bU6sHLw",
    "outputId": "56cf9751-34c6-47ac-c8b8-86c58473b086",
    "ExecuteTime": {
     "end_time": "2024-07-15T19:07:33.806435Z",
     "start_time": "2024-07-15T19:07:22.831188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Running:  cpu\n",
      "Running Experiment 0 (12, 2, 1024)\n",
      "State Space of 12 to Embedding Space of 2\n",
      "Successfully loaded initializer model quantum_experiments/experiment_12_2.model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 222\u001B[0m\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRunning Experiment\u001B[39m\u001B[38;5;124m'\u001B[39m, i, params)\n\u001B[1;32m    221\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mState Space of\u001B[39m\u001B[38;5;124m'\u001B[39m, params[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mto Embedding Space of\u001B[39m\u001B[38;5;124m'\u001B[39m, params[\u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m--> 222\u001B[0m run_dim_red_process(device, \u001B[38;5;241m*\u001B[39mparams)\n",
      "Cell \u001B[0;32mIn[13], line 189\u001B[0m, in \u001B[0;36mrun_dim_red_process\u001B[0;34m(device, state_space, embedding_space_size, batch_size, num_steps)\u001B[0m\n\u001B[1;32m    186\u001B[0m params \u001B[38;5;241m=\u001B[39m generate_circuit_params(file_name \u001B[38;5;241m=\u001B[39m directory_path \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdense_small.param\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    187\u001B[0m dataset \u001B[38;5;241m=\u001B[39m QuantumSimulationDatasetFast(params, batch_size, \u001B[38;5;241m4\u001B[39m, device, inverse_density\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m)\n\u001B[0;32m--> 189\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, (final_state, initial_state) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataset):\n\u001B[1;32m    190\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m    191\u001B[0m     loss, actual_loss \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtest_objective_function(initial_state, final_state)\n",
      "File \u001B[0;32m~/Desktop/QuantumDynamicsAI/QuantumSimulatorDataset.py:188\u001B[0m, in \u001B[0;36mQuantumSimulationDatasetFast.__getitem__\u001B[0;34m(self, _)\u001B[0m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, _):\n\u001B[0;32m--> 188\u001B[0m     initial_states, final_states \u001B[38;5;241m=\u001B[39m run_batch_dense_torch(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmat, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_qubits, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_size,\n\u001B[1;32m    189\u001B[0m                                                          \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_samples, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minverse_density)\n\u001B[1;32m    190\u001B[0m     initial_states \u001B[38;5;241m=\u001B[39m initial_states\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mexpand_as(final_states)\u001B[38;5;241m.\u001B[39mclone()\n\u001B[1;32m    191\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m initial_states\u001B[38;5;241m.\u001B[39mflatten(end_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m), final_states\u001B[38;5;241m.\u001B[39mflatten(end_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/Desktop/QuantumDynamicsAI/QuantumSimulatorDataset.py:170\u001B[0m, in \u001B[0;36mrun_batch_dense_torch\u001B[0;34m(unitary_circuit, num_qbits, batch_size, num_samples, device, inverse_density)\u001B[0m\n\u001B[1;32m    168\u001B[0m decimal \u001B[38;5;241m=\u001B[39m decimal_from_initial_state(bits, num_qbits)\n\u001B[1;32m    169\u001B[0m one_hot \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mone_hot(decimal, num_classes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m num_qbits)\n\u001B[0;32m--> 170\u001B[0m final_state \u001B[38;5;241m=\u001B[39m get_final_state_vector(one_hot\u001B[38;5;241m.\u001B[39mcfloat(), unitary_circuit)\n\u001B[1;32m    171\u001B[0m sample \u001B[38;5;241m=\u001B[39m sample_from_state_vector(final_state, num_qbits, num_samples)\n\u001B[1;32m    172\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m bits\u001B[38;5;241m.\u001B[39mfloat(), sample\n",
      "File \u001B[0;32m~/Desktop/QuantumDynamicsAI/QuantumSimulatorDataset.py:139\u001B[0m, in \u001B[0;36mget_final_state_vector\u001B[0;34m(batched_initial_state, op_mat)\u001B[0m\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_final_state_vector\u001B[39m(batched_initial_state, op_mat):\n\u001B[0;32m--> 139\u001B[0m     sampled_final_state \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmatmul(op_mat\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m), batched_initial_state\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m2\u001B[39m))\n\u001B[1;32m    140\u001B[0m     probs \u001B[38;5;241m=\u001B[39m sampled_final_state \u001B[38;5;241m*\u001B[39m sampled_final_state\u001B[38;5;241m.\u001B[39mconj()\n\u001B[1;32m    141\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mreal(probs)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from re import X\n",
    "from math import e\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import DiscreteVariationalParameterizationsDeepV3 as DVP\n",
    "from torch.autograd.functional import vjp\n",
    "from torch.autograd.function import Function\n",
    "from QuantumSimulatorDataset import QuantumSimulationDatasetFast, generate_circuit_params\n",
    "from GibbsSampling import BatchedConditionalGibbsSampler, BatchedConditionalDoubleGibbsSampler\n",
    "\n",
    "# uncomment if mounting google drive\n",
    "directory_path = ''\n",
    "\n",
    "class EmbeddingMI3(nn.Module):\n",
    "    def __init__(self, batch_size, in_dim, out_dim, num_ones):\n",
    "        super().__init__()\n",
    "        self.encoder = DVP.BoltzmannBasedEncoder(in_dim=in_dim, out_dim=out_dim)\n",
    "        self.decoder = DVP.EnergyBasedDecoder(in_dim=out_dim, out_dim=in_dim, num_ones=num_ones)\n",
    "        self.num_ones = num_ones\n",
    "        self.embedding_dynamics = DVP.EnergyBasedModelEmbeddingDynamics(dim=out_dim)\n",
    "        self.loss_func = MutualInformationLossV3.apply\n",
    "        self.embedding_sampler = BatchedConditionalGibbsSampler(batch_size=batch_size, num_samples=256, # needs to be tuned\n",
    "                                                                mixing_time=5, # seems like this can be low and still work\n",
    "                                                                joint_distribution=self.embedding_dynamics)\n",
    "        self.decoder_sampler = BatchedConditionalDoubleGibbsSampler(batch_size=batch_size, num_samples=256, # needs to be tuned\n",
    "                                                                mixing_time=24, # seems like this can be low and still work\n",
    "                                                                joint_distribution=self.decoder, dim=in_dim, num_ones=self.num_ones)\n",
    "    def test_objective_function(self, x, y):\n",
    "        w = self.encoder.encoder_sample(x).detach()\n",
    "        z = self.encoder.encoder_sample(y).detach()\n",
    "        # print(w, z, x, y)\n",
    "\n",
    "        w_tilde = self.embedding_sampler.run_batched_gibbs(z).detach()\n",
    "        x_tilde = self.decoder_sampler.run_batched_gibbs(w).detach()\n",
    "\n",
    "        return -self.loss_func(self.num_ones, *(z, y, w, x, w_tilde, x_tilde), *self.encoder.params(), *self.decoder.params(), *self.embedding_dynamics.params()), torch.tensor([ 0.])\n",
    "\n",
    "\n",
    "    def calculate_mutual_information(self, x, y, num_ones):\n",
    "        in_dim = x.shape[1]\n",
    "        hidden_dim = 32\n",
    "\n",
    "        linear_1_weight = nn.Parameter(torch.zeros((hidden_dim, in_dim * in_dim), device=x.device))\n",
    "        linear_1_bias = nn.Parameter(torch.zeros((hidden_dim), device=x.device))\n",
    "\n",
    "        linear_2_weight = nn.Parameter(torch.zeros((hidden_dim, hidden_dim), device=x.device))\n",
    "        linear_2_bias = nn.Parameter(torch.zeros((hidden_dim), device=x.device))\n",
    "\n",
    "        linear_3_weight = nn.Parameter(torch.zeros((hidden_dim, hidden_dim), device=x.device))\n",
    "        linear_3_bias = nn.Parameter(torch.zeros((hidden_dim), device=x.device))\n",
    "\n",
    "        linear_4_weight = nn.Parameter(torch.zeros((1, hidden_dim), device=x.device))\n",
    "        linear_4_bias = nn.Parameter(torch.zeros((1), device=x.device))\n",
    "        temp_params1 = (linear_1_weight, linear_1_bias, linear_2_weight, linear_2_bias,\n",
    "                       linear_3_weight, linear_3_bias, linear_4_weight, linear_4_bias)\n",
    "\n",
    "        b = nn.Parameter(torch.zeros((1, in_dim), device=x.device))\n",
    "        W = nn.Parameter(torch.zeros((1, in_dim, in_dim), device=x.device))\n",
    "        padding1 = nn.Parameter(torch.zeros(1, device=x.device))\n",
    "        padding2 = nn.Parameter(torch.zeros(1, device=x.device))\n",
    "        temp_params2 = (b, W, padding1, padding2)\n",
    "\n",
    "        p_x_y_estimate = DVP.EnergyBasedDecoder.estimated_conditional_log_probability_a_given_b(x, y, num_ones, *temp_params1)\n",
    "        p_y_x_estimate = DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params(y, x, *temp_params2)\n",
    "        mutual_info_x_y = p_x_y_estimate - p_y_x_estimate\n",
    "\n",
    "        return mutual_info_x_y\n",
    "\n",
    "    def test_calculate_mutual_information_loss(self, x, y):\n",
    "        # Ensure x and y are tensors and have the same shape\n",
    "        assert x.shape == y.shape, \"x and y must have the same shape\"\n",
    "\n",
    "        # Compute joint histogram\n",
    "        joint_hist = torch.histc((x * y).float(), bins=256, min=0, max=256)\n",
    "\n",
    "        # Compute marginal histograms\n",
    "        x_hist = torch.histc(x.float(), bins=256, min=0, max=256)\n",
    "        y_hist = torch.histc(y.float(), bins=256, min=0, max=256)\n",
    "\n",
    "        # Normalize histograms to get probabilities\n",
    "        joint_prob = joint_hist / joint_hist.sum()\n",
    "        x_prob = x_hist / x_hist.sum()\n",
    "        y_prob = y_hist / y_hist.sum()\n",
    "\n",
    "        # Compute the entropies\n",
    "        H_x = -torch.sum(x_prob * torch.log(x_prob + 1e-12))\n",
    "        H_y = -torch.sum(y_prob * torch.log(y_prob + 1e-12))\n",
    "        H_xy = -torch.sum(joint_prob * torch.log(joint_prob + 1e-12))\n",
    "\n",
    "        # Compute mutual information\n",
    "        I_xy = H_x + H_y - H_xy\n",
    "\n",
    "        # Return mutual information loss (negative mutual information)\n",
    "        mutual_info_loss = -I_xy\n",
    "\n",
    "        return mutual_info_loss\n",
    "\n",
    "class MutualInformationLossV3(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, *inputs):\n",
    "        num_ones = inputs[0]\n",
    "        zywx_w_tilde_ins = inputs[1:7]\n",
    "        encoder_params = inputs[7:11]\n",
    "        decoder_params = inputs[11:19]\n",
    "        embedding_params = inputs[19:27]\n",
    "\n",
    "        z, y, w, x, _, _ = zywx_w_tilde_ins\n",
    "\n",
    "        #print(x[0], '|' ,y[0])\n",
    "        #print(w[0], '|' ,z[0])\n",
    "\n",
    "        p_x_w_estimate = DVP.EnergyBasedDecoder.estimated_conditional_log_probability_a_given_b(x, w, num_ones, *decoder_params)\n",
    "        p_w_x = DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params(w, x, *encoder_params)\n",
    "        r_w_z_estimate = DVP.EnergyBasedModelEmbeddingDynamics.estimated_normalized_log_probabilities_w_given_z_params(z, w, *embedding_params)\n",
    "\n",
    "        out = p_x_w_estimate - p_w_x + r_w_z_estimate\n",
    "        ctx.num_ones = num_ones\n",
    "        ctx.save_for_backward(*zywx_w_tilde_ins, *encoder_params, *decoder_params, *embedding_params, r_w_z_estimate, out)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        num_ones = ctx.num_ones\n",
    "        z, y, w, x, w_tilde, x_tilde = ctx.saved_tensors[0:6]\n",
    "        encoder_params = ctx.saved_tensors[6:10]\n",
    "        decoder_params = ctx.saved_tensors[10:18]\n",
    "        embedding_params = ctx.saved_tensors[18:26]\n",
    "        r_w_z = ctx.saved_tensors[26]\n",
    "        MI = ctx.saved_tensors[27]\n",
    "\n",
    "        decoder_unnormalized_probs = lambda x, w, *params: DVP.EnergyBasedDecoder.unnormalized_log_probs_a_given_b_params(num_ones, x, w, *params)\n",
    "        decoder_expected_unnormalized_probs = lambda x_tilde, w, *params: DVP.EnergyBasedDecoder.expected_unnormalized_log_probs_a_given_b(num_ones, x_tilde, w, *params)\n",
    "\n",
    "        _, decoder_grad_1 = vjp(decoder_unnormalized_probs, (x, w, *decoder_params), grad_output, create_graph=False)\n",
    "        _, decoder_grad_2 = vjp(decoder_expected_unnormalized_probs, (x_tilde, w.expand(x_tilde.shape[0], -1, -1), *decoder_params), grad_output, create_graph=False)\n",
    "\n",
    "        decoder_grad = tuple(map(lambda x, y: x - y, decoder_grad_1[2:], decoder_grad_2[2:]))\n",
    "\n",
    "        #_, encoder_grad_term_1 = vjp(DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params, (w, x, encoder_params[0], encoder_params[1]), grad_output * (MI - 1), create_graph=False)\n",
    "        _, encoder_grad_term_1 = vjp(DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params, (w, x, *encoder_params), grad_output * (MI - 1), create_graph=False)\n",
    "        encoder_grad_term_1 = encoder_grad_term_1[2:]\n",
    "\n",
    "        #_, encoder_grad_term_2 = vjp(DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params, (z, y, encoder_params[0], encoder_params[1]), grad_output * r_w_z, create_graph=False)\n",
    "        _, encoder_grad_term_2 = vjp(DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params, (z, y, *encoder_params), grad_output * r_w_z, create_graph=False)\n",
    "        encoder_grad_term_2 = encoder_grad_term_2[2:]\n",
    "\n",
    "        encoder_grad = tuple(map(lambda x, y: x + y, encoder_grad_term_1, encoder_grad_term_2))\n",
    "\n",
    "        _, embedding_grad_1 = vjp(DVP.EnergyBasedModelEmbeddingDynamics.unnormalized_log_probs_w_given_z_params, (z, w, *embedding_params), grad_output, create_graph=False)\n",
    "        _, embedding_grad_2 = vjp(DVP.EnergyBasedModelEmbeddingDynamics.expected_unnormalized_log_probs_w_given_z, (z.expand(w_tilde.shape[0], -1, -1), w_tilde, *embedding_params), grad_output, create_graph=False)\n",
    "\n",
    "        embedding_grad = tuple(map(lambda x, y: x - y, embedding_grad_1[2:], embedding_grad_2[2:]))\n",
    "\n",
    "        #print(len(encoder_grad), len(decoder_grad), len(embedding_grad))\n",
    "\n",
    "        return None, None, None, None, None, None, None, *encoder_grad, *decoder_grad, *embedding_grad\n",
    "\n",
    "def run_dim_red_process(device, state_space, embedding_space_size, batch_size=256, num_steps=10000):\n",
    "\n",
    "    model = EmbeddingMI3(batch_size, state_space, embedding_space_size, num_ones=4)\n",
    "\n",
    "    # Path to the state dictionary file\n",
    "    state_dict_path = 'quantum_experiments/initializer.model'\n",
    "    specific_dict_path = f'quantum_experiments/experiment_{state_space}_{embedding_space_size}.model'\n",
    "\n",
    "    # Check if the state dictionary file exists\n",
    "    if os.path.exists(directory_path + specific_dict_path):\n",
    "        # Load the state dictionary\n",
    "        state_dict = torch.load(specific_dict_path, map_location=device)\n",
    "        # Get the current state dictionary of the model\n",
    "        old_state_dict = model.state_dict()\n",
    "        # Modify the state dictionary to match the embedding_space_size\n",
    "        old_state_dict['encoder.b'] = state_dict['encoder.b'][:, :embedding_space_size]\n",
    "        old_state_dict['encoder.W'] = state_dict['encoder.W'][:, :embedding_space_size, :]\n",
    "\n",
    "        # Load the adjusted state dictionary into the model\n",
    "        model.load_state_dict(old_state_dict, strict=False)\n",
    "        print(\"| Successfully loaded initializer model\", specific_dict_path)\n",
    "    #else:\n",
    "        #print(f\"State dictionary file '{state_dict_path}' does not exist. Continuing without loading pre-trained weights.\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "    #params = generate_circuit_params(12,12)\n",
    "    params = generate_circuit_params(file_name = directory_path + 'dense_small.param')\n",
    "    dataset = QuantumSimulationDatasetFast(params, batch_size, 4, device, inverse_density=3)\n",
    "\n",
    "    for i, (final_state, initial_state) in enumerate(dataset):\n",
    "        optimizer.zero_grad()\n",
    "        loss, actual_loss = model.test_objective_function(initial_state, final_state)\n",
    "        loss = loss.mean()\n",
    "        actual_loss = actual_loss.mean()\n",
    "        loss.backward()\n",
    "        if i % 10 == 0:\n",
    "            print('| Iteration', i, 'I(W,Z) > ', f\"{-loss.detach().cpu().item():,.5f}\", ' I(X,Y) > ', f\"{-actual_loss.detach().cpu().item():,.6f}\")\n",
    "        optimizer.step()\n",
    "        if i > num_steps:\n",
    "            print('Training Terminated')\n",
    "            break\n",
    "        if i % 1000 == 999:\n",
    "            torch.save(model.state_dict(), f'quantum_experiments/experiment_{state_space}_{embedding_space_size}_{i}.model')\n",
    "\n",
    "    # Save the model at the end of training\n",
    "    final_save_path = f'quantum_experiments/experiment_{state_space}_{embedding_space_size}_final.model'\n",
    "    torch.save(model.state_dict(), final_save_path)\n",
    "    print(f\"Final model saved to {final_save_path}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('Device Running: ', device)\n",
    "\n",
    "    experiments = [\n",
    "        (12, 2, 1024), (12, 3, 1024), (12, 4, 1024), (12, 5, 1024),\n",
    "        (12, 6, 1024), (12, 7, 1024), (12, 8, 1024), (12, 9, 1024),\n",
    "        (12, 10, 1024), (12, 11, 1024), (12, 12, 1024)\n",
    "    ]\n",
    "\n",
    "    for i, params in enumerate(experiments):\n",
    "        print('Running Experiment', i, params)\n",
    "        print('State Space of', params[0], 'to Embedding Space of', params[1])\n",
    "        run_dim_red_process(device, *params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPMeg+QPYunP2XoyRdolJ5b",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
