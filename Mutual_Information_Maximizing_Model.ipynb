{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "authorship_tag": "ABX9TyPMeg+QPYunP2XoyRdolJ5b",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B70wgiwMLH8R",
    "outputId": "7a21aa53-f2bc-470a-aa1d-ae8f7b734e03",
    "ExecuteTime": {
     "end_time": "2024-07-12T21:21:55.730731Z",
     "start_time": "2024-07-12T21:21:55.729084Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Symmetric_Exclusion_Process _Simulator.ipynb', 'Quantum_Transformer.ipynb', 'DiscreteVariationalParameterizationsDeepV2.py', 'DiscreteVariationalParameterizationsDeepV3.py', 'README.md', 'Mutual_Information_Maximizing_Model.ipynb', '.git', 'Quantum_Brickworks_Circuit_Generator.ipynb', 'QuantumSimulatorDataset.py', 'GibbsSampling.py', '.idea']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install qiskit-aer\n",
    "!pip install qiskit\n",
    "!pip install pylatexenc"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "u3NkfGVa1e-e",
    "outputId": "275fb468-b45a-4ab3-fa7e-ad7a08f19637",
    "ExecuteTime": {
     "end_time": "2024-07-12T21:22:05.612062Z",
     "start_time": "2024-07-12T21:21:58.141076Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting qiskit-aer\r\n",
      "  Downloading qiskit_aer-0.14.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.1 kB)\r\n",
      "Requirement already satisfied: qiskit>=0.45.2 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit-aer) (1.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.16.3 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit-aer) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit-aer) (1.11.4)\r\n",
      "Requirement already satisfied: psutil>=5 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit-aer) (5.9.0)\r\n",
      "Requirement already satisfied: rustworkx>=0.14.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (0.14.2)\r\n",
      "Requirement already satisfied: sympy>=1.3 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (1.12)\r\n",
      "Requirement already satisfied: dill>=0.3 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (0.3.7)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (2.8.2)\r\n",
      "Requirement already satisfied: stevedore>=3.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (5.2.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (4.9.0)\r\n",
      "Requirement already satisfied: symengine>=0.11 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (0.11.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.0->qiskit>=0.45.2->qiskit-aer) (1.16.0)\r\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from stevedore>=3.0.0->qiskit>=0.45.2->qiskit-aer) (6.0.0)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy>=1.3->qiskit>=0.45.2->qiskit-aer) (1.3.0)\r\n",
      "Downloading qiskit_aer-0.14.2-cp311-cp311-macosx_11_0_arm64.whl (2.2 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.2/2.2 MB\u001B[0m \u001B[31m4.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m0m\r\n",
      "\u001B[?25hInstalling collected packages: qiskit-aer\r\n",
      "Successfully installed qiskit-aer-0.14.2\r\n",
      "Requirement already satisfied: qiskit in /opt/anaconda3/lib/python3.11/site-packages (1.1.0)\r\n",
      "Requirement already satisfied: rustworkx>=0.14.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (0.14.2)\r\n",
      "Requirement already satisfied: numpy<3,>=1.17 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (1.11.4)\r\n",
      "Requirement already satisfied: sympy>=1.3 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (1.12)\r\n",
      "Requirement already satisfied: dill>=0.3 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (0.3.7)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (2.8.2)\r\n",
      "Requirement already satisfied: stevedore>=3.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (5.2.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (4.9.0)\r\n",
      "Requirement already satisfied: symengine>=0.11 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (0.11.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.0->qiskit) (1.16.0)\r\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from stevedore>=3.0.0->qiskit) (6.0.0)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy>=1.3->qiskit) (1.3.0)\r\n",
      "Collecting pylatexenc\r\n",
      "  Downloading pylatexenc-2.10.tar.gz (162 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m162.6/162.6 kB\u001B[0m \u001B[31m817.8 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hBuilding wheels for collected packages: pylatexenc\r\n",
      "  Building wheel for pylatexenc (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136818 sha256=3650090752c224091b97535ef6a2e804929c336ca675b32f9c3c5420933dcca8\r\n",
      "  Stored in directory: /Users/sonny/Library/Caches/pip/wheels/b1/7a/33/9fdd892f784ed4afda62b685ae3703adf4c91aa0f524c28f03\r\n",
      "Successfully built pylatexenc\r\n",
      "Installing collected packages: pylatexenc\r\n",
      "Successfully installed pylatexenc-2.10\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from re import X\n",
    "from math import e\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import DiscreteVariationalParameterizationsDeepV3 as DVP\n",
    "from torch.autograd.functional import vjp\n",
    "from torch.autograd.function import Function\n",
    "from QuantumSimulatorDataset import QuantumSimulationDatasetFast, generate_circuit_params\n",
    "from GibbsSampling import BatchedConditionalGibbsSampler, BatchedConditionalDoubleGibbsSampler\n",
    "\n",
    "class EmbeddingMI3(nn.Module):\n",
    "    def __init__(self, batch_size, in_dim, out_dim, num_ones):\n",
    "        super().__init__()\n",
    "        self.encoder = DVP.BoltzmannBasedEncoder(in_dim=in_dim, out_dim=out_dim)\n",
    "        self.decoder = DVP.EnergyBasedDecoder(in_dim=out_dim, out_dim=in_dim, num_ones=num_ones)\n",
    "        self.num_ones = num_ones\n",
    "        self.embedding_dynamics = DVP.EnergyBasedModelEmbeddingDynamics(dim=out_dim)\n",
    "        self.loss_func = MutualInformationLossV3.apply\n",
    "        self.embedding_sampler = BatchedConditionalGibbsSampler(batch_size=batch_size, num_samples=256, # needs to be tuned\n",
    "                                                                mixing_time=5, # seems like this can be low and still work\n",
    "                                                                joint_distribution=self.embedding_dynamics)\n",
    "        self.decoder_sampler = BatchedConditionalDoubleGibbsSampler(batch_size=batch_size, num_samples=256, # needs to be tuned\n",
    "                                                                mixing_time=24, # seems like this can be low and still work\n",
    "                                                                joint_distribution=self.decoder, dim=in_dim, num_ones=self.num_ones)\n",
    "    def test_objective_function(self, x, y):\n",
    "        w = self.encoder.encoder_sample(x).detach()\n",
    "        z = self.encoder.encoder_sample(y).detach()\n",
    "        # print(w, z, x, y)\n",
    "\n",
    "        w_tilde = self.embedding_sampler.run_batched_gibbs(z).detach()\n",
    "        x_tilde = self.decoder_sampler.run_batched_gibbs(w).detach()\n",
    "\n",
    "        return -self.loss_func(self.num_ones, *(z, y, w, x, w_tilde, x_tilde), *self.encoder.params(), *self.decoder.params(), *self.embedding_dynamics.params()), torch.tensor([ 0.])\n",
    "\n",
    "\n",
    "    def calculate_mutual_information(self, x, y, num_ones):\n",
    "        in_dim = x.shape[1]\n",
    "        hidden_dim = 32\n",
    "\n",
    "        linear_1_weight = nn.Parameter(torch.zeros((hidden_dim, in_dim * in_dim), device=x.device))\n",
    "        linear_1_bias = nn.Parameter(torch.zeros((hidden_dim), device=x.device))\n",
    "\n",
    "        linear_2_weight = nn.Parameter(torch.zeros((hidden_dim, hidden_dim), device=x.device))\n",
    "        linear_2_bias = nn.Parameter(torch.zeros((hidden_dim), device=x.device))\n",
    "\n",
    "        linear_3_weight = nn.Parameter(torch.zeros((hidden_dim, hidden_dim), device=x.device))\n",
    "        linear_3_bias = nn.Parameter(torch.zeros((hidden_dim), device=x.device))\n",
    "\n",
    "        linear_4_weight = nn.Parameter(torch.zeros((1, hidden_dim), device=x.device))\n",
    "        linear_4_bias = nn.Parameter(torch.zeros((1), device=x.device))\n",
    "        temp_params1 = (linear_1_weight, linear_1_bias, linear_2_weight, linear_2_bias,\n",
    "                       linear_3_weight, linear_3_bias, linear_4_weight, linear_4_bias)\n",
    "\n",
    "        b = nn.Parameter(torch.zeros((1, in_dim), device=x.device))\n",
    "        W = nn.Parameter(torch.zeros((1, in_dim, in_dim), device=x.device))\n",
    "        padding1 = nn.Parameter(torch.zeros(1, device=x.device))\n",
    "        padding2 = nn.Parameter(torch.zeros(1, device=x.device))\n",
    "        temp_params2 = (b, W, padding1, padding2)\n",
    "\n",
    "        p_x_y_estimate = DVP.EnergyBasedDecoder.estimated_conditional_log_probability_a_given_b(x, y, num_ones, *temp_params1)\n",
    "        p_y_x_estimate = DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params(y, x, *temp_params2)\n",
    "        mutual_info_x_y = p_x_y_estimate - p_y_x_estimate\n",
    "\n",
    "        return mutual_info_x_y\n",
    "\n",
    "    def test_calculate_mutual_information_loss(self, x, y):\n",
    "        # Ensure x and y are tensors and have the same shape\n",
    "        assert x.shape == y.shape, \"x and y must have the same shape\"\n",
    "\n",
    "        # Compute joint histogram\n",
    "        joint_hist = torch.histc((x * y).float(), bins=256, min=0, max=256)\n",
    "\n",
    "        # Compute marginal histograms\n",
    "        x_hist = torch.histc(x.float(), bins=256, min=0, max=256)\n",
    "        y_hist = torch.histc(y.float(), bins=256, min=0, max=256)\n",
    "\n",
    "        # Normalize histograms to get probabilities\n",
    "        joint_prob = joint_hist / joint_hist.sum()\n",
    "        x_prob = x_hist / x_hist.sum()\n",
    "        y_prob = y_hist / y_hist.sum()\n",
    "\n",
    "        # Compute the entropies\n",
    "        H_x = -torch.sum(x_prob * torch.log(x_prob + 1e-12))\n",
    "        H_y = -torch.sum(y_prob * torch.log(y_prob + 1e-12))\n",
    "        H_xy = -torch.sum(joint_prob * torch.log(joint_prob + 1e-12))\n",
    "\n",
    "        # Compute mutual information\n",
    "        I_xy = H_x + H_y - H_xy\n",
    "\n",
    "        # Return mutual information loss (negative mutual information)\n",
    "        mutual_info_loss = -I_xy\n",
    "\n",
    "        return mutual_info_loss\n",
    "\n",
    "class MutualInformationLossV3(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, *inputs):\n",
    "        num_ones = inputs[0]\n",
    "        zywx_w_tilde_ins = inputs[1:7]\n",
    "        encoder_params = inputs[7:11]\n",
    "        decoder_params = inputs[11:19]\n",
    "        embedding_params = inputs[19:27]\n",
    "\n",
    "        z, y, w, x, _, _ = zywx_w_tilde_ins\n",
    "\n",
    "        #print(x[0], '|' ,y[0])\n",
    "        #print(w[0], '|' ,z[0])\n",
    "\n",
    "        p_x_w_estimate = DVP.EnergyBasedDecoder.estimated_conditional_log_probability_a_given_b(x, w, num_ones, *decoder_params)\n",
    "        p_w_x = DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params(w, x, *encoder_params)\n",
    "        r_w_z_estimate = DVP.EnergyBasedModelEmbeddingDynamics.estimated_normalized_log_probabilities_w_given_z_params(z, w, *embedding_params)\n",
    "\n",
    "        out = p_x_w_estimate - p_w_x + r_w_z_estimate\n",
    "        ctx.num_ones = num_ones\n",
    "        ctx.save_for_backward(*zywx_w_tilde_ins, *encoder_params, *decoder_params, *embedding_params, r_w_z_estimate, out)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        num_ones = ctx.num_ones\n",
    "        z, y, w, x, w_tilde, x_tilde = ctx.saved_tensors[0:6]\n",
    "        encoder_params = ctx.saved_tensors[6:10]\n",
    "        decoder_params = ctx.saved_tensors[10:18]\n",
    "        embedding_params = ctx.saved_tensors[18:26]\n",
    "        r_w_z = ctx.saved_tensors[26]\n",
    "        MI = ctx.saved_tensors[27]\n",
    "\n",
    "        decoder_unnormalized_probs = lambda x, w, *params: DVP.EnergyBasedDecoder.unnormalized_log_probs_a_given_b_params(num_ones, x, w, *params)\n",
    "        decoder_expected_unnormalized_probs = lambda x_tilde, w, *params: DVP.EnergyBasedDecoder.expected_unnormalized_log_probs_a_given_b(num_ones, x_tilde, w, *params)\n",
    "\n",
    "        _, decoder_grad_1 = vjp(decoder_unnormalized_probs, (x, w, *decoder_params), grad_output, create_graph=False)\n",
    "        _, decoder_grad_2 = vjp(decoder_expected_unnormalized_probs, (x_tilde, w.expand(x_tilde.shape[0], -1, -1), *decoder_params), grad_output, create_graph=False)\n",
    "\n",
    "        decoder_grad = tuple(map(lambda x, y: x - y, decoder_grad_1[2:], decoder_grad_2[2:]))\n",
    "\n",
    "        #_, encoder_grad_term_1 = vjp(DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params, (w, x, encoder_params[0], encoder_params[1]), grad_output * (MI - 1), create_graph=False)\n",
    "        _, encoder_grad_term_1 = vjp(DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params, (w, x, *encoder_params), grad_output * (MI - 1), create_graph=False)\n",
    "        encoder_grad_term_1 = encoder_grad_term_1[2:]\n",
    "\n",
    "        #_, encoder_grad_term_2 = vjp(DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params, (z, y, encoder_params[0], encoder_params[1]), grad_output * r_w_z, create_graph=False)\n",
    "        _, encoder_grad_term_2 = vjp(DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params, (z, y, *encoder_params), grad_output * r_w_z, create_graph=False)\n",
    "        encoder_grad_term_2 = encoder_grad_term_2[2:]\n",
    "\n",
    "        encoder_grad = tuple(map(lambda x, y: x + y, encoder_grad_term_1, encoder_grad_term_2))\n",
    "\n",
    "        _, embedding_grad_1 = vjp(DVP.EnergyBasedModelEmbeddingDynamics.unnormalized_log_probs_w_given_z_params, (z, w, *embedding_params), grad_output, create_graph=False)\n",
    "        _, embedding_grad_2 = vjp(DVP.EnergyBasedModelEmbeddingDynamics.expected_unnormalized_log_probs_w_given_z, (z.expand(w_tilde.shape[0], -1, -1), w_tilde, *embedding_params), grad_output, create_graph=False)\n",
    "\n",
    "        embedding_grad = tuple(map(lambda x, y: x - y, embedding_grad_1[2:], embedding_grad_2[2:]))\n",
    "\n",
    "        #print(len(encoder_grad), len(decoder_grad), len(embedding_grad))\n",
    "\n",
    "        return None, None, None, None, None, None, None, *encoder_grad, *decoder_grad, *embedding_grad\n",
    "\n",
    "def run_dim_red_process(device, state_space, embedding_space_size, batch_size=256, num_steps=10000):\n",
    "\n",
    "    model = EmbeddingMI3(batch_size, state_space, embedding_space_size, num_ones=4)\n",
    "\n",
    "    # Path to the state dictionary file\n",
    "    state_dict_path = 'quantum_experiments/initializer.model'\n",
    "    specific_dict_path = f'quantum_experiments/experiment_{state_space}_{embedding_space_size}.model'\n",
    "\n",
    "    # Check if the state dictionary file exists\n",
    "    if os.path.exists(directory_path + specific_dict_path):\n",
    "        # Load the state dictionary\n",
    "        state_dict = torch.load(state_dict_path)\n",
    "        # Get the current state dictionary of the model\n",
    "        old_state_dict = model.state_dict()\n",
    "        # Modify the state dictionary to match the embedding_space_size\n",
    "        old_state_dict['encoder.b'] = state_dict['encoder.b'][:, :embedding_space_size]\n",
    "        old_state_dict['encoder.W'] = state_dict['encoder.W'][:, :embedding_space_size, :]\n",
    "\n",
    "        # Load the adjusted state dictionary into the model\n",
    "        model.load_state_dict(old_state_dict, strict=False)\n",
    "    #else:\n",
    "        #print(f\"State dictionary file '{state_dict_path}' does not exist. Continuing without loading pre-trained weights.\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "    #params = generate_circuit_params(12,12)\n",
    "    params = generate_circuit_params(file_name = directory_path + 'dense_small.param')\n",
    "    dataset = QuantumSimulationDatasetFast(params, batch_size, 4, device, inverse_density=3)\n",
    "\n",
    "    for i, (final_state, initial_state) in enumerate(dataset):\n",
    "        optimizer.zero_grad()\n",
    "        loss, actual_loss = model.test_objective_function(initial_state, final_state)\n",
    "        loss = loss.mean()\n",
    "        actual_loss = actual_loss.mean()\n",
    "        loss.backward()\n",
    "        print('| Iteration', i, 'I(W,Z) > ', f\"{-loss.detach().cpu().item():,.5f}\", ' I(X,Y) > ', f\"{-actual_loss.detach().cpu().item():,.6f}\")\n",
    "        optimizer.step()\n",
    "        if i > num_steps:\n",
    "            print('Training Terminated')\n",
    "            break\n",
    "        if i % 1000 == 999:\n",
    "            torch.save(model.state_dict(), f'quantum_experiments/experiment_{state_space}_{embedding_space_size}_{i}.model')\n",
    "\n",
    "    # Save the model at the end of training\n",
    "    final_save_path = f'quantum_experiments/experiment_{state_space}_{embedding_space_size}_v1.model'\n",
    "    torch.save(model.state_dict(), final_save_path)\n",
    "    print(f\"Final model saved to {final_save_path}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('Device Running: ', device)\n",
    "\n",
    "    # experiments = [\n",
    "    #     (12, 2, 1024), (12, 3, 1024), (12, 4, 1024), (12, 5, 1024),\n",
    "    #     (12, 6, 1024), (12, 7, 1024), (12, 8, 1024), (12, 9, 1024),\n",
    "    #     (12, 10, 1024), (12, 11, 1024), (12, 12, 1024)\n",
    "    # ]\n",
    "\n",
    "    experiments = [\n",
    "        (12, 12, 512)\n",
    "    ]\n",
    "\n",
    "    for i, params in enumerate(experiments):\n",
    "        print('Running Experiment', i, params)\n",
    "        print('State Space of', params[0], 'to Embedding Space of', params[1])\n",
    "        run_dim_red_process(device, *params)\n"
   ],
   "metadata": {
    "id": "u6Rq3bU6sHLw",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "outputId": "56cf9751-34c6-47ac-c8b8-86c58473b086"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'torch.nn' has no attribute 'DModule'",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-436c8046f2a7>\u001B[0m in \u001B[0;36m<cell line: 6>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnn\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mnn\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctional\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mDiscreteVariationalParameterizationsDeepV3\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mDVP\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctional\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mvjp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunction\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mFunction\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/content/drive/MyDrive/Quantum/DiscreteVariationalParameterizationsDeepV3.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0mrng\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrandom\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdefault_rng\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 8\u001B[0;31m \u001B[0;32mclass\u001B[0m \u001B[0mEnergyBasedModelEmbeddingDynamics\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDModule\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      9\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhidden_dim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'torch.nn' has no attribute 'DModule'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "VYtCQlWbXH7o"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
