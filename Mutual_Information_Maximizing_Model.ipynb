{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# File/Environment Preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B70wgiwMLH8R",
    "outputId": "7a21aa53-f2bc-470a-aa1d-ae8f7b734e03",
    "ExecuteTime": {
     "end_time": "2024-07-17T19:09:27.484091Z",
     "start_time": "2024-07-17T19:09:27.479205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Symmetric_Exclusion_Process _Simulator.ipynb', 'Quantum_Transformer.ipynb', 'dense_small.param', '.DS_Store', 'quantum_experiments', 'DiscreteVariationalParameterizationsDeepV2.py', 'Mutual_Information_Transformer.ipynb', 'DiscreteVariationalParameterizationsDeepV3.py', 'README.md', 'Mutual_Information_Maximizing_Model.ipynb', 'temp.txt', '.ipynb_checkpoints', '.git', 'Quantum_Brickworks_Circuit_Generator.ipynb', 'QuantumSimulatorDataset.py', 'GibbsSampling.py', '.idea']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir())\n",
    "\n",
    "# uncomment if mounting google drive\n",
    "directory_path = ''\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# import sys\n",
    "# directory_path = '/content/drive/MyDrive/Quantum/'\n",
    "# sys.path.append('/content/drive/MyDrive/Quantum')\n",
    "#print(os.listdir(directory_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u3NkfGVa1e-e",
    "outputId": "275fb468-b45a-4ab3-fa7e-ad7a08f19637",
    "ExecuteTime": {
     "end_time": "2024-07-17T21:05:43.541956Z",
     "start_time": "2024-07-17T21:05:34.765730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.11/site-packages (2.3.1)\r\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.11/site-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1.3)\r\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.11/site-packages (from torch) (2023.10.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\r\n",
      "Requirement already satisfied: qiskit-aer in /opt/anaconda3/lib/python3.11/site-packages (0.14.2)\r\n",
      "Requirement already satisfied: qiskit>=0.45.2 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit-aer) (1.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.16.3 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit-aer) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit-aer) (1.11.4)\r\n",
      "Requirement already satisfied: psutil>=5 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit-aer) (5.9.0)\r\n",
      "Requirement already satisfied: rustworkx>=0.14.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (0.14.2)\r\n",
      "Requirement already satisfied: sympy>=1.3 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (1.12)\r\n",
      "Requirement already satisfied: dill>=0.3 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (0.3.7)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (2.8.2)\r\n",
      "Requirement already satisfied: stevedore>=3.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (5.2.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (4.9.0)\r\n",
      "Requirement already satisfied: symengine>=0.11 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit>=0.45.2->qiskit-aer) (0.11.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.0->qiskit>=0.45.2->qiskit-aer) (1.16.0)\r\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from stevedore>=3.0.0->qiskit>=0.45.2->qiskit-aer) (6.0.0)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy>=1.3->qiskit>=0.45.2->qiskit-aer) (1.3.0)\r\n",
      "Requirement already satisfied: qiskit in /opt/anaconda3/lib/python3.11/site-packages (1.1.0)\r\n",
      "Requirement already satisfied: rustworkx>=0.14.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (0.14.2)\r\n",
      "Requirement already satisfied: numpy<3,>=1.17 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (1.11.4)\r\n",
      "Requirement already satisfied: sympy>=1.3 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (1.12)\r\n",
      "Requirement already satisfied: dill>=0.3 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (0.3.7)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (2.8.2)\r\n",
      "Requirement already satisfied: stevedore>=3.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (5.2.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (4.9.0)\r\n",
      "Requirement already satisfied: symengine>=0.11 in /opt/anaconda3/lib/python3.11/site-packages (from qiskit) (0.11.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.0->qiskit) (1.16.0)\r\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from stevedore>=3.0.0->qiskit) (6.0.0)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy>=1.3->qiskit) (1.3.0)\r\n",
      "Requirement already satisfied: pylatexenc in /opt/anaconda3/lib/python3.11/site-packages (2.10)\r\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (4.65.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install qiskit-aer\n",
    "!pip install qiskit\n",
    "!pip install pylatexenc\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import DiscreteVariationalParameterizationsDeepV3 as DVP\n",
    "from torch.autograd.functional import vjp\n",
    "from torch.autograd.function import Function\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from QuantumSimulatorDataset import QuantumSimulationDatasetFast, generate_circuit_params\n",
    "from GibbsSampling import BatchedConditionalGibbsSampler, BatchedConditionalDoubleGibbsSampler"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T21:06:55.910948Z",
     "start_time": "2024-07-17T21:06:55.907287Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MI Model Definition"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "id": "u6Rq3bU6sHLw",
    "outputId": "56cf9751-34c6-47ac-c8b8-86c58473b086",
    "ExecuteTime": {
     "end_time": "2024-07-17T21:20:20.851893Z",
     "start_time": "2024-07-17T21:20:20.838492Z"
    }
   },
   "outputs": [],
   "source": [
    "class EmbeddingMI3(nn.Module):\n",
    "    def __init__(self, batch_size, in_dim, out_dim, num_ones):\n",
    "        super().__init__()\n",
    "        self.encoder = DVP.BoltzmannBasedEncoder(in_dim=in_dim, out_dim=out_dim)\n",
    "        self.decoder = DVP.EnergyBasedDecoder(in_dim=out_dim, out_dim=in_dim, num_ones=num_ones)\n",
    "        self.num_ones = num_ones\n",
    "        self.embedding_dynamics = DVP.EnergyBasedModelEmbeddingDynamics(dim=out_dim)\n",
    "        self.loss_func = MutualInformationLossV3.apply\n",
    "        self.embedding_sampler = BatchedConditionalGibbsSampler(batch_size=batch_size, num_samples=256, # needs to be tuned\n",
    "                                                                mixing_time=5, # seems like this can be low and still work\n",
    "                                                                joint_distribution=self.embedding_dynamics)\n",
    "        self.decoder_sampler = BatchedConditionalDoubleGibbsSampler(batch_size=batch_size, num_samples=256, # needs to be tuned\n",
    "                                                                mixing_time=24, # seems like this can be low and still work\n",
    "                                                                joint_distribution=self.decoder, dim=in_dim, num_ones=self.num_ones)\n",
    "    def test_objective_function(self, x, y):\n",
    "        w = self.encoder.encoder_sample(x).detach()\n",
    "        z = self.encoder.encoder_sample(y).detach()\n",
    "        # print(w, z, x, y)\n",
    "\n",
    "        w_tilde = self.embedding_sampler.run_batched_gibbs(z).detach()\n",
    "        x_tilde = self.decoder_sampler.run_batched_gibbs(w).detach()\n",
    "\n",
    "        return -self.loss_func(self.num_ones, *(z, y, w, x, w_tilde, x_tilde), *self.encoder.params(), *self.decoder.params(), *self.embedding_dynamics.params()), \\\n",
    "                mutual_info_score(w.view(-1), z.view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MI Loss and Entropy Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [],
   "source": [
    "class MutualInformationLossV3(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, *inputs):\n",
    "        num_ones = inputs[0]\n",
    "        zywx_w_tilde_ins = inputs[1:7]\n",
    "        encoder_params = inputs[7:11]\n",
    "        decoder_params = inputs[11:19]\n",
    "        embedding_params = inputs[19:27]\n",
    "\n",
    "        z, y, w, x, _, _ = zywx_w_tilde_ins\n",
    "\n",
    "        print(encoder_params, decoder_params, embedding_params)\n",
    "        #print(x[0], '|' ,y[0])\n",
    "        #print(w[0], '|' ,z[0])\n",
    "\n",
    "        p_x_w_estimate = DVP.EnergyBasedDecoder.estimated_conditional_log_probability_a_given_b(x, w, num_ones, *decoder_params)\n",
    "        p_w_x = DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params(w, x, *encoder_params)\n",
    "        r_w_z_estimate = DVP.EnergyBasedModelEmbeddingDynamics.estimated_normalized_log_probabilities_w_given_z_params(z, w, *embedding_params)\n",
    "\n",
    "        out = p_x_w_estimate - p_w_x + r_w_z_estimate\n",
    "        ctx.num_ones = num_ones\n",
    "        ctx.save_for_backward(*zywx_w_tilde_ins, *encoder_params, *decoder_params, *embedding_params, r_w_z_estimate, out)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        num_ones = ctx.num_ones\n",
    "        z, y, w, x, w_tilde, x_tilde = ctx.saved_tensors[0:6]\n",
    "        encoder_params = ctx.saved_tensors[6:10]\n",
    "        decoder_params = ctx.saved_tensors[10:18]\n",
    "        embedding_params = ctx.saved_tensors[18:26]\n",
    "        r_w_z = ctx.saved_tensors[26]\n",
    "        MI = ctx.saved_tensors[27]\n",
    "\n",
    "        # decoder gradient handling\n",
    "        decoder_unnormalized_probs = lambda x, w, *params: DVP.EnergyBasedDecoder.unnormalized_log_probs_a_given_b_params(num_ones, x, w, *params)\n",
    "        decoder_expected_unnormalized_probs = lambda x_tilde, w, *params: DVP.EnergyBasedDecoder.expected_unnormalized_log_probs_a_given_b(num_ones, x_tilde, w, *params)\n",
    "\n",
    "        _, decoder_grad_1 = vjp(decoder_unnormalized_probs, (x, w, *decoder_params), grad_output, create_graph=False)\n",
    "        _, decoder_grad_2 = vjp(decoder_expected_unnormalized_probs, (x_tilde, w.expand(x_tilde.shape[0], -1, -1), *decoder_params), grad_output, create_graph=False)\n",
    "\n",
    "        decoder_grad = tuple(map(lambda x, y: x - y, decoder_grad_1[2:], decoder_grad_2[2:]))\n",
    "\n",
    "        # encoder gradient handling\n",
    "        _, encoder_grad_term_1 = vjp(DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params, (w, x, *encoder_params), grad_output * (MI - 1), create_graph=False)\n",
    "        encoder_grad_term_1 = encoder_grad_term_1[2:]\n",
    "\n",
    "        _, encoder_grad_term_2 = vjp(DVP.BoltzmannBasedEncoder.conditional_log_probability_a_given_b_params, (z, y, *encoder_params), grad_output * r_w_z, create_graph=False)\n",
    "        encoder_grad_term_2 = encoder_grad_term_2[2:]\n",
    "\n",
    "        encoder_grad = tuple(map(lambda x, y: x + y, encoder_grad_term_1, encoder_grad_term_2))\n",
    "\n",
    "        # embedding gradient handling\n",
    "        _, embedding_grad_1 = vjp(DVP.EnergyBasedModelEmbeddingDynamics.unnormalized_log_probs_w_given_z_params, (z, w, *embedding_params), grad_output, create_graph=False)\n",
    "        _, embedding_grad_2 = vjp(DVP.EnergyBasedModelEmbeddingDynamics.expected_unnormalized_log_probs_w_given_z, (z.expand(w_tilde.shape[0], -1, -1), w_tilde, *embedding_params), grad_output, create_graph=False)\n",
    "\n",
    "        embedding_grad = tuple(map(lambda x, y: x - y, embedding_grad_1[2:], embedding_grad_2[2:]))\n",
    "\n",
    "        return None, None, None, None, None, None, None, *encoder_grad, *decoder_grad, *embedding_grad"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T22:06:56.103891Z",
     "start_time": "2024-07-17T22:06:56.099293Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [],
   "source": [
    "def entropy(X):\n",
    "    # Flatten the tensor to 1D\n",
    "    X_flat = X.view(-1)\n",
    "    \n",
    "    # Count the occurrences of each unique value\n",
    "    unique_vals, counts = X_flat.unique(return_counts=True)\n",
    "    probabilities = counts.float() / counts.sum()\n",
    "    entropy = -torch.sum(probabilities * torch.log(probabilities))\n",
    "        \n",
    "    return entropy.item()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T22:06:56.784412Z",
     "start_time": "2024-07-17T22:06:56.778241Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dimension Reduction Process"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [],
   "source": [
    "def run_dim_red_process(device, state_space, embedding_space_size, batch_size=256, num_steps=10000):\n",
    "\n",
    "    model = EmbeddingMI3(batch_size, state_space, embedding_space_size, num_ones=4).to(device)\n",
    "\n",
    "    # Path to the state dictionary file\n",
    "    state_dict_path = 'quantum_experiments/initializer.model'\n",
    "    specific_dict_path = f'quantum_experiments/experiment_{state_space}_{embedding_space_size}_good.model'\n",
    "\n",
    "    # Check if the state dictionary file exists\n",
    "    if os.path.exists(directory_path + specific_dict_path):\n",
    "        state_dict = torch.load(specific_dict_path, map_location=device)\n",
    "        old_state_dict = model.state_dict()\n",
    "        old_state_dict['encoder.b'] = state_dict['encoder.b'][:, :embedding_space_size]\n",
    "        old_state_dict['encoder.W'] = state_dict['encoder.W'][:, :embedding_space_size, :]\n",
    "    \n",
    "        model.load_state_dict(old_state_dict, strict=False)\n",
    "        print(\"| Successfully loaded initializer model\", specific_dict_path)\n",
    "    #else:\n",
    "        #print(f\"State dictionary file '{state_dict_path}' does not exist. Continuing without loading pre-trained weights.\")\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    # Uncomment below to test a Circuit Length of 0 (no time evolution)\n",
    "    params = generate_circuit_params(0,12)\n",
    "    #params = generate_circuit_params(file_name = directory_path + 'dense_small.param')\n",
    "    dataset = QuantumSimulationDatasetFast(params, batch_size, 4, device, inverse_density=3)\n",
    "\n",
    "    for i, (final_state, initial_state) in enumerate(dataset):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss, loss2 = model.test_objective_function(initial_state, final_state)\n",
    "        loss = loss.mean()\n",
    "        loss.backward()\n",
    "        \n",
    "        expected_mutual_info = mutual_info_score(initial_state.view(-1), final_state.view(-1))\n",
    "        initial_entropy = entropy(initial_state)\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Adjust max_norm as needed\n",
    "        \n",
    "        # if i % 10 == 0:\n",
    "        print('| Iteration', i, ' > I(W,Z):', f\"{-loss.detach().cpu().item():,.5f}\", \n",
    "              ' I(W,Z):', f\"{loss2:,.5f}\",\n",
    "              ' I(X,Y):', f\"{expected_mutual_info:,.5f}\",\n",
    "              ' H(X):', f\"{initial_entropy:,.5f}\")\n",
    "        \n",
    "        optimizer.step()\n",
    "        if i > num_steps:\n",
    "            print('Training Terminated')\n",
    "            break\n",
    "        if i % 1000 == 999:\n",
    "            torch.save(model.state_dict(), f'quantum_experiments/experiment_{state_space}_{embedding_space_size}_{i}.model')\n",
    "\n",
    "    # Save the model at the end of training\n",
    "    final_save_path = f'quantum_experiments/experiment_{state_space}_{embedding_space_size}_final.model'\n",
    "    torch.save(model.state_dict(), final_save_path)\n",
    "    print(f\"Final model saved to {final_save_path}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T22:06:59.612148Z",
     "start_time": "2024-07-17T22:06:59.609810Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hyperparameter Tuning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T22:07:02.534137Z",
     "start_time": "2024-07-17T22:07:02.530772Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_learning_rate(device, state_space, embedding_space_size, batch_size=256, num_steps=300):\n",
    "    model = EmbeddingMI3(batch_size, state_space, embedding_space_size, num_ones=4).to(device)\n",
    "    \n",
    "    # Load parameters (adapt as needed)\n",
    "    params = generate_circuit_params(file_name=directory_path + 'dense_small.param')\n",
    "    dataset = QuantumSimulationDatasetFast(params, batch_size, 4, device, inverse_density=3)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-7)\n",
    "    lrs = []\n",
    "    losses = []\n",
    "    lr = 1e-7\n",
    "\n",
    "    for i, (final_state, initial_state) in enumerate(dataset):\n",
    "        if i > num_steps:\n",
    "            break\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = model.test_objective_function(initial_state, final_state).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        lrs.append(lr)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Increase learning rate\n",
    "        lr *= 1.05\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f'| Iteration {i}, Learning Rate: {lr:.8f}, Loss: {loss.item():.8f}')\n",
    "    \n",
    "    # Plot results\n",
    "    plt.plot(lrs, losses)\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Learning Rate')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Learning Rate Finder')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T22:08:07.850438Z",
     "start_time": "2024-07-17T22:07:03.419859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Running:  cpu\n",
      "Running Experiment 0 (12, 12, 1024)\n",
      "State Space of 12 to Embedding Space of 12\n",
      "(Parameter containing:\n",
      "tensor([[ 0.1279,  0.1062, -0.1326, -0.6593, -0.0706,  0.4276,  0.4161,  0.2417,\n",
      "          0.2607,  0.5498, -0.0252,  0.3133]], requires_grad=True), Parameter containing:\n",
      "tensor([[[-0.0373,  0.0663,  0.0657,  0.0833,  0.0306,  0.0435,  0.0764,\n",
      "          -0.0398,  0.0902,  0.0223, -0.1645,  0.1574],\n",
      "         [ 0.1946,  0.1469, -0.0794, -0.0907,  0.0555, -0.1393,  0.0500,\n",
      "           0.1475,  0.0031,  0.0126, -0.0116, -0.1408],\n",
      "         [ 0.1231,  0.1174, -0.0144, -0.1857, -0.0639,  0.1828,  0.1595,\n",
      "          -0.0423,  0.1322,  0.0791, -0.0023,  0.1949],\n",
      "         [-0.1502, -0.0329, -0.1415,  0.0494, -0.0933,  0.0181,  0.1100,\n",
      "          -0.0430, -0.0688, -0.1138,  0.0610,  0.0402],\n",
      "         [-0.1170,  0.0720, -0.1014, -0.1869,  0.0835,  0.1954, -0.1362,\n",
      "           0.1644, -0.0825,  0.0919, -0.0766,  0.1027],\n",
      "         [-0.1635,  0.1298, -0.1845,  0.0018, -0.1289,  0.0994,  0.0485,\n",
      "          -0.0151, -0.0803, -0.0772,  0.1784,  0.0742],\n",
      "         [-0.1765,  0.0282, -0.1346, -0.0605, -0.1245,  0.1552,  0.1297,\n",
      "          -0.1638, -0.1156, -0.1366,  0.1918, -0.0613],\n",
      "         [ 0.1461, -0.1950,  0.1588, -0.1431, -0.0665, -0.0172, -0.1024,\n",
      "           0.1269,  0.0933, -0.1102, -0.1574, -0.1940],\n",
      "         [ 0.0319, -0.1642,  0.1481,  0.0810, -0.0149, -0.1233,  0.1357,\n",
      "          -0.1381, -0.0866,  0.1899,  0.0146,  0.0328],\n",
      "         [-0.1812,  0.1646, -0.1516, -0.0748, -0.0093,  0.0678,  0.1623,\n",
      "           0.1126, -0.1555, -0.1020, -0.1291,  0.1915],\n",
      "         [ 0.1369,  0.0807, -0.1270, -0.1011, -0.0716, -0.0830, -0.0031,\n",
      "          -0.1319, -0.0023,  0.1630,  0.0971, -0.0331],\n",
      "         [ 0.1077, -0.1700, -0.1763, -0.1001,  0.1353,  0.0123,  0.1857,\n",
      "           0.0345, -0.1206,  0.0012,  0.0388, -0.0587]]], requires_grad=True), Parameter containing:\n",
      "tensor([0.], requires_grad=True), Parameter containing:\n",
      "tensor([0.], requires_grad=True)) (Parameter containing:\n",
      "tensor([[-0.1251, -0.0342,  0.0362,  ...,  0.1620, -0.1639,  0.1449],\n",
      "        [-0.1767, -0.0215,  0.0280,  ...,  0.0845,  0.0670,  0.0447],\n",
      "        [ 0.1780,  0.0763,  0.0206,  ...,  0.1180, -0.1362, -0.1257],\n",
      "        ...,\n",
      "        [ 0.1172, -0.0767,  0.1115,  ..., -0.0552,  0.1618,  0.0655],\n",
      "        [-0.1196, -0.1518,  0.0869,  ...,  0.0622, -0.1452, -0.1779],\n",
      "        [-0.0609,  0.0253, -0.0534,  ..., -0.1151,  0.0994, -0.1784]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0326,  0.1924,  0.2532,  ..., -0.0619,  0.1325,  0.2127],\n",
      "        [-0.0228, -0.1350,  0.0449,  ...,  0.0489, -0.2545, -0.3053],\n",
      "        [ 0.2496,  0.3057,  0.0935,  ...,  0.0257, -0.0518, -0.0566],\n",
      "        ...,\n",
      "        [ 0.2473,  0.2395, -0.2433,  ...,  0.0314,  0.0747, -0.1924],\n",
      "        [-0.1104,  0.0282, -0.2175,  ..., -0.2842, -0.0484, -0.1553],\n",
      "        [-0.0494, -0.0168, -0.2875,  ..., -0.1983, -0.2881, -0.1919]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0954, -0.2560, -0.0497,  ..., -0.0057,  0.1606, -0.1460],\n",
      "        [-0.2173,  0.2291,  0.2329,  ...,  0.1274,  0.2720,  0.2252],\n",
      "        [ 0.2240, -0.2973,  0.1853,  ..., -0.0072,  0.1869,  0.2461],\n",
      "        ...,\n",
      "        [ 0.2584,  0.1709, -0.1258,  ..., -0.1022, -0.0982,  0.1431],\n",
      "        [ 0.0747, -0.2313,  0.0239,  ..., -0.0745, -0.0732,  0.0152],\n",
      "        [ 0.1026, -0.1655, -0.2345,  ...,  0.2692, -0.0613,  0.1580]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.2335,  0.4240, -0.3321,  0.4139, -0.2412, -0.1360,  0.2895,  0.2805,\n",
      "          0.2468, -0.1328,  0.1666, -0.1724, -0.2912,  0.0190,  0.1302,  0.0126,\n",
      "         -0.3778,  0.0228, -0.1732,  0.0879,  0.0291, -0.1326, -0.2120, -0.0562,\n",
      "          0.4235,  0.3843,  0.3112, -0.2248,  0.3042, -0.0893, -0.1709,  0.3549]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0.], requires_grad=True)) (Parameter containing:\n",
      "tensor([[-0.0981, -0.1481, -0.0876,  ..., -0.0526, -0.1680, -0.0116],\n",
      "        [ 0.0780,  0.1289, -0.1031,  ...,  0.0529,  0.1147,  0.1069],\n",
      "        [-0.1699, -0.1829,  0.1622,  ..., -0.1691,  0.1009, -0.1835],\n",
      "        ...,\n",
      "        [-0.0888, -0.0083,  0.0285,  ...,  0.0908,  0.1710, -0.1517],\n",
      "        [-0.0094,  0.1551,  0.0527,  ..., -0.1348, -0.0968, -0.0688],\n",
      "        [ 0.0471, -0.0752, -0.0520,  ...,  0.0536,  0.0732, -0.1227]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.1544,  0.0506, -0.0747,  ...,  0.0535, -0.1023, -0.2166],\n",
      "        [-0.0059,  0.0388,  0.2728,  ..., -0.2285,  0.0481,  0.0789],\n",
      "        [ 0.2301,  0.0216,  0.0687,  ...,  0.1992,  0.0889, -0.2808],\n",
      "        ...,\n",
      "        [ 0.0072, -0.2313, -0.0044,  ...,  0.1495,  0.2729, -0.3022],\n",
      "        [ 0.0745, -0.3006, -0.0972,  ..., -0.1295,  0.1436, -0.2478],\n",
      "        [ 0.1570, -0.2519, -0.1733,  ...,  0.0756,  0.1931,  0.1982]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.2294, -0.2818, -0.0575,  ...,  0.2448,  0.2783, -0.2962],\n",
      "        [-0.1214,  0.2785, -0.1908,  ...,  0.0433, -0.1400,  0.1603],\n",
      "        [ 0.0783, -0.2839,  0.2132,  ..., -0.0851,  0.2707, -0.1931],\n",
      "        ...,\n",
      "        [ 0.0273,  0.1229,  0.1679,  ..., -0.0167,  0.0220, -0.0909],\n",
      "        [-0.0549,  0.2742, -0.1218,  ...,  0.0689,  0.2474, -0.0560],\n",
      "        [ 0.2288, -0.1938,  0.2188,  ...,  0.0413, -0.2533,  0.0710]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.2327, -0.1367,  0.4260,  0.3315, -0.3838,  0.0748, -0.3656, -0.2980,\n",
      "          0.0006,  0.3267, -0.2823, -0.3597,  0.1312,  0.2549, -0.1392, -0.0918,\n",
      "          0.4135, -0.1375, -0.0699, -0.2366, -0.1751,  0.1211,  0.3008, -0.2227,\n",
      "          0.0627, -0.3101,  0.3902,  0.2492, -0.0679,  0.1300, -0.2804, -0.2326]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0.], requires_grad=True))\n",
      "| Iteration 0  > I(W,Z): -6.44950  I(W,Z): 0.00135  I(X,Y): 0.63651  H(X): 0.63651\n",
      "(Parameter containing:\n",
      "tensor([[ 0.1289,  0.1072, -0.1316, -0.6583, -0.0716,  0.4266,  0.4151,  0.2427,\n",
      "          0.2617,  0.5488, -0.0242,  0.3123]], requires_grad=True), Parameter containing:\n",
      "tensor([[[-3.8287e-02,  6.7280e-02,  6.4695e-02,  8.4335e-02,  3.1561e-02,\n",
      "           4.2451e-02,  7.5375e-02, -3.8839e-02,  8.9236e-02,  2.3294e-02,\n",
      "          -1.6551e-01,  1.5835e-01],\n",
      "         [ 1.9363e-01,  1.4590e-01, -8.0352e-02, -8.9721e-02,  5.4461e-02,\n",
      "          -1.3831e-01,  5.1046e-02,  1.4646e-01,  4.1099e-03,  1.1600e-02,\n",
      "          -1.0621e-02, -1.4178e-01],\n",
      "         [ 1.2207e-01,  1.1845e-01, -1.5421e-02, -1.8470e-01, -6.2869e-02,\n",
      "           1.8380e-01,  1.6048e-01, -4.1321e-02,  1.3117e-01,  8.0122e-02,\n",
      "          -1.2982e-03,  1.9392e-01],\n",
      "         [-1.4919e-01, -3.1888e-02, -1.4049e-01,  5.0365e-02, -9.4313e-02,\n",
      "           1.7067e-02,  1.1098e-01, -4.4031e-02, -6.7817e-02, -1.1283e-01,\n",
      "           6.1998e-02,  4.1187e-02],\n",
      "         [-1.1795e-01,  7.3030e-02, -1.0043e-01, -1.8788e-01,  8.4498e-02,\n",
      "           1.9445e-01, -1.3719e-01,  1.6336e-01, -8.3458e-02,  9.0869e-02,\n",
      "          -7.7639e-02,  1.0175e-01],\n",
      "         [-1.6447e-01,  1.2883e-01, -1.8553e-01,  7.9926e-04, -1.2989e-01,\n",
      "           9.8357e-02,  4.9489e-02, -1.6066e-02, -7.9292e-02, -7.6230e-02,\n",
      "           1.7942e-01,  7.3236e-02],\n",
      "         [-1.7547e-01,  2.9222e-02, -1.3564e-01, -6.1489e-02, -1.2352e-01,\n",
      "           1.5422e-01,  1.2867e-01, -1.6484e-01, -1.1655e-01, -1.3761e-01,\n",
      "           1.9277e-01, -6.0307e-02],\n",
      "         [ 1.4710e-01, -1.9602e-01,  1.5779e-01, -1.4409e-01, -6.5459e-02,\n",
      "          -1.6243e-02, -1.0140e-01,  1.2590e-01,  9.4315e-02, -1.0921e-01,\n",
      "          -1.5643e-01, -1.9304e-01],\n",
      "         [ 3.2881e-02, -1.6324e-01,  1.4913e-01,  8.2008e-02, -1.5882e-02,\n",
      "          -1.2434e-01,  1.3674e-01, -1.3711e-01, -8.5634e-02,  1.8891e-01,\n",
      "           1.5635e-02,  3.1797e-02],\n",
      "         [-1.8221e-01,  1.6363e-01, -1.5265e-01, -7.5838e-02, -1.0293e-02,\n",
      "           6.6802e-02,  1.6131e-01,  1.1164e-01, -1.5646e-01, -1.0296e-01,\n",
      "          -1.3013e-01,  1.9050e-01],\n",
      "         [ 1.3786e-01,  8.1686e-02, -1.2600e-01, -1.0011e-01, -7.0613e-02,\n",
      "          -8.4036e-02, -2.0622e-03, -1.3093e-01, -1.3320e-03,  1.6396e-01,\n",
      "           9.6136e-02, -3.2077e-02],\n",
      "         [ 1.0673e-01, -1.7104e-01, -1.7730e-01, -9.9112e-02,  1.3626e-01,\n",
      "           1.1284e-02,  1.8669e-01,  3.5518e-02, -1.2158e-01,  1.5206e-04,\n",
      "           3.9751e-02, -5.9663e-02]]], requires_grad=True), Parameter containing:\n",
      "tensor([0.], requires_grad=True), Parameter containing:\n",
      "tensor([0.], requires_grad=True)) (Parameter containing:\n",
      "tensor([[-0.1261, -0.0352,  0.0352,  ...,  0.1610, -0.1649,  0.1439],\n",
      "        [-0.1777, -0.0225,  0.0270,  ...,  0.0835,  0.0660,  0.0437],\n",
      "        [ 0.1770,  0.0773,  0.0196,  ...,  0.1170, -0.1372, -0.1267],\n",
      "        ...,\n",
      "        [ 0.1182, -0.0777,  0.1125,  ..., -0.0542,  0.1628,  0.0665],\n",
      "        [-0.1206, -0.1528,  0.0859,  ...,  0.0612, -0.1442, -0.1769],\n",
      "        [-0.0599,  0.0263, -0.0524,  ..., -0.1161,  0.0984, -0.1794]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0010, -0.0010, -0.0010,  0.0010, -0.0010, -0.0010, -0.0010,  0.0010,\n",
      "        -0.0010, -0.0010, -0.0010, -0.0010,  0.0010,  0.0010,  0.0010, -0.0010,\n",
      "        -0.0010, -0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0010,\n",
      "        -0.0010, -0.0010,  0.0010,  0.0010, -0.0010,  0.0010,  0.0010, -0.0010],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0316,  0.1914,  0.2542,  ..., -0.0609,  0.1315,  0.2117],\n",
      "        [-0.0238, -0.1360,  0.0439,  ...,  0.0479, -0.2555, -0.3063],\n",
      "        [ 0.2506,  0.3047,  0.0945,  ...,  0.0267, -0.0508, -0.0576],\n",
      "        ...,\n",
      "        [ 0.2463,  0.2405, -0.2443,  ...,  0.0304,  0.0737, -0.1914],\n",
      "        [-0.1094,  0.0292, -0.2165,  ..., -0.2832, -0.0474, -0.1543],\n",
      "        [-0.0504, -0.0158, -0.2865,  ..., -0.1993, -0.2891, -0.1909]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0010, -0.0010,  0.0010,  0.0010, -0.0010, -0.0010, -0.0010,  0.0010,\n",
      "         0.0010,  0.0010, -0.0010, -0.0010, -0.0010, -0.0010,  0.0010,  0.0010,\n",
      "         0.0010,  0.0010, -0.0010,  0.0010,  0.0010, -0.0010, -0.0010, -0.0010,\n",
      "        -0.0010, -0.0010, -0.0010, -0.0010,  0.0010, -0.0010,  0.0010,  0.0010],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0944, -0.2550, -0.0487,  ..., -0.0047,  0.1616, -0.1450],\n",
      "        [-0.2163,  0.2301,  0.2319,  ...,  0.1264,  0.2730,  0.2242],\n",
      "        [ 0.2250, -0.2963,  0.1863,  ..., -0.0062,  0.1879,  0.2471],\n",
      "        ...,\n",
      "        [ 0.2574,  0.1699, -0.1248,  ..., -0.1012, -0.0992,  0.1421],\n",
      "        [ 0.0737, -0.2323,  0.0249,  ..., -0.0735, -0.0742,  0.0142],\n",
      "        [ 0.1036, -0.1645, -0.2355,  ...,  0.2682, -0.0603,  0.1570]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0010, -0.0010,  0.0010,  0.0010,  0.0010, -0.0010, -0.0010,  0.0010,\n",
      "         0.0010, -0.0010,  0.0010, -0.0010, -0.0010, -0.0010,  0.0010, -0.0010,\n",
      "         0.0010,  0.0010,  0.0010, -0.0010,  0.0010,  0.0010, -0.0010, -0.0010,\n",
      "        -0.0010, -0.0010, -0.0010, -0.0010,  0.0010, -0.0010, -0.0010, -0.0010],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-0.2345,  0.4230, -0.3331,  0.4149, -0.2422, -0.1350,  0.2885,  0.2795,\n",
      "          0.2458, -0.1318,  0.1676, -0.1714, -0.2902,  0.0180,  0.1312,  0.0116,\n",
      "         -0.3768,  0.0238, -0.1742,  0.0869,  0.0301, -0.1336, -0.2110, -0.0552,\n",
      "          0.4225,  0.3833,  0.3102, -0.2238,  0.3032, -0.0883, -0.1699,  0.3539]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0.], requires_grad=True)) (Parameter containing:\n",
      "tensor([[-0.0971, -0.1491, -0.0866,  ..., -0.0516, -0.1690, -0.0106],\n",
      "        [ 0.0770,  0.1279, -0.1041,  ...,  0.0519,  0.1137,  0.1059],\n",
      "        [-0.1689, -0.1819,  0.1632,  ..., -0.1681,  0.0999, -0.1825],\n",
      "        ...,\n",
      "        [-0.0898, -0.0093,  0.0275,  ...,  0.0898,  0.1700, -0.1527],\n",
      "        [-0.0084,  0.1541,  0.0537,  ..., -0.1338, -0.0978, -0.0678],\n",
      "        [ 0.0461, -0.0762, -0.0530,  ...,  0.0526,  0.0742, -0.1237]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0010, -0.0010,  0.0010, -0.0010, -0.0010, -0.0010, -0.0010,  0.0010,\n",
      "         0.0010, -0.0010,  0.0010,  0.0010,  0.0010, -0.0010,  0.0010,  0.0010,\n",
      "        -0.0010,  0.0010, -0.0010, -0.0010,  0.0010,  0.0010, -0.0010,  0.0010,\n",
      "         0.0010,  0.0010,  0.0010,  0.0010, -0.0010, -0.0010, -0.0010,  0.0010],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.1554,  0.0516, -0.0757,  ...,  0.0545, -0.1033, -0.2156],\n",
      "        [-0.0069,  0.0398,  0.2718,  ..., -0.2275,  0.0471,  0.0799],\n",
      "        [ 0.2291,  0.0226,  0.0677,  ...,  0.2002,  0.0879, -0.2818],\n",
      "        ...,\n",
      "        [ 0.0082, -0.2303, -0.0034,  ...,  0.1505,  0.2719, -0.3012],\n",
      "        [ 0.0755, -0.2996, -0.0962,  ..., -0.1305,  0.1446, -0.2488],\n",
      "        [ 0.1560, -0.2509, -0.1743,  ...,  0.0746,  0.1921,  0.1992]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0010, -0.0010, -0.0010, -0.0010, -0.0010,  0.0010,  0.0010,  0.0010,\n",
      "        -0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0010, -0.0010,  0.0010,\n",
      "        -0.0010, -0.0010, -0.0010,  0.0010,  0.0010, -0.0010,  0.0010, -0.0010,\n",
      "        -0.0010,  0.0010, -0.0010, -0.0010, -0.0010,  0.0010,  0.0010, -0.0010],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.2304, -0.2808, -0.0565,  ...,  0.2458,  0.2793, -0.2952],\n",
      "        [-0.1204,  0.2775, -0.1918,  ...,  0.0443, -0.1410,  0.1593],\n",
      "        [ 0.0793, -0.2849,  0.2122,  ..., -0.0861,  0.2717, -0.1921],\n",
      "        ...,\n",
      "        [ 0.0283,  0.1239,  0.1689,  ..., -0.0157,  0.0230, -0.0899],\n",
      "        [-0.0539,  0.2732, -0.1208,  ...,  0.0699,  0.2464, -0.0570],\n",
      "        [ 0.2298, -0.1928,  0.2178,  ...,  0.0423, -0.2523,  0.0700]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010,  0.0010,\n",
      "        -0.0010, -0.0010,  0.0010,  0.0010,  0.0010, -0.0010,  0.0010, -0.0010,\n",
      "        -0.0010, -0.0010, -0.0010,  0.0010, -0.0010,  0.0010,  0.0010, -0.0010,\n",
      "         0.0010, -0.0010,  0.0010, -0.0010, -0.0010,  0.0010,  0.0010,  0.0010],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-0.2337, -0.1357,  0.4270,  0.3305, -0.3828,  0.0738, -0.3646, -0.2990,\n",
      "          0.0016,  0.3257, -0.2813, -0.3607,  0.1322,  0.2559, -0.1382, -0.0908,\n",
      "          0.4125, -0.1385, -0.0689, -0.2376, -0.1741,  0.1221,  0.3018, -0.2217,\n",
      "          0.0637, -0.3091,  0.3892,  0.2482, -0.0669,  0.1310, -0.2794, -0.2316]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0.], requires_grad=True))\n",
      "| Iteration 1  > I(W,Z): -6.38871  I(W,Z): 0.00132  I(X,Y): 0.63651  H(X): 0.63651\n",
      "(Parameter containing:\n",
      "tensor([[ 0.1298,  0.1080, -0.1307, -0.6581, -0.0726,  0.4257,  0.4149,  0.2435,\n",
      "          0.2615,  0.5480, -0.0232,  0.3126]], requires_grad=True), Parameter containing:\n",
      "tensor([[[-3.8791e-02,  6.7132e-02,  6.5062e-02,  8.4100e-02,  3.1511e-02,\n",
      "           4.1673e-02,  7.5459e-02, -3.7843e-02,  8.8292e-02,  2.2730e-02,\n",
      "          -1.6481e-01,  1.5935e-01],\n",
      "         [ 1.9285e-01,  1.4651e-01, -8.1352e-02, -8.8723e-02,  5.4743e-02,\n",
      "          -1.3734e-01,  5.1991e-02,  1.4709e-01,  5.0291e-03,  1.2029e-02,\n",
      "          -1.0634e-02, -1.4185e-01],\n",
      "         [ 1.2259e-01,  1.1923e-01, -1.6011e-02, -1.8372e-01, -6.2080e-02,\n",
      "           1.8473e-01,  1.6038e-01, -4.0320e-02,  1.3024e-01,  8.0948e-02,\n",
      "          -3.0537e-04,  1.9422e-01],\n",
      "         [-1.4855e-01, -3.2493e-02, -1.4007e-01,  5.0452e-02, -9.5313e-02,\n",
      "           1.6076e-02,  1.1193e-01, -4.3594e-02, -6.7174e-02, -1.1352e-01,\n",
      "           6.2942e-02,  4.2183e-02],\n",
      "         [-1.1850e-01,  7.2367e-02, -9.9853e-02, -1.8882e-01,  8.4136e-02,\n",
      "           1.9361e-01, -1.3754e-01,  1.6260e-01, -8.4369e-02,  8.9934e-02,\n",
      "          -7.8337e-02,  1.0085e-01],\n",
      "         [-1.6503e-01,  1.2838e-01, -1.8642e-01,  1.3913e-04, -1.3088e-01,\n",
      "           9.7895e-02,  5.0459e-02, -1.7046e-02, -7.9394e-02, -7.6582e-02,\n",
      "           1.7987e-01,  7.2295e-02],\n",
      "         [-1.7448e-01,  2.9009e-02, -1.3653e-01, -6.1469e-02, -1.2277e-01,\n",
      "           1.5411e-01,  1.2770e-01, -1.6557e-01, -1.1686e-01, -1.3814e-01,\n",
      "           1.9375e-01, -5.9490e-02],\n",
      "         [ 1.4790e-01, -1.9540e-01,  1.5815e-01, -1.4411e-01, -6.4567e-02,\n",
      "          -1.5497e-02, -1.0040e-01,  1.2635e-01,  9.5229e-02, -1.0828e-01,\n",
      "          -1.5564e-01, -1.9226e-01],\n",
      "         [ 3.3871e-02, -1.6289e-01,  1.4968e-01,  8.1643e-02, -1.6868e-02,\n",
      "          -1.2432e-01,  1.3640e-01, -1.3613e-01, -8.5897e-02,  1.8919e-01,\n",
      "           1.5244e-02,  3.1069e-02],\n",
      "         [-1.8257e-01,  1.6278e-01, -1.5265e-01, -7.6786e-02, -1.1098e-02,\n",
      "           6.6084e-02,  1.6080e-01,  1.1153e-01, -1.5603e-01, -1.0387e-01,\n",
      "          -1.3029e-01,  1.8951e-01],\n",
      "         [ 1.3873e-01,  8.1692e-02, -1.2510e-01, -9.9154e-02, -6.9711e-02,\n",
      "          -8.4024e-02, -1.1594e-03, -1.3005e-01, -4.0054e-04,  1.6482e-01,\n",
      "           9.5290e-02, -3.2066e-02],\n",
      "         [ 1.0573e-01, -1.7157e-01, -1.7766e-01, -9.8220e-02,  1.3716e-01,\n",
      "           1.1200e-02,  1.8756e-01,  3.6041e-02, -1.2247e-01, -6.9460e-05,\n",
      "           4.0697e-02, -5.9490e-02]]], requires_grad=True), Parameter containing:\n",
      "tensor([0.], requires_grad=True), Parameter containing:\n",
      "tensor([0.], requires_grad=True)) (Parameter containing:\n",
      "tensor([[-0.1269, -0.0362,  0.0342,  ...,  0.1603, -0.1641,  0.1440],\n",
      "        [-0.1783, -0.0233,  0.0261,  ...,  0.0826,  0.0650,  0.0428],\n",
      "        [ 0.1771,  0.0782,  0.0195,  ...,  0.1168, -0.1381, -0.1275],\n",
      "        ...,\n",
      "        [ 0.1192, -0.0771,  0.1133,  ..., -0.0535,  0.1635,  0.0675],\n",
      "        [-0.1211, -0.1535,  0.0854,  ...,  0.0618, -0.1434, -0.1760],\n",
      "        [-0.0594,  0.0272, -0.0520,  ..., -0.1170,  0.0974, -0.1804]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0018, -0.0019, -0.0014,  0.0016, -0.0020, -0.0020, -0.0006,  0.0019,\n",
      "        -0.0019, -0.0020, -0.0020, -0.0014,  0.0004,  0.0020,  0.0020, -0.0012,\n",
      "        -0.0020, -0.0017,  0.0019,  0.0019,  0.0020,  0.0019,  0.0018, -0.0018,\n",
      "        -0.0017, -0.0020,  0.0020,  0.0017, -0.0019,  0.0020,  0.0020, -0.0020],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0307,  0.1906,  0.2538,  ..., -0.0599,  0.1305,  0.2108],\n",
      "        [-0.0246, -0.1370,  0.0430,  ...,  0.0473, -0.2564, -0.3070],\n",
      "        [ 0.2510,  0.3037,  0.0955,  ...,  0.0277, -0.0499, -0.0580],\n",
      "        ...,\n",
      "        [ 0.2454,  0.2415, -0.2453,  ...,  0.0297,  0.0733, -0.1904],\n",
      "        [-0.1088,  0.0301, -0.2156,  ..., -0.2823, -0.0468, -0.1533],\n",
      "        [-0.0512, -0.0149, -0.2856,  ..., -0.2001, -0.2894, -0.1900]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0018, -0.0018,  0.0020,  0.0019, -0.0020, -0.0019, -0.0007,  0.0018,\n",
      "         0.0018,  0.0016, -0.0019, -0.0019, -0.0020, -0.0007,  0.0015,  0.0017,\n",
      "         0.0011,  0.0013, -0.0019,  0.0019,  0.0016, -0.0019, -0.0004, -0.0016,\n",
      "        -0.0019, -0.0020, -0.0018, -0.0020,  0.0004, -0.0018,  0.0018,  0.0017],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0936, -0.2542, -0.0477,  ..., -0.0039,  0.1620, -0.1442],\n",
      "        [-0.2153,  0.2310,  0.2310,  ...,  0.1255,  0.2734,  0.2234],\n",
      "        [ 0.2254, -0.2953,  0.1873,  ..., -0.0053,  0.1888,  0.2480],\n",
      "        ...,\n",
      "        [ 0.2564,  0.1689, -0.1238,  ..., -0.1006, -0.0999,  0.1426],\n",
      "        [ 0.0727, -0.2332,  0.0259,  ..., -0.0727, -0.0746,  0.0141],\n",
      "        [ 0.1046, -0.1641, -0.2364,  ...,  0.2674, -0.0599,  0.1561]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0016, -0.0019,  0.0019,  0.0017,  0.0013, -0.0020, -0.0019,  0.0020,\n",
      "         0.0014, -0.0018,  0.0020, -0.0012, -0.0018, -0.0019,  0.0016, -0.0017,\n",
      "         0.0017,  0.0014,  0.0019, -0.0020,  0.0019,  0.0015, -0.0016, -0.0019,\n",
      "        -0.0014, -0.0019, -0.0011, -0.0014,  0.0008, -0.0020, -0.0018, -0.0019],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-0.2353,  0.4220, -0.3339,  0.4156, -0.2428, -0.1341,  0.2875,  0.2785,\n",
      "          0.2448, -0.1310,  0.1686, -0.1715, -0.2893,  0.0170,  0.1319,  0.0109,\n",
      "         -0.3774,  0.0231, -0.1752,  0.0859,  0.0308, -0.1340, -0.2103, -0.0557,\n",
      "          0.4215,  0.3824,  0.3096, -0.2237,  0.3022, -0.0874, -0.1690,  0.3530]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0.], requires_grad=True)) (Parameter containing:\n",
      "tensor([[-0.0964, -0.1501, -0.0858,  ..., -0.0510, -0.1691, -0.0097],\n",
      "        [ 0.0762,  0.1269, -0.1047,  ...,  0.0514,  0.1132,  0.1054],\n",
      "        [-0.1681, -0.1810,  0.1641,  ..., -0.1674,  0.0990, -0.1816],\n",
      "        ...,\n",
      "        [-0.0901, -0.0089,  0.0278,  ...,  0.0890,  0.1694, -0.1535],\n",
      "        [-0.0090,  0.1532,  0.0546,  ..., -0.1329, -0.0987, -0.0668],\n",
      "        [ 0.0452, -0.0760, -0.0538,  ...,  0.0517,  0.0747, -0.1246]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0020, -0.0017,  0.0019, -0.0019, -0.0003, -0.0019, -0.0020,  0.0018,\n",
      "         0.0019, -0.0020,  0.0020,  0.0019,  0.0020, -0.0013,  0.0018,  0.0006,\n",
      "        -0.0018,  0.0019, -0.0010, -0.0018,  0.0020,  0.0020, -0.0020,  0.0020,\n",
      "         0.0019,  0.0011,  0.0018,  0.0020, -0.0018, -0.0011, -0.0020,  0.0013],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.1563,  0.0524, -0.0753,  ...,  0.0554, -0.1038, -0.2147],\n",
      "        [-0.0076,  0.0407,  0.2709,  ..., -0.2265,  0.0461,  0.0808],\n",
      "        [ 0.2282,  0.0235,  0.0667,  ...,  0.2012,  0.0870, -0.2827],\n",
      "        ...,\n",
      "        [ 0.0090, -0.2294, -0.0024,  ...,  0.1515,  0.2709, -0.3003],\n",
      "        [ 0.0765, -0.2988, -0.0954,  ..., -0.1315,  0.1456, -0.2496],\n",
      "        [ 0.1556, -0.2504, -0.1752,  ...,  0.0737,  0.1911,  0.1985]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0005, -0.0011, -0.0019, -0.0020, -0.0003,  0.0020,  0.0014,  0.0018,\n",
      "        -0.0014,  0.0019,  0.0020,  0.0018,  0.0019, -0.0004, -0.0020,  0.0018,\n",
      "        -0.0020, -0.0020, -0.0008,  0.0020,  0.0003, -0.0019,  0.0015, -0.0020,\n",
      "        -0.0017,  0.0020, -0.0019, -0.0019, -0.0020,  0.0020,  0.0020, -0.0020],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.2302, -0.2798, -0.0556,  ...,  0.2468,  0.2802, -0.2943],\n",
      "        [-0.1212,  0.2765, -0.1928,  ...,  0.0452, -0.1420,  0.1596],\n",
      "        [ 0.0801, -0.2856,  0.2123,  ..., -0.0866,  0.2711, -0.1912],\n",
      "        ...,\n",
      "        [ 0.0293,  0.1249,  0.1698,  ..., -0.0147,  0.0240, -0.0891],\n",
      "        [-0.0542,  0.2722, -0.1198,  ...,  0.0708,  0.2461, -0.0568],\n",
      "        [ 0.2293, -0.1934,  0.2170,  ...,  0.0429, -0.2513,  0.0690]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0020, -0.0019, -0.0020, -0.0020, -0.0018, -0.0019, -0.0010,  0.0020,\n",
      "        -0.0004, -0.0014,  0.0020,  0.0006,  0.0020, -0.0016,  0.0010, -0.0017,\n",
      "        -0.0017, -0.0012, -0.0020,  0.0008, -0.0019,  0.0020,  0.0019, -0.0018,\n",
      "         0.0020, -0.0020,  0.0019, -0.0017, -0.0015,  0.0020,  0.0020,  0.0006],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-0.2347, -0.1348,  0.4280,  0.3296, -0.3819,  0.0729, -0.3637, -0.3000,\n",
      "          0.0025,  0.3260, -0.2816, -0.3600,  0.1332,  0.2566, -0.1372, -0.0906,\n",
      "          0.4118, -0.1393, -0.0679, -0.2384, -0.1731,  0.1231,  0.3025, -0.2208,\n",
      "          0.0641, -0.3081,  0.3886,  0.2473, -0.0662,  0.1320, -0.2791, -0.2320]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0.], requires_grad=True))\n",
      "| Iteration 2  > I(W,Z): -6.40854  I(W,Z): 0.00057  I(X,Y): 0.63651  H(X): 0.63651\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[177], line 19\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRunning Experiment\u001B[39m\u001B[38;5;124m'\u001B[39m, i, params)\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mState Space of\u001B[39m\u001B[38;5;124m'\u001B[39m, params[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mto Embedding Space of\u001B[39m\u001B[38;5;124m'\u001B[39m, params[\u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m---> 19\u001B[0m run_dim_red_process(device, \u001B[38;5;241m*\u001B[39mparams)\n",
      "Cell \u001B[0;32mIn[175], line 31\u001B[0m, in \u001B[0;36mrun_dim_red_process\u001B[0;34m(device, state_space, embedding_space_size, batch_size, num_steps)\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, (final_state, initial_state) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataset):\n\u001B[1;32m     30\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 31\u001B[0m     loss, loss2 \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtest_objective_function(initial_state, final_state)\n\u001B[1;32m     32\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mmean()\n\u001B[1;32m     33\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "Cell \u001B[0;32mIn[157], line 20\u001B[0m, in \u001B[0;36mEmbeddingMI3.test_objective_function\u001B[0;34m(self, x, y)\u001B[0m\n\u001B[1;32m     17\u001B[0m z \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder\u001B[38;5;241m.\u001B[39mencoder_sample(y)\u001B[38;5;241m.\u001B[39mdetach()\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# print(w, z, x, y)\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m w_tilde \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding_sampler\u001B[38;5;241m.\u001B[39mrun_batched_gibbs(z)\u001B[38;5;241m.\u001B[39mdetach()\n\u001B[1;32m     21\u001B[0m x_tilde \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder_sampler\u001B[38;5;241m.\u001B[39mrun_batched_gibbs(w)\u001B[38;5;241m.\u001B[39mdetach()\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m-\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_func(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_ones, \u001B[38;5;241m*\u001B[39m(z, y, w, x, w_tilde, x_tilde), \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder\u001B[38;5;241m.\u001B[39mparams(), \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder\u001B[38;5;241m.\u001B[39mparams(), \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding_dynamics\u001B[38;5;241m.\u001B[39mparams()), \\\n\u001B[1;32m     24\u001B[0m         mutual_info_score(w\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m), z\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/Desktop/QuantumDynamicsAI/GibbsSampling.py:41\u001B[0m, in \u001B[0;36mBatchedConditionalGibbsSampler.run_batched_gibbs\u001B[0;34m(self, z)\u001B[0m\n\u001B[1;32m     39\u001B[0m     x_prime \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mclone()\n\u001B[1;32m     40\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdim):\n\u001B[0;32m---> 41\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgibbs_update(x, x_prime, j, conditioned)  \u001B[38;5;66;03m# sample size x batch size x dim\u001B[39;00m\n\u001B[1;32m     42\u001B[0m     x \u001B[38;5;241m=\u001B[39m x_prime\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minitial_guess \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mclone()  \u001B[38;5;66;03m# initial guess changes throughout iterations\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/Desktop/QuantumDynamicsAI/GibbsSampling.py:59\u001B[0m, in \u001B[0;36mBatchedConditionalGibbsSampler.gibbs_update\u001B[0;34m(self, x, x_prime, index, conditioned)\u001B[0m\n\u001B[1;32m     57\u001B[0m index_1[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, index] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mones\n\u001B[1;32m     58\u001B[0m log_likelihood_zero \u001B[38;5;241m=\u001B[39m unnormalized_log_probs(index_0)\n\u001B[0;32m---> 59\u001B[0m log_likelihood_one \u001B[38;5;241m=\u001B[39m unnormalized_log_probs(index_1)\n\u001B[1;32m     60\u001B[0m x_prime[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, index] \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdistributions\u001B[38;5;241m.\u001B[39mbernoulli\u001B[38;5;241m.\u001B[39mBernoulli(\n\u001B[1;32m     61\u001B[0m     logits\u001B[38;5;241m=\u001B[39mlog_likelihood_one \u001B[38;5;241m-\u001B[39m log_likelihood_zero)\u001B[38;5;241m.\u001B[39msample()\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/Desktop/QuantumDynamicsAI/GibbsSampling.py:50\u001B[0m, in \u001B[0;36mBatchedConditionalGibbsSampler.gibbs_update.<locals>.<lambda>\u001B[0;34m(w)\u001B[0m\n\u001B[1;32m     48\u001B[0m unnormalized_log_probs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjoint_distribution, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124munnormalized_log_probs_w_given_z_double_batched\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m---> 50\u001B[0m     unnormalized_log_probs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m w: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjoint_distribution\u001B[38;5;241m.\u001B[39munnormalized_log_probs_w_given_z_double_batched_params(conditioned, w, \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjoint_distribution\u001B[38;5;241m.\u001B[39mparams())\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     52\u001B[0m     unnormalized_log_probs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m w: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjoint_distribution\u001B[38;5;241m.\u001B[39munnormalized_log_probs_a_given_b_double_batched(w, conditioned)\n",
      "File \u001B[0;32m~/Desktop/QuantumDynamicsAI/DiscreteVariationalParameterizationsDeepV3.py:84\u001B[0m, in \u001B[0;36munnormalized_log_probs_w_given_z_double_batched_params\u001B[0;34m(z, w, W1, b1, W2, b2, W3, b3, W4, b4)\u001B[0m\n\u001B[1;32m     82\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munnormalized_log_probs_w_given_z\u001B[39m(\u001B[38;5;28mself\u001B[39m, z, w):\n\u001B[1;32m     83\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m-\u001B[39mEnergyBasedModelEmbeddingDynamics\u001B[38;5;241m.\u001B[39menergy(z, w, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear_1_weight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear_1_bias,\n\u001B[0;32m---> 84\u001B[0m                                                      \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear_2_weight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear_2_bias,\n\u001B[1;32m     85\u001B[0m                                                      \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear_3_weight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear_3_bias,\n\u001B[1;32m     86\u001B[0m                                                      \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear_4_weight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear_4_bias)\n",
      "File \u001B[0;32m~/Desktop/QuantumDynamicsAI/DiscreteVariationalParameterizationsDeepV3.py:67\u001B[0m, in \u001B[0;36menergy\u001B[0;34m(z, w, W1, b1, W2, b2, W3, b3, W4, b4)\u001B[0m\n\u001B[1;32m     65\u001B[0m temp \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mrelu(temp)\n\u001B[1;32m     66\u001B[0m temp \u001B[38;5;241m=\u001B[39m dropout(temp)\n\u001B[0;32m---> 67\u001B[0m temp \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mlinear(temp, W3, b3)\n\u001B[1;32m     68\u001B[0m temp \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mrelu(temp)\n\u001B[1;32m     69\u001B[0m temp \u001B[38;5;241m=\u001B[39m dropout(temp)\n",
      "File \u001B[0;32m~/Desktop/QuantumDynamicsAI/DiscreteVariationalParameterizationsDeepV3.py:56\u001B[0m, in \u001B[0;36menergy_function_bilinear\u001B[0;34m(i1, i2, W1, b1, W2, b2, W3, b3, W4, b4)\u001B[0m\n\u001B[1;32m     52\u001B[0m @staticmethod\n\u001B[1;32m     53\u001B[0m def energy_function_bilinear(i1, i2, W1, b1, W2, b2, W3, b3, W4, b4):\n\u001B[1;32m     54\u001B[0m     batch_size = i1.shape[0]\n\u001B[1;32m     55\u001B[0m     # outer_product = torch.einsum('bi,bj->bij', (i1, i2))\n\u001B[0;32m---> 56\u001B[0m     # outer_product = outer_product.view(batch_size, -1)\n\u001B[1;32m     57\u001B[0m     outer_product = torch.bmm(i1.unsqueeze(2), i2.unsqueeze(1)).view(batch_size, -1)\n\u001B[1;32m     58\u001B[0m     dropout = torch.nn.Dropout(p=0.1)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('Device Running: ', device)\n",
    "\n",
    "    # experiments = [\n",
    "    #     (12, 2, 1024), (12, 3, 1024), (12, 4, 1024), (12, 5, 1024),\n",
    "    #     (12, 6, 1024), (12, 7, 1024), (12, 8, 1024), (12, 9, 1024),\n",
    "    #     (12, 10, 1024), (12, 11, 1024), (12, 12, 1024)\n",
    "    # ]\n",
    "    \n",
    "    experiments = [\n",
    "        (12, 12, 1024), (12, 2, 1024), (12, 4, 1024), (12, 8, 1024)\n",
    "    ]\n",
    "\n",
    "    for i, params in enumerate(experiments):\n",
    "        print('Running Experiment', i, params)\n",
    "        print('State Space of', params[0], 'to Embedding Space of', params[1])\n",
    "        \n",
    "        run_dim_red_process(device, *params)\n",
    "        #find_learning_rate(device, *params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPMeg+QPYunP2XoyRdolJ5b",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
