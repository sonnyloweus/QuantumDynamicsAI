{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Attention Pseudocode"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e99be2b4a076444"
  },
  {
   "cell_type": "markdown",
   "source": [
    "1.   **Tokenization**       (word --> vector)\n",
    "\n",
    "Embeds a token $w$ into a higher dimensional vector $\\overrightarrow{E}$. At this point, $\\overrightarrow{E}$ tells the model what the word is, and where it is locationally in the sequence. (Has yet to encode any inter-token relationships)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cf4ab203ae020cd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.   **Attention** (contextualization)\n",
    "\n",
    "A trained attention block takes a generic tokenization and updates its direction to point towards a space that is more highly correlated to its surrounding tokens.\n",
    "\n",
    "In other words, it allows its surrounding tokens to affect its own embedding. Here is how thats done:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7e99eb9e22e4e18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "338aa4abc88f9722"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**The Matrix Manipulation:**\n",
    "\n",
    "Every token has a corresponding vector $\\overrightarrow{E}$. During every step of attention, each token generates a query vector $\\overrightarrow{Q}$ such that $\\overrightarrow{Q}$ = $M$ * $\\overrightarrow{E}$ where $M_Q$ is a query matrix of learned weights. Take the phrase dirty quantum plumbing for example.\n",
    "\n",
    "[dirty]:  $M_Q * \\overrightarrow{E_0} = \\overrightarrow{Q_0}$\n",
    "[quantum]:   $M_Q * \\overrightarrow{E_1} = \\overrightarrow{Q_1}$\n",
    "[plumbing]:  $M_Q * \\overrightarrow{E_2} = \\overrightarrow{Q_2}$\n",
    "\n",
    "An attention block contains many different attention mechanisms, called a head. Each attention head has a different corresponding matrix query $M_Q$, and each query matrix can be imagined to ask some form of contextualization. For example, what are the adjectives that preceed a noun and thus describe it? Or, what is the setting of the sentence?\n",
    "\n",
    "To answer these queries, there are also key matrices $M_K$, that are also learned weights that seem to answer the question.\n",
    "\n",
    "[dirty]:  $M_K * \\overrightarrow{E_0} = \\overrightarrow{K_0}$\n",
    "[quantum]:   $M_K * \\overrightarrow{E_1} = \\overrightarrow{K_1}$\n",
    "[plumbing]:  $M_K * \\overrightarrow{E_2} = \\overrightarrow{K_2}$\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a06c88aff2ba2344"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bdbb394099bfefdf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "How well a key answers a query is defined by how directionally aligned the key vector is to the query vector. This is done by computing a dot product between every key-query pair.\n",
    "\n",
    "$\\overrightarrow{K_0}$•$\\overrightarrow{Q_0}$___$\\overrightarrow{K_0}$•$\\overrightarrow{Q_1}$___$\\overrightarrow{K_0}$•$\\overrightarrow{Q_2}$\n",
    "$\\overrightarrow{K_1}$•$\\overrightarrow{Q_0}$___$\\overrightarrow{K_1}$•$\\overrightarrow{Q_1}$___$\\overrightarrow{K_1}$•$\\overrightarrow{Q_2}$\n",
    "$\\overrightarrow{K_2}$•$\\overrightarrow{Q_0}$___$\\overrightarrow{K_2}$•$\\overrightarrow{Q_1}$___$\\overrightarrow{K_2}$•$\\overrightarrow{Q_2}$\n",
    "\n",
    "Larger dot products produce larger values where the keys attend to the queries. In this case, dirty and quantum would attend (match) well."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5c9e70562738a46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7a1187b9de4d78ed"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
