{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qu_Gw4cCnX4t"
      ],
      "authorship_tag": "ABX9TyPOE5srDUN7b1XBoq0yhwEv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sonnyloweus/QuantumDynamicsAI/blob/main/BoltzmannEncoderDecoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split"
      ],
      "metadata": {
        "id": "tzxJehwgnm13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "directory_path = '/content/drive/MyDrive/Quantum/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQOBDHptnsJl",
        "outputId": "f976d7e0-939f-4b6a-b145-1489fccfd8a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder Decoder Definition"
      ],
      "metadata": {
        "id": "VibxST8unTYC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zfzm3Hubb2Sr"
      },
      "outputs": [],
      "source": [
        "class BoltzmannEncoderDecoder(nn.Module):\n",
        "    def __init__(self, batch_size, in_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.in_dim = in_dim  # dimension of y\n",
        "        self.out_dim = out_dim  # dimension of z\n",
        "\n",
        "        self.b = nn.Parameter(torch.zeros((1, self.out_dim)))\n",
        "        self.c = nn.Parameter(torch.zeros((1, self.in_dim)))\n",
        "        self.W = nn.Parameter(torch.zeros((1, self.out_dim, self.in_dim)))\n",
        "        self.init()\n",
        "\n",
        "    def init(self):\n",
        "        torch.nn.init.xavier_uniform_(self.b)\n",
        "        torch.nn.init.xavier_uniform_(self.c)\n",
        "        torch.nn.init.xavier_uniform_(self.W, gain=1)\n",
        "\n",
        "    @staticmethod\n",
        "    def conditional_log_probability_x_given_w(w, x, W, c):\n",
        "        batch_size = x.shape[0]\n",
        "        return torch.nn.functional.logsigmoid((2 * x.view(batch_size, -1, 1) - 1) *\n",
        "                                              (c.unsqueeze(-1) + torch.transpose(W, 1, 2) @\n",
        "                                               w.view(batch_size, -1, 1))).sum(dim=-2)\n",
        "    @staticmethod\n",
        "    def conditional_log_probability_w_given_x(w, x, W, b):\n",
        "        batch_size = x.shape[0]\n",
        "        return torch.nn.functional.logsigmoid((2 * w.view(batch_size, -1, 1) - 1) *\n",
        "                                              (b.unsqueeze(-1) + W @ x.view(batch_size, -1,\n",
        "                                                                                      1))).sum(dim=-2)\n",
        "    @staticmethod\n",
        "    def encoder_sample(x, W, b):\n",
        "        batch_size = x.shape[0]\n",
        "        thresholds = torch.sigmoid(\n",
        "            (b.unsqueeze(-1) + W @ x.view(batch_size, -1, 1)))\n",
        "        return (torch.rand_like(thresholds) < thresholds).float().squeeze(-1)\n",
        "\n",
        "    @staticmethod\n",
        "    def decoder_sample(w, W, c):\n",
        "        batch_size = w.shape[0]\n",
        "        thresholds = torch.sigmoid(\n",
        "            (c.unsqueeze(-1) + torch.transpose(W, 1, 2) @ w.view(batch_size, -1, 1)))\n",
        "        return (torch.rand_like(thresholds) < thresholds).float().squeeze(-1)\n",
        "\n",
        "    def conditional_log_probability_x_given_w_(self, w, x):\n",
        "        return BoltzmannEncoderDecoder.conditional_log_probability_x_given_w(w, x, self.W, self.c)\n",
        "\n",
        "    def conditional_log_probability_w_given_x_(self, w, x):\n",
        "        return BoltzmannEncoderDecoder.conditional_log_probability_w_given_x(w,x, self.W, self.b)\n",
        "\n",
        "    def conditional_log_probability_w_given_x_double_batched_(self, w, x):\n",
        "        dim_0 = w.shape[0]\n",
        "        dim_1 = w.shape[1]\n",
        "        return BoltzmannEncoderDecoder.conditional_log_probability_w_given_x(w.reshape(dim_0*dim_1, -1),x.reshape(dim_0*dim_1, -1), self.W, self.b).reshape(dim_0, dim_1, -1)\n",
        "\n",
        "    def conditional_log_probability_w_given_x_double_batched(w, x, W, b):\n",
        "        dim_0 = w.shape[0]\n",
        "        dim_1 = w.shape[1]\n",
        "        return BoltzmannEncoderDecoder.conditional_log_probability_w_given_x(w.reshape(dim_0*dim_1, -1),x.reshape(dim_0*dim_1, -1), W, b).reshape(dim_0, dim_1, -1)\n",
        "\n",
        "    # simple since factorial distribution\n",
        "    def encoder_sample_(self, x):\n",
        "        return BoltzmannEncoderDecoder.encoder_sample(x, self.W, self.b)\n",
        "\n",
        "    # simple since factorial distribution\n",
        "    def batched_encoder_sample_(self, x):\n",
        "        dim_0 = x.shape[0]\n",
        "        dim_1 = x.shape[1]\n",
        "        return BoltzmannEncoderDecoder.encoder_sample(x.reshape(dim_0*dim_1, -1), self.W, self.b).reshape(dim_0, dim_1, -1)\n",
        "\n",
        "    # simple since factorial distribution\n",
        "    @staticmethod\n",
        "    def batched_encoder_sample(x, W, b):\n",
        "        dim_0 = x.shape[0]\n",
        "        dim_1 = x.shape[1]\n",
        "        return BoltzmannEncoderDecoder.encoder_sample(x.reshape(dim_0*dim_1, -1), W, b).reshape(dim_0, dim_1, -1)\n",
        "\n",
        "\n",
        "    def decoder_sample_(self, w):\n",
        "        return BoltzmannEncoderDecoder.decoder_sample(w, self.W, self.c)\n",
        "\n",
        "\n",
        "class EnergyBasedModelEmbeddingDynamics(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim = None):\n",
        "        super().__init__()\n",
        "        self.dim = dim  # dimension of y\n",
        "        if hidden_dim is None:\n",
        "            hidden_dim = 12\n",
        "        #self.linear_1_weight = nn.Parameter(torch.zeros((hidden_dim, self.dim*self.dim)))\n",
        "        self.linear_1_weight = nn.Parameter(torch.zeros((hidden_dim, 2*self.dim)))\n",
        "\n",
        "        self.linear_1_bias = nn.Parameter(torch.zeros((hidden_dim)))\n",
        "        self.linear_2_weight = nn.Parameter(torch.zeros((1, hidden_dim)))\n",
        "        self.linear_2_bias = nn.Parameter(torch.zeros((1)))\n",
        "        self.init()\n",
        "\n",
        "    def init(self):\n",
        "        torch.nn.init.xavier_uniform_(self.linear_1_weight)\n",
        "        torch.nn.init.xavier_uniform_(self.linear_2_weight)\n",
        "\n",
        "    def unnormalized_log_probs_w_given_z_double_batched_(self, z, w):\n",
        "        return EnergyBasedModelEmbeddingDynamics.unnormalized_log_probs_w_given_z_double_batched(z, w, self.linear_1_weight, self.linear_1_bias, self.linear_2_weight, self.linear_2_bias)\n",
        "\n",
        "    @staticmethod\n",
        "    def energy_function_bilinear(i1, i2, W1, b1, W2, b2):\n",
        "        batch_size = i1.shape[0]\n",
        "        outer_product = torch.einsum('bi,bj->bij', (i1, i2))\n",
        "        outer_product = outer_product.view(batch_size, -1)\n",
        "        temp = torch.nn.functional.linear(outer_product, W1, bias=b1)\n",
        "        temp = torch.nn.functional.relu(temp)\n",
        "        o = torch.nn.functional.linear(temp, W2, b2)\n",
        "        return o\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def energy_function_linear(i1, i2, W1, b1, W2, b2):\n",
        "        batch_size = i1.shape[0]\n",
        "        #outer_product = torch.einsum('bi,bj->bij', (i1, i2))\n",
        "        #outer_product = outer_product.view(batch_size, -1)\n",
        "        temp = torch.nn.functional.linear(torch.cat((i1,i2), dim = -1), W1, bias=b1)\n",
        "        temp = torch.nn.functional.relu(temp)\n",
        "        o = torch.nn.functional.linear(temp, W2, b2)\n",
        "        return o\n",
        "\n",
        "    @staticmethod\n",
        "    def _energy(z, w, W1, b1, W2, b2):\n",
        "        return EnergyBasedModelEmbeddingDynamics.energy_function_linear(z,w, W1, b1, W2, b2)\n",
        "        #return EnergyBasedModelEmbeddingDynamics.energy_function_bilinear(z,w, W1, b1, W2, b2)\n",
        "\n",
        "    @staticmethod\n",
        "    def unnormalized_log_probs_w_given_z(z, w, W1, b1, W2, b2):\n",
        "        return -EnergyBasedModelEmbeddingDynamics._energy(z, w, W1, b1, W2, b2)\n",
        "\n",
        "    def unnormalized_log_probs_w_given_z_(self, z, w):\n",
        "        return -EnergyBasedModelEmbeddingDynamics._energy(z, w, self.linear_1_weight, self.linear_1_bias, self.linear_2_weight, self.linear_2_bias)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def unnormalized_log_probs_w_given_z_double_batched(z, w, W1, b1, W2, b2):\n",
        "        first_dim = z.shape[0]\n",
        "        second_dim = z.shape[1]\n",
        "        dim = z.shape[2]\n",
        "        energy = EnergyBasedModelEmbeddingDynamics._energy(z.reshape(-1, dim), w.reshape(-1, dim), W1, b1, W2, b2).reshape(first_dim, second_dim, 1)\n",
        "        return -energy\n",
        "\n",
        "    @staticmethod\n",
        "    def expected_unnormalized_log_probs_w_given_z(z, w, W1, b1, W2, b2):\n",
        "        samples_dim = z.shape[0]\n",
        "        batch_dim = z.shape[1]\n",
        "        dim = z.shape[2]\n",
        "        energy = EnergyBasedModelEmbeddingDynamics._energy(z.reshape(-1, dim), w.reshape(-1, dim), W1, b1, W2, b2).reshape(samples_dim, batch_dim, 1)\n",
        "        return -energy.mean(dim=0)\n",
        "\n",
        "    # for small state spaces it is possible to manually compute the partition function\n",
        "    @staticmethod\n",
        "    def log_partition_function(z, W1, b1, W2, b2):\n",
        "        dim = z.shape[-1]\n",
        "        z = z.view(-1, 1, dim).expand(-1, 2 ** dim, -1)\n",
        "        W = torch.arange(0, 2 ** dim, device = z.device).unsqueeze(-1).bitwise_and(2 ** torch.arange(\n",
        "            dim, device = z.device)).ne(0).unsqueeze(0).expand(z.shape[0], -1, -1).float()\n",
        "        log_probs = EnergyBasedModelEmbeddingDynamics.unnormalized_log_probs_w_given_z_double_batched(z, W, W1, b1, W2, b2)\n",
        "        partitions = torch.logsumexp(log_probs, dim=1)\n",
        "        return partitions\n",
        "\n",
        "    # the initial state will help create the optimal proposal distribution\n",
        "    @staticmethod\n",
        "    @torch.no_grad()\n",
        "    def estimated_log_partition_function_better(z, initial_state, W, b, _, W1, b1, W2, b2, samples = 512):\n",
        "            z = z.expand(samples, -1, -1)\n",
        "            initial_state = initial_state.expand(samples, -1, -1)\n",
        "            w_batched = BoltzmannEncoderDecoder.batched_encoder_sample(initial_state, W, b)\n",
        "            proposal_log_probs = BoltzmannEncoderDecoder.conditional_log_probability_w_given_x_double_batched(w_batched, initial_state, W, b)\n",
        "            log_probs = EnergyBasedModelEmbeddingDynamics.unnormalized_log_probs_w_given_z_double_batched(z, w_batched, W1, b1, W2, b2)\n",
        "            return  - math.log(samples) + torch.logsumexp(log_probs-proposal_log_probs, dim = 0)\n",
        "\n",
        "\n",
        "    # do some importance sampling here, note that this is probably good enough for training where the gradient can be noisy\n",
        "    # and is in generally the right direction, but is definitely not good enough for evaluation\n",
        "    @staticmethod\n",
        "    @torch.no_grad()\n",
        "    def estimated_log_partition_function(z, W1, b1, W2, b2, samples = 512):\n",
        "        dim = z.shape[-1]\n",
        "        z = z.expand(samples, -1, -1)\n",
        "        w = (torch.rand_like(z) < 0.5).float()\n",
        "        log_probs = EnergyBasedModelEmbeddingDynamics.unnormalized_log_probs_w_given_z_double_batched(z, w, W1, b1, W2, b2)\n",
        "        return dim*math.log(2) - math.log(samples) + torch.logsumexp(log_probs, dim = 0)\n",
        "\n",
        "    def estimated_log_partition_function_(self, z):\n",
        "        return EnergyBasedModelEmbeddingDynamics.estimated_log_partition_function(z, self.linear_1_weight, self.linear_1_bias, self.linear_2_weight, self.linear_2_bias)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def normalized_log_probabilities_w_given_z(z, w, W1, b1, W2, b2):\n",
        "        return EnergyBasedModelEmbeddingDynamics.unnormalized_log_probs_w_given_z(z, w, W1, b1, W2, b2) - EnergyBasedModelEmbeddingDynamics.log_partition_function(z, W1, b1, W2, b2)\n",
        "\n",
        "\n",
        "    def normalized_log_probabilities_w_given_z_(self, z, w):\n",
        "        return EnergyBasedModelEmbeddingDynamics.normalized_log_probabilities_w_given_z(z,w, self.linear_1_weight, self.linear_1_bias, self.linear_2_weight, self.linear_2_bias)\n",
        "\n",
        "    def estimated_normalized_log_probabilities_w_given_z_(self, z, w):\n",
        "        return EnergyBasedModelEmbeddingDynamics.estimated_normalized_log_probabilities_w_given_z(z,w, self.linear_1_weight, self.linear_1_bias, self.linear_2_weight, self.linear_2_bias)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def estimated_normalized_log_probabilities_w_given_z(z, w, W1, b1, W2, b2):\n",
        "        return EnergyBasedModelEmbeddingDynamics.unnormalized_log_probs_w_given_z(z, w, W1, b1, W2, b2) - EnergyBasedModelEmbeddingDynamics.estimated_log_partition_function(z, W1, b1, W2, b2)\n",
        "\n",
        "    @staticmethod\n",
        "    def estimated_normalized_log_probabilities_w_given_z_better(z, w, x, W, b, _, W1, b1, W2, b2):\n",
        "        return EnergyBasedModelEmbeddingDynamics.unnormalized_log_probs_w_given_z(z, w, W1, b1, W2, b2) - EnergyBasedModelEmbeddingDynamics.estimated_log_partition_function_better(z, x, W, b, None, W1, b1, W2, b2)\n",
        "\n",
        "    @staticmethod\n",
        "    def estimated_normalized_log_probabilities_w_given_z_better_(z, w, x, model, samples = 1024):\n",
        "        z_tilde = z.expand(samples, -1, -1)\n",
        "        initial_state = x.expand(samples, -1, -1)\n",
        "\n",
        "        # all from the proposal distribution\n",
        "        w_tilde = model.encoder_decoder.batched_encoder_sample_(initial_state)\n",
        "        proposal_log_probs = model.encoder_decoder.conditional_log_probability_w_given_x_double_batched_(w_tilde, initial_state)\n",
        "\n",
        "        log_probs = model.embedding_dynamics.unnormalized_log_probs_w_given_z_double_batched_(z_tilde, w_tilde)\n",
        "        return  model.embedding_dynamics.unnormalized_log_probs_w_given_z_(z,w) + math.log(samples) - torch.logsumexp(log_probs-proposal_log_probs, dim = 0)\n",
        "\n",
        "\n",
        "class EnergyBasedEncoderDecoder(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim,  hidden_dim = None):\n",
        "        super().__init__()\n",
        "        self.in_dim = in_dim  # dimension of x\n",
        "        self.out_dim = out_dim # dimension of w\n",
        "        if hidden_dim is None:\n",
        "            hidden_dim = 12\n",
        "\n",
        "        self.linear_1_weight = nn.Parameter(torch.zeros((hidden_dim, self.in_dim+ self.out_dim)))\n",
        "        self.linear_1_bias = nn.Parameter(torch.zeros((hidden_dim)))\n",
        "        self.linear_2_weight = nn.Parameter(torch.zeros((1, hidden_dim)))\n",
        "        self.linear_2_bias = nn.Parameter(torch.zeros((1)))\n",
        "        self.init()\n",
        "\n",
        "    def init(self):\n",
        "        torch.nn.init.xavier_uniform_(self.linear_1_weight)\n",
        "        torch.nn.init.xavier_uniform_(self.linear_2_weight)\n",
        "\n",
        "    def forward(self, input_a, input_b):\n",
        "        return self._energy(input_a, input_b, self.linear_1_weight, self.linear_1_bias, self.linear_2_weight, self.linear_2_bias)\n",
        "\n",
        "    @staticmethod\n",
        "    def energy_function_linear(a, b, W1, b1, W2, b2):\n",
        "        temp = torch.nn.functional.linear(torch.cat((a,b), dim = -1), W1, bias=b1)\n",
        "        temp = torch.nn.functional.relu(temp)\n",
        "        o = torch.nn.functional.linear(temp, W2, b2)\n",
        "        return o\n",
        "\n",
        "    @staticmethod\n",
        "    def _energy(a, b, W1, b1, W2, b2):\n",
        "        return EnergyBasedEncoderDecoder.energy_function_linear(a, b, W1, b1, W2, b2)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def unnormalized_log_probs_a_given_b(a, b, W1, b1, W2, b2):\n",
        "        return -EnergyBasedEncoderDecoder._energy(a, b, W1, b1, W2, b2)\n",
        "\n",
        "    def unnormalized_log_probs_a_given_b_(self, a, b):\n",
        "        return EnergyBasedEncoderDecoder.unnormalized_log_probs_a_given_b(a, b, self.linear_1_weight, self.linear_1_bias, self.linear_2_weight, self.linear_2_bias)\n",
        "\n",
        "    @staticmethod\n",
        "    def unnormalized_log_probs_a_given_b_double_batched(a, b, W1, b1, W2, b2):\n",
        "        first_dim = a.shape[0]\n",
        "        second_dim = a.shape[1]\n",
        "        dim_a = a.shape[2]\n",
        "        dim_b = b.shape[2]\n",
        "        energy = EnergyBasedEncoderDecoder._energy(a.reshape(-1, dim_a), b.reshape(-1, dim_b), W1, b1, W2, b2).reshape(first_dim, second_dim, 1)\n",
        "        return -energy\n",
        "\n",
        "    # for small state spaces it is possible to manually compute the partition function\n",
        "    @staticmethod\n",
        "    def log_partition_function(b, W1, b1, W2, b2):\n",
        "        dim = b.shape[-1]\n",
        "        b = b.view(-1, 1, dim).expand(-1, 2 ** dim, -1)\n",
        "        A = torch.arange(0, 2 ** dim, device = b.device).unsqueeze(-1).bitwise_and(2 ** torch.arange(\n",
        "            dim, device = b.device)).ne(0).unsqueeze(0).expand(b.shape[0], -1, -1).float()\n",
        "        log_probs = EnergyBasedEncoderDecoder.unnormalized_log_probs_a_given_b_double_batched(A, b, W1, b1, W2, b2)\n",
        "        partitions = torch.logsumexp(log_probs, dim=1)\n",
        "        return partitions\n",
        "\n",
        "    @staticmethod\n",
        "    def conditional_log_probability_a_given_b(a, b, W1, b1, W2, b2):\n",
        "        return EnergyBasedEncoderDecoder.unnormalized_log_probs_a_given_b(a, b, W1, b1, W2, b2) - EnergyBasedEncoderDecoder.log_partition_function(b, W1, b1, W2, b2)\n",
        "\n",
        "\n",
        "    def conditional_log_probability_a_given_b_(self, a, b):\n",
        "        return EnergyBasedEncoderDecoder.conditional_log_probability_a_given_b(a, b, self.linear_1_weight, self.linear_1_bias, self.linear_2_weight, self.linear_2_bias)\n",
        "\n",
        "\n",
        "    def conditional_log_probability_a_given_b_double_batched_(self, a, b):\n",
        "        dim_0 = a.shape[0]\n",
        "        dim_1 = a.shape[1]\n",
        "        return EnergyBasedEncoderDecoder.conditional_log_probability_a_given_b(a.reshape(dim_0*dim_1, -1),b.reshape(dim_0*dim_1, -1)).reshape(dim_0, dim_1, -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing"
      ],
      "metadata": {
        "id": "qu_Gw4cCnX4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and prepare the data\n",
        "data_path = '/content/drive/MyDrive/Quantum/quantum_simulation_data.pkl'\n",
        "data = pd.read_pickle(data_path)\n",
        "\n",
        "def to_tensor(item):\n",
        "    if isinstance(item, torch.Tensor):\n",
        "        # If already a tensor, clone and detach it to prevent issues\n",
        "        return item.clone().detach()\n",
        "    elif isinstance(item, str):\n",
        "        # Convert string to list using ast.literal_eval\n",
        "        item_list = ast.literal_eval(item)\n",
        "        return torch.tensor(item_list)\n",
        "    elif isinstance(item, list):\n",
        "        return torch.tensor(item)\n",
        "    else:\n",
        "        raise ValueError(f\"Expected a list or tensor, but got {type(item)}\")\n",
        "\n",
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a07XJB2fmVET",
        "outputId": "f82e9da0-e498-4c68-c7e9-5bd44e9a353f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64000, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['Initial_State'] = [to_tensor(lst) for lst in data['Initial_State']]\n",
        "data['Final_State'] = [to_tensor(lst) for lst in data['Final_State']]\n",
        "\n",
        "print(data['Final_State'][0])\n",
        "\n",
        "# Convert Series to list of tensors before stacking\n",
        "initial_state_tensors = list(data['Initial_State'])\n",
        "final_state_tensors = list(data['Final_State'])\n",
        "\n",
        "dataset = TensorDataset(torch.stack(initial_state_tensors), torch.stack(final_state_tensors))\n",
        "print(dataset)\n",
        "\n",
        "# Define the split sizes (e.g., 80% train, 20% test)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "# Perform the train-test split\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Example of how to use DataLoader for training and testing\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(initial_state_tensors, final_state_tensors, test_size=0.2, random_state=42)\n",
        "# train_loader = torch.utils.data.DataLoader(list(zip(X_train, y_train)), batch_size=32, shuffle=False)\n",
        "# test_loader = torch.utils.data.DataLoader(list(zip(X_test, y_test)), batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHSYKX6wn1zm",
        "outputId": "1cfa31f0-6bf4-4eb3-8963-7e345bb39a61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1.])\n",
            "<torch.utils.data.dataset.TensorDataset object at 0x7b37e0179690>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Evaluation"
      ],
      "metadata": {
        "id": "WBilM5rxnaez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "input_dim = 12  # Assuming each bitstring has 12 bits\n",
        "hidden_dim = 64\n",
        "num_layers = 3\n",
        "num_heads = 4\n",
        "output_dim = 12  # Predicting 12-bit output\n",
        "max_seq_len = 12  # Maximum length of the input sequence\n",
        "\n",
        "# Initialize model\n",
        "model = EnergyBasedQuantumTransformer(input_dim, hidden_dim, num_layers, num_heads, output_dim, max_seq_len)\n",
        "\n",
        "# # Example input: batch_size = 2, sequence length = 12\n",
        "# input_bitstrings = torch.tensor([[0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1], [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]], dtype=torch.float32)\n",
        "# # Forward pass\n",
        "# output_bitstrings = model.predict_binary(input_bitstrings)\n",
        "# print(\"Predicted Output Bitstrings:\", output_bitstrings)"
      ],
      "metadata": {
        "id": "QtClo01ol1xR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def energy_loss(output, target):\n",
        "    return F.mse_loss(output, target)  # Using MSE as an example loss function\n",
        "    # return F.binary_cross_entropy_with_logits(output, target)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 100\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for initial, final in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(initial)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = energy_loss(outputs, final)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "kmzfOWXTmuq4",
        "outputId": "2836772a-2554-42de-9630-9a7ef79aad47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "EnergyBasedEncoderDecoder.energy_function_linear() missing 1 required positional argument: 'b2'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-d8981e1a896e>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-f440aabe927f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Initial state encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0msrc_energy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape: [batch_size, hidden_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Embedding with dynamics and positional encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-6c1f3e7a6246>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_a)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menergy_function_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_1_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_1_bias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_2_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_2_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: EnergyBasedEncoderDecoder.energy_function_linear() missing 1 required positional argument: 'b2'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    total_loss = 0\n",
        "    for initial, final in val_dataloader:\n",
        "        outputs = model(initial, final)\n",
        "        loss = energy_loss(outputs, final)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(val_dataloader)\n",
        "    print(f'Validation Loss: {avg_loss:.4f}')"
      ],
      "metadata": {
        "id": "xnYGSHvNm013"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}